{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started EDS-NLP provides a set of spaCy components that are used to extract information from clinical notes written in French. If it's your first time with spaCy, we recommend you familiarise yourself with some of their key concepts by looking at the \" spaCy 101 \" page. Quick start Installation You can install EDS-NLP via pip : pip install edsnlp Installed We recommend pinning the library version in your projects, or use a strict package manager like Poetry . pip install edsnlp==0.4.0 A first pipeline Once you've installed the library, let's begin with a very simple example that extracts mentions of COVID19 in a text, and detects whether they are negated. import spacy nlp = spacy . blank ( \"fr\" ) # (1) terms = dict ( covid = [ \"covid\" , \"coronavirus\" ], # (2) ) # Sentencizer component, needed for negation detection nlp . add_pipe ( \"eds.sentences\" ) # (3) # Matcher component nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms )) # (4) # Negation detection nlp . add_pipe ( \"eds.negation\" ) # Process your text in one call ! doc = nlp ( \"Le patient est atteint de covid\" ) doc . ents # (5) # Out: (covid,) doc . ents [ 0 ] . _ . negation # (6) # Out: False We only need spaCy's French tokenizer. This example terminology provides a very simple, and by no means exhaustive, list of synonyms for COVID19. In spaCy, pipelines are added via the nlp.add_pipe method . EDS-NLP pipelines are automatically discovered by spaCy. See the matching tutorial for mode details. spaCy stores extracted entities in the Doc.ents attribute . The eds.negation pipeline has added a negation custom attribute. This example is complete, it should run as-is. Check out the spaCy 101 page if you're not familiar with spaCy. Available pipeline components Core Qualifiers Miscellaneous NER Pipeline Description eds.normalizer Non-destructive input text normalisation eds.sentences Better sentence boundary detection eds.matcher A simple yet powerful entity extractor eds.advanced-matcher A conditional entity extractor eds.endlines An unsupervised model to classify each end line Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based medical history detection Pipeline Description eds.dates Date extraction and normalisation eds.sections Section detection eds.reason Rule-based hospitalisation reason detection Pipeline Description eds.charlson A Charlson score extractor eds.sofa A SOFA score extractor eds.emergency.priority A priority score extractor eds.emergency.ccmu A CCMU score extractor eds.emergency.gemsa A GEMSA score extractor Disclaimer The performances of an extraction pipeline may depend on the population and documents that are considered. Contributing to EDS-NLP We welcome contributions ! Fork the project and propose a pull request. Take a look at the dedicated page for detail.","title":"Getting started"},{"location":"#getting-started","text":"EDS-NLP provides a set of spaCy components that are used to extract information from clinical notes written in French. If it's your first time with spaCy, we recommend you familiarise yourself with some of their key concepts by looking at the \" spaCy 101 \" page.","title":"Getting started"},{"location":"#quick-start","text":"","title":"Quick start"},{"location":"#installation","text":"You can install EDS-NLP via pip : pip install edsnlp Installed We recommend pinning the library version in your projects, or use a strict package manager like Poetry . pip install edsnlp==0.4.0","title":"Installation"},{"location":"#a-first-pipeline","text":"Once you've installed the library, let's begin with a very simple example that extracts mentions of COVID19 in a text, and detects whether they are negated. import spacy nlp = spacy . blank ( \"fr\" ) # (1) terms = dict ( covid = [ \"covid\" , \"coronavirus\" ], # (2) ) # Sentencizer component, needed for negation detection nlp . add_pipe ( \"eds.sentences\" ) # (3) # Matcher component nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms )) # (4) # Negation detection nlp . add_pipe ( \"eds.negation\" ) # Process your text in one call ! doc = nlp ( \"Le patient est atteint de covid\" ) doc . ents # (5) # Out: (covid,) doc . ents [ 0 ] . _ . negation # (6) # Out: False We only need spaCy's French tokenizer. This example terminology provides a very simple, and by no means exhaustive, list of synonyms for COVID19. In spaCy, pipelines are added via the nlp.add_pipe method . EDS-NLP pipelines are automatically discovered by spaCy. See the matching tutorial for mode details. spaCy stores extracted entities in the Doc.ents attribute . The eds.negation pipeline has added a negation custom attribute. This example is complete, it should run as-is. Check out the spaCy 101 page if you're not familiar with spaCy.","title":"A first pipeline"},{"location":"#available-pipeline-components","text":"Core Qualifiers Miscellaneous NER Pipeline Description eds.normalizer Non-destructive input text normalisation eds.sentences Better sentence boundary detection eds.matcher A simple yet powerful entity extractor eds.advanced-matcher A conditional entity extractor eds.endlines An unsupervised model to classify each end line Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based medical history detection Pipeline Description eds.dates Date extraction and normalisation eds.sections Section detection eds.reason Rule-based hospitalisation reason detection Pipeline Description eds.charlson A Charlson score extractor eds.sofa A SOFA score extractor eds.emergency.priority A priority score extractor eds.emergency.ccmu A CCMU score extractor eds.emergency.gemsa A GEMSA score extractor","title":"Available pipeline components"},{"location":"#disclaimer","text":"The performances of an extraction pipeline may depend on the population and documents that are considered.","title":"Disclaimer"},{"location":"#contributing-to-eds-nlp","text":"We welcome contributions ! Fork the project and propose a pull request. Take a look at the dedicated page for detail.","title":"Contributing to EDS-NLP"},{"location":"changelog/","text":"Changelog v0.4.0 Profound re-write of the normalisation : The custom attribute CUSTOM_NORM is completely abandoned in favour of a more spacyfic alternative The normalizer pipeline modifies the NORM attribute in place Other pipelines can modify the Token._.excluded custom attribute EDS regex and term matchers can ignore excluded tokens during matching, effectively adding a second dimension to normalisation (choice of the attribute and possibility to skip pollution tokens regardless of the attribute) Matching can be performed on custom attributes more easily Qualifiers are regrouped together within the edsnlp.qualifiers submodule, the inheritance from the GenericMatcher is dropped. edsnlp.utils.filter.filter_spans now accepts a label_to_remove parameter. If set, only corresponding spans are removed, along with overlapping spans. Primary use-case: removing pseudo cues for qualifiers. Generalise the naming convention for extensions, which keep the same name as the pipeline that created them (eg Span._.negation for the eds.negation pipeline). The previous convention is kept for now, but calling it issues a warning. The dates pipeline underwent some light formatting to increase robustness and fix a few issues A new consultation_dates pipeline was added, which looks for dates preceded by expressions specific to consultation dates In rule-based processing, the terms.py submodule is replaced by patterns.py to reflect the possible presence of regular expressions Refactoring of the architecture : pipelines are now regrouped by type ( core , ner , misc , qualifiers ) matchers submodule contains RegexMatcher and PhraseMatcher classes, which interact with the normalisation multiprocessing submodule contains spark and local multiprocessing tools connectors contains Brat , OMOP and LabelTool connectors utils contains various utilities Add entry points to make pipeline usable directly, removing the need to import edsnlp.components . Add a eds namespace for components: for instance, negation becomes eds.negation . Using the former pipeline name still works, but issues a deprecation warning. Add 3 score pipelines related to emergency Add a helper function to use a spaCy pipeline as a Spark UDF. Fix alignment issues in RegexMatcher Change the alignment procedure, dropping clumsy numpy dependency in favour of bisect Change the name of eds.antecedents to eds.history . Calling eds.antecedents still works, but issues a deprecation warning and support will be removed in a future version. Add a eds.covid component, that identifies mentions of COVID Change the demo, to include NER components v0.3.2 Major revamp of the normalisation. The normalizer pipeline now adds atomic components ( lowercase , accents , quotes , pollution & endlines ) to the processing pipeline, and compiles the results into a new Doc._.normalized extension. The latter is itself a spaCy Doc object, wherein tokens are normalised and pollution tokens are removed altogether. Components that match on the CUSTOM_NORM attribute process the normalized document, and matches are brought back to the original document using a token-wise mapping. Update the RegexMatcher to use the CUSTOM_NORM attribute Add an EDSPhraseMatcher , wrapping spaCy's PhraseMatcher to enable matching on CUSTOM_NORM . Update the matcher and advanced pipelines to enable matching on the CUSTOM_NORM attribute. Add an OMOP connector, to help go back and forth between OMOP-formatted pandas dataframes and spaCy documents. Add a reason pipeline, that extracts the reason for visit. Add an endlines pipeline, that classifies newline characters between spaces and actual ends of line. Add possibility to annotate within entities for qualifiers ( negation , hypothesis , etc), ie if the cue is within the entity. Disabled by default. v0.3.1 Update dates to remove miscellaneous bugs. Add isort pre-commit hook. Improve performance for negation , hypothesis , antecedents , family and rspeech by using spaCy's filter_spans and our consume_spans methods. Add proposition segmentation to hypothesis and family , enhancing results. v0.3.0 Renamed generic to matcher . This is a non-breaking change for the average user, adding the pipeline is still : nlp . add_pipe ( \"matcher\" , config = dict ( terms = dict ( maladie = \"maladie\" ))) Removed quickumls pipeline. It was untested, unmaintained. Will be added back in a future release. Add score pipeline, and charlson . Add advanced-regex pipeline Corrected bugs in the negation pipeline v0.2.0 Add negation pipeline Add family pipeline Add hypothesis pipeline Add antecedents pipeline Add rspeech pipeline Refactor the library : Remove the rules folder Add a pipelines folder, containing one subdirectory per component Every component subdirectory contains a module defining the component, and a module defining a factory, plus any other utilities (eg terms.py ) v0.1.0 First working version. Available pipelines : section sentences normalization pollution","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v040","text":"Profound re-write of the normalisation : The custom attribute CUSTOM_NORM is completely abandoned in favour of a more spacyfic alternative The normalizer pipeline modifies the NORM attribute in place Other pipelines can modify the Token._.excluded custom attribute EDS regex and term matchers can ignore excluded tokens during matching, effectively adding a second dimension to normalisation (choice of the attribute and possibility to skip pollution tokens regardless of the attribute) Matching can be performed on custom attributes more easily Qualifiers are regrouped together within the edsnlp.qualifiers submodule, the inheritance from the GenericMatcher is dropped. edsnlp.utils.filter.filter_spans now accepts a label_to_remove parameter. If set, only corresponding spans are removed, along with overlapping spans. Primary use-case: removing pseudo cues for qualifiers. Generalise the naming convention for extensions, which keep the same name as the pipeline that created them (eg Span._.negation for the eds.negation pipeline). The previous convention is kept for now, but calling it issues a warning. The dates pipeline underwent some light formatting to increase robustness and fix a few issues A new consultation_dates pipeline was added, which looks for dates preceded by expressions specific to consultation dates In rule-based processing, the terms.py submodule is replaced by patterns.py to reflect the possible presence of regular expressions Refactoring of the architecture : pipelines are now regrouped by type ( core , ner , misc , qualifiers ) matchers submodule contains RegexMatcher and PhraseMatcher classes, which interact with the normalisation multiprocessing submodule contains spark and local multiprocessing tools connectors contains Brat , OMOP and LabelTool connectors utils contains various utilities Add entry points to make pipeline usable directly, removing the need to import edsnlp.components . Add a eds namespace for components: for instance, negation becomes eds.negation . Using the former pipeline name still works, but issues a deprecation warning. Add 3 score pipelines related to emergency Add a helper function to use a spaCy pipeline as a Spark UDF. Fix alignment issues in RegexMatcher Change the alignment procedure, dropping clumsy numpy dependency in favour of bisect Change the name of eds.antecedents to eds.history . Calling eds.antecedents still works, but issues a deprecation warning and support will be removed in a future version. Add a eds.covid component, that identifies mentions of COVID Change the demo, to include NER components","title":"v0.4.0"},{"location":"changelog/#v032","text":"Major revamp of the normalisation. The normalizer pipeline now adds atomic components ( lowercase , accents , quotes , pollution & endlines ) to the processing pipeline, and compiles the results into a new Doc._.normalized extension. The latter is itself a spaCy Doc object, wherein tokens are normalised and pollution tokens are removed altogether. Components that match on the CUSTOM_NORM attribute process the normalized document, and matches are brought back to the original document using a token-wise mapping. Update the RegexMatcher to use the CUSTOM_NORM attribute Add an EDSPhraseMatcher , wrapping spaCy's PhraseMatcher to enable matching on CUSTOM_NORM . Update the matcher and advanced pipelines to enable matching on the CUSTOM_NORM attribute. Add an OMOP connector, to help go back and forth between OMOP-formatted pandas dataframes and spaCy documents. Add a reason pipeline, that extracts the reason for visit. Add an endlines pipeline, that classifies newline characters between spaces and actual ends of line. Add possibility to annotate within entities for qualifiers ( negation , hypothesis , etc), ie if the cue is within the entity. Disabled by default.","title":"v0.3.2"},{"location":"changelog/#v031","text":"Update dates to remove miscellaneous bugs. Add isort pre-commit hook. Improve performance for negation , hypothesis , antecedents , family and rspeech by using spaCy's filter_spans and our consume_spans methods. Add proposition segmentation to hypothesis and family , enhancing results.","title":"v0.3.1"},{"location":"changelog/#v030","text":"Renamed generic to matcher . This is a non-breaking change for the average user, adding the pipeline is still : nlp . add_pipe ( \"matcher\" , config = dict ( terms = dict ( maladie = \"maladie\" ))) Removed quickumls pipeline. It was untested, unmaintained. Will be added back in a future release. Add score pipeline, and charlson . Add advanced-regex pipeline Corrected bugs in the negation pipeline","title":"v0.3.0"},{"location":"changelog/#v020","text":"Add negation pipeline Add family pipeline Add hypothesis pipeline Add antecedents pipeline Add rspeech pipeline Refactor the library : Remove the rules folder Add a pipelines folder, containing one subdirectory per component Every component subdirectory contains a module defining the component, and a module defining a factory, plus any other utilities (eg terms.py )","title":"v0.2.0"},{"location":"changelog/#v010","text":"First working version. Available pipelines : section sentences normalization pollution","title":"v0.1.0"},{"location":"contributing/","text":"Contributing to EDS-NLP We welcome contributions ! There are many ways to help. For example, you can: Help us track bugs by filing issues Suggest and help prioritise new functionalities Develop a new pipeline ! Fork the project and propose a new functionality through a pull request Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you. Development installation To be able to run the test suite, run the example notebooks and develop your own pipeline, you should clone the repo and install it locally. # Clone the repository and change directory git clone https://github.com/aphp/edsnlp.git cd edsnlp # Optional: create a virtual environment python -m venv venv source venv/bin/activate # Install setup dependencies and build resources pip install -r requirements.txt pip install -r requirements-setup.txt python scripts/conjugate_verbs.py # Install development dependencies pip install -r requirements-dev.txt pip install -r requirements-docs.txt # Finally, install the package pip install . To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the pre-commit Python library. To use it, simply install it: pre-commit install The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong. The hooks only run on staged changes. To force-run it on all files, run: pre-commit run --all-files All good ! Proposing a merge request At the very least, your changes should : Be well-documented ; Pass every tests, and preferably implement its own ; Follow the style guide. Testing your code We use the Pytest test suite. The following command will run the test suite. Writing your own tests is encouraged ! python -m pytest Should your contribution propose a bug fix, we require the bug be thoroughly tested. Architecture of a pipeline All pipelines should follow the same pattern : edsnlp/pipelines/<pipeline> |-- <pipeline>.py # Defines the component logic |-- patterns.py # Defines matched patterns |-- factory.py # Declares the pipeline to spaCy Style Guide We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short : Black reformats entire files in place. It is not configurable. Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use pre-commit to keep our codebase clean. Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save. Documentation Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be. We use MkDocs for EDS-NLP's documentation. You can checkout the changes you make with: # Install the requirements pip install -r requirements-docs.txt Installation successful A # Run the documentation mkdocs serve Go to localhost:8000 to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page.","title":"Contributing to EDS-NLP"},{"location":"contributing/#contributing-to-eds-nlp","text":"We welcome contributions ! There are many ways to help. For example, you can: Help us track bugs by filing issues Suggest and help prioritise new functionalities Develop a new pipeline ! Fork the project and propose a new functionality through a pull request Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you.","title":"Contributing to EDS-NLP"},{"location":"contributing/#development-installation","text":"To be able to run the test suite, run the example notebooks and develop your own pipeline, you should clone the repo and install it locally. # Clone the repository and change directory git clone https://github.com/aphp/edsnlp.git cd edsnlp # Optional: create a virtual environment python -m venv venv source venv/bin/activate # Install setup dependencies and build resources pip install -r requirements.txt pip install -r requirements-setup.txt python scripts/conjugate_verbs.py # Install development dependencies pip install -r requirements-dev.txt pip install -r requirements-docs.txt # Finally, install the package pip install . To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the pre-commit Python library. To use it, simply install it: pre-commit install The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong. The hooks only run on staged changes. To force-run it on all files, run: pre-commit run --all-files All good !","title":"Development installation"},{"location":"contributing/#proposing-a-merge-request","text":"At the very least, your changes should : Be well-documented ; Pass every tests, and preferably implement its own ; Follow the style guide.","title":"Proposing a merge request"},{"location":"contributing/#testing-your-code","text":"We use the Pytest test suite. The following command will run the test suite. Writing your own tests is encouraged ! python -m pytest Should your contribution propose a bug fix, we require the bug be thoroughly tested.","title":"Testing your code"},{"location":"contributing/#architecture-of-a-pipeline","text":"All pipelines should follow the same pattern : edsnlp/pipelines/<pipeline> |-- <pipeline>.py # Defines the component logic |-- patterns.py # Defines matched patterns |-- factory.py # Declares the pipeline to spaCy","title":"Architecture of a pipeline"},{"location":"contributing/#style-guide","text":"We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short : Black reformats entire files in place. It is not configurable. Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use pre-commit to keep our codebase clean. Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save.","title":"Style Guide"},{"location":"contributing/#documentation","text":"Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be. We use MkDocs for EDS-NLP's documentation. You can checkout the changes you make with: # Install the requirements pip install -r requirements-docs.txt Installation successful A # Run the documentation mkdocs serve Go to localhost:8000 to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page.","title":"Documentation"},{"location":"advanced-tutorials/","text":"Advanced use cases In this section, we review a few advanced use cases: Adding pre-computed word vectors to spaCy Deploying your spaCy pipeline as an API Creating your own component","title":"Advanced use cases"},{"location":"advanced-tutorials/#advanced-use-cases","text":"In this section, we review a few advanced use cases: Adding pre-computed word vectors to spaCy Deploying your spaCy pipeline as an API Creating your own component","title":"Advanced use cases"},{"location":"advanced-tutorials/fastapi/","text":"Deploying as an API In this section, we will see how you can deploy your pipeline as a REST API using the power of FastAPI . The NLP pipeline Let's create a simple NLP model, that can: match synonyms of COVID19 check for negation, speculation and reported speech. You know the drill: pipeline.py import spacy nlp = spacy . blank ( 'fr' ) nlp . add_pipe ( \"eds.sentences\" ) config = dict ( regex = dict ( covid = [ \"covid\" , r \"covid[-\\s]?19\" , r \"sars[-\\s]?cov[-\\s]?2\" , r \"corona[-\\s]?virus\" , ], ), attr = \"LOWER\" , ) nlp . add_pipe ( 'eds.matcher' , config = config ) nlp . add_pipe ( \"eds.negation\" ) nlp . add_pipe ( \"eds.family\" ) nlp . add_pipe ( \"eds.hypothesis\" ) nlp . add_pipe ( \"eds.reported_speech\" ) Creating the FastAPI app FastAPI is a incredibly efficient framework, based on Python type hints from the ground up, with the help of Pydantic (another great library for building modern Python). We won't go into too much detail about FastAPI in this tutorial. For further information on how the framework operates, go to its excellent documentation ! We'll need to create two things: A module containing the models for inputs and outputs. The script that defines the application itself. models.py from typing import List from pydantic import BaseModel class Entity ( BaseModel ): # (1) # OMOP-style attributes start : int end : int label : str lexical_variant : str normalized_variant : str # Qualifiers negated : bool hypothesis : bool family : bool reported_speech : bool class Document ( BaseModel ): # (2) text : str ents : List [ Entity ] The Entity model contains attributes that define a matched entity, as well as variables that contain the output of the qualifier components. The Document model contains the input text, and a list of detected entities Having defined the output models and the pipeline, we can move on to creating the application itself: app.py from typing import List from fastapi import FastAPI from pipeline import nlp from models import Entity , Document app = FastAPI ( title = \"EDS-NLP\" , version = edsnlp . __version__ ) @app . post ( \"/covid\" , response_model = List [ Document ]) # (1) async def process ( notes : List [ str ], # (2) ): documents = [] for doc in nlp . pipe ( notes ): entities = [] for ent in doc . ents : entity = Entity ( start = ent . start_char , end = ent . end_char , label = ent . label_ , lexical_variant = ent . text , normalized_variant = ent . _ . normalized_variant , negated = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , reported_speech = ent . _ . reported_speech , ) entities . append ( entity ) documents . append ( Document ( text = doc . text , ents = entities , ) ) return documents By telling FastAPI what output format is expected, you get automatic data validation. In FastAPI, input and output schemas are defined through Python type hinting. Here, we tell FastAPI to expect a list of strings in the POST request body. As a bonus, you get data validation for free. Running the API Our simple API is ready to launch! We'll just need to install FastAPI along with a ASGI server to run it. This can be done in one go: pip install fastapi[uvicorn] Successfully installed fastapi Launching the API is trivial: uvicorn app:app --reload Go to localhost:8000/docs to admire the automatically generated documentation! Using the API You can try the API directly from the documentation. Otherwise, you may use the requests package: import request notes = [ \"Le p\u00e8re du patient n'est pas atteint de la covid.\" , \"Probable coronavirus.\" , ] r = requests . post ( \"http://localhost:8000/covid\" , json = notes , ) r . json () You should get something like: [ { \"text\" : \"Le p\u00e8re du patient n'est pas atteint de la covid.\" , \"ents\" : [ { \"start\" : 43 , \"end\" : 48 , \"label\" : \"covid\" , \"lexical_variant\" : \"covid\" , \"normalized_variant\" : \"covid\" , \"negated\" : true , \"hypothesis\" : false , \"family\" : true , \"reported_speech\" : false } ] }, { \"text\" : \"Probable coronavirus.\" , \"ents\" : [ { \"start\" : 9 , \"end\" : 20 , \"label\" : \"covid\" , \"lexical_variant\" : \"coronavirus\" , \"normalized_variant\" : \"coronavirus\" , \"negated\" : false , \"hypothesis\" : true , \"family\" : false , \"reported_speech\" : false } ] } ]","title":"Deploying as an API"},{"location":"advanced-tutorials/fastapi/#deploying-as-an-api","text":"In this section, we will see how you can deploy your pipeline as a REST API using the power of FastAPI .","title":"Deploying as an API"},{"location":"advanced-tutorials/fastapi/#the-nlp-pipeline","text":"Let's create a simple NLP model, that can: match synonyms of COVID19 check for negation, speculation and reported speech. You know the drill: pipeline.py import spacy nlp = spacy . blank ( 'fr' ) nlp . add_pipe ( \"eds.sentences\" ) config = dict ( regex = dict ( covid = [ \"covid\" , r \"covid[-\\s]?19\" , r \"sars[-\\s]?cov[-\\s]?2\" , r \"corona[-\\s]?virus\" , ], ), attr = \"LOWER\" , ) nlp . add_pipe ( 'eds.matcher' , config = config ) nlp . add_pipe ( \"eds.negation\" ) nlp . add_pipe ( \"eds.family\" ) nlp . add_pipe ( \"eds.hypothesis\" ) nlp . add_pipe ( \"eds.reported_speech\" )","title":"The NLP pipeline"},{"location":"advanced-tutorials/fastapi/#creating-the-fastapi-app","text":"FastAPI is a incredibly efficient framework, based on Python type hints from the ground up, with the help of Pydantic (another great library for building modern Python). We won't go into too much detail about FastAPI in this tutorial. For further information on how the framework operates, go to its excellent documentation ! We'll need to create two things: A module containing the models for inputs and outputs. The script that defines the application itself. models.py from typing import List from pydantic import BaseModel class Entity ( BaseModel ): # (1) # OMOP-style attributes start : int end : int label : str lexical_variant : str normalized_variant : str # Qualifiers negated : bool hypothesis : bool family : bool reported_speech : bool class Document ( BaseModel ): # (2) text : str ents : List [ Entity ] The Entity model contains attributes that define a matched entity, as well as variables that contain the output of the qualifier components. The Document model contains the input text, and a list of detected entities Having defined the output models and the pipeline, we can move on to creating the application itself: app.py from typing import List from fastapi import FastAPI from pipeline import nlp from models import Entity , Document app = FastAPI ( title = \"EDS-NLP\" , version = edsnlp . __version__ ) @app . post ( \"/covid\" , response_model = List [ Document ]) # (1) async def process ( notes : List [ str ], # (2) ): documents = [] for doc in nlp . pipe ( notes ): entities = [] for ent in doc . ents : entity = Entity ( start = ent . start_char , end = ent . end_char , label = ent . label_ , lexical_variant = ent . text , normalized_variant = ent . _ . normalized_variant , negated = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , reported_speech = ent . _ . reported_speech , ) entities . append ( entity ) documents . append ( Document ( text = doc . text , ents = entities , ) ) return documents By telling FastAPI what output format is expected, you get automatic data validation. In FastAPI, input and output schemas are defined through Python type hinting. Here, we tell FastAPI to expect a list of strings in the POST request body. As a bonus, you get data validation for free.","title":"Creating the FastAPI app"},{"location":"advanced-tutorials/fastapi/#running-the-api","text":"Our simple API is ready to launch! We'll just need to install FastAPI along with a ASGI server to run it. This can be done in one go: pip install fastapi[uvicorn] Successfully installed fastapi Launching the API is trivial: uvicorn app:app --reload Go to localhost:8000/docs to admire the automatically generated documentation!","title":"Running the API"},{"location":"advanced-tutorials/fastapi/#using-the-api","text":"You can try the API directly from the documentation. Otherwise, you may use the requests package: import request notes = [ \"Le p\u00e8re du patient n'est pas atteint de la covid.\" , \"Probable coronavirus.\" , ] r = requests . post ( \"http://localhost:8000/covid\" , json = notes , ) r . json () You should get something like: [ { \"text\" : \"Le p\u00e8re du patient n'est pas atteint de la covid.\" , \"ents\" : [ { \"start\" : 43 , \"end\" : 48 , \"label\" : \"covid\" , \"lexical_variant\" : \"covid\" , \"normalized_variant\" : \"covid\" , \"negated\" : true , \"hypothesis\" : false , \"family\" : true , \"reported_speech\" : false } ] }, { \"text\" : \"Probable coronavirus.\" , \"ents\" : [ { \"start\" : 9 , \"end\" : 20 , \"label\" : \"covid\" , \"lexical_variant\" : \"coronavirus\" , \"normalized_variant\" : \"coronavirus\" , \"negated\" : false , \"hypothesis\" : true , \"family\" : false , \"reported_speech\" : false } ] } ]","title":"Using the API"},{"location":"advanced-tutorials/word-vectors/","text":"Word embeddings EDS-NLP proposes rule-based components exclusively. However, that does not prohibit you from exploiting spaCy's machine learning capabilities! You can mix and match machine learning pipelines, trainable or not, with EDS-NLP rule-based components. In this tutorial, we will explore how you can use static word vectors trained with Gensim within spaCy. Training the word embedding, however, is outside the scope of this post. You'll find very well designed resources on the subject in Gensim's documenation . Using Transformer models spaCy v3 introduced support for Transformer models through their helper library spacy-transformers that interfaces with HuggingFace's transformers library. Using transformer models can significantly increase your model's performance. Adding pre-trained word vectors spaCy provides a init vectors CLI utility that takes a Gensim-trained binary and transforms it to a spaCy-readable pipeline. Using it is straightforward : spacy init vectors fr /path/to/vectors /path/to/pipeline Conversion successful! See the documentation for implementation details.","title":"Word embeddings"},{"location":"advanced-tutorials/word-vectors/#word-embeddings","text":"EDS-NLP proposes rule-based components exclusively. However, that does not prohibit you from exploiting spaCy's machine learning capabilities! You can mix and match machine learning pipelines, trainable or not, with EDS-NLP rule-based components. In this tutorial, we will explore how you can use static word vectors trained with Gensim within spaCy. Training the word embedding, however, is outside the scope of this post. You'll find very well designed resources on the subject in Gensim's documenation . Using Transformer models spaCy v3 introduced support for Transformer models through their helper library spacy-transformers that interfaces with HuggingFace's transformers library. Using transformer models can significantly increase your model's performance.","title":"Word embeddings"},{"location":"advanced-tutorials/word-vectors/#adding-pre-trained-word-vectors","text":"spaCy provides a init vectors CLI utility that takes a Gensim-trained binary and transforms it to a spaCy-readable pipeline. Using it is straightforward : spacy init vectors fr /path/to/vectors /path/to/pipeline Conversion successful! See the documentation for implementation details.","title":"Adding pre-trained word vectors"},{"location":"pipelines/","text":"Pipelines overview EDS-NLP provides easy-to-use spaCy components. Core Qualifiers Miscellaneous NER Pipeline Description eds.normalizer Non-destructive input text normalisation eds.sentences Better sentence boundary detection eds.matcher A simple yet powerful entity extractor eds.advanced-matcher A conditional entity extractor eds.endlines An unsupervised model to classify each end line Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based medical history detection Pipeline Description eds.dates Date extraction and normalisation eds.sections Section detection eds.reason Rule-based hospitalisation reason detection Pipeline Description eds.charlson A Charlson score extractor eds.sofa A SOFA score extractor eds.emergency.priority A priority score extractor eds.emergency.ccmu A CCMU score extractor eds.emergency.gemsa A GEMSA score extractor You can add them to your spaCy pipeline by simply calling add_pipe , for instance: # \u2191 Omitted code that defines the nlp object \u2191 nlp . add_pipe ( \"eds.normalizer\" )","title":"Pipelines overview"},{"location":"pipelines/#pipelines-overview","text":"EDS-NLP provides easy-to-use spaCy components. Core Qualifiers Miscellaneous NER Pipeline Description eds.normalizer Non-destructive input text normalisation eds.sentences Better sentence boundary detection eds.matcher A simple yet powerful entity extractor eds.advanced-matcher A conditional entity extractor eds.endlines An unsupervised model to classify each end line Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based medical history detection Pipeline Description eds.dates Date extraction and normalisation eds.sections Section detection eds.reason Rule-based hospitalisation reason detection Pipeline Description eds.charlson A Charlson score extractor eds.sofa A SOFA score extractor eds.emergency.priority A priority score extractor eds.emergency.ccmu A CCMU score extractor eds.emergency.gemsa A GEMSA score extractor You can add them to your spaCy pipeline by simply calling add_pipe , for instance: # \u2191 Omitted code that defines the nlp object \u2191 nlp . add_pipe ( \"eds.normalizer\" )","title":"Pipelines overview"},{"location":"pipelines/architecture/","text":"Basic Architecture Most pipelines provided by EDS-NLP aim to qualify pre-extracted entities. To wit, the basic usage of the library: Implement a normaliser (see normalizer ) Add an entity recognition component (eg the simple but powerful matcher pipeline ) Add zero or more entity qualification components, such as negation , family or hypothesis . These qualifiers typically help detect false-positives. Scope Since the basic usage of EDS-NLP components is to qualify entities, most pipelines can function in two modes: Annotation of the extracted entities (this is the default). To increase throughput, only pre-extracted entities (found in doc.ents ) are processed. Full-text, token-wise annotation. This mode is activated by setting the on_ents_only parameter to False . The possibility to do full-text annotation implies that one could use the pipelines the other way around, eg detecting all negations once and for all in an ETL phase, and reusing the results consequently. However, this is not the intended use of the library, which aims to help researchers downstream as a standalone application. Result persistence Depending on their purpose (entity extraction, qualification, etc), EDS-NLP pipelines write their results to Doc.ents , Doc.spans or in a custom attribute. Extraction pipelines Extraction pipelines (matchers, the date detector or NER pipelines, for instance) keep their results to the Doc.ents attribute directly. Note that spaCy prohibits overlapping entities within the Doc.ents attribute. To circumvent this limitation, we filter spans , and keep all discarded entities within the discarded key of the Doc.spans attribute. Some pipelines write their output to the Doc.spans dictionary. We enforce the following doctrine: Should the pipe extract entities that are directly informative (typically the output of the eds.matcher component), said entities are stashed in the Doc.ents attribute. On the other hand, should the entity be useful to another pipe, but less so in itself (eg the output of the eds.sections or eds.dates component), it will be stashed in a specific key within the Doc.spans attribute. Entity tagging Moreover, most pipelines declare spaCy extensions , on the Doc , Span and/or Token objects. These extensions are especially useful for qualifier pipelines, but can also be used by other pipelines to persist relevant information. For instance, the eds.dates pipeline: Populates Doc . spans [ \"dates\" ] For each detected item, keeps the normalised date in Span . _ . date","title":"Basic Architecture"},{"location":"pipelines/architecture/#basic-architecture","text":"Most pipelines provided by EDS-NLP aim to qualify pre-extracted entities. To wit, the basic usage of the library: Implement a normaliser (see normalizer ) Add an entity recognition component (eg the simple but powerful matcher pipeline ) Add zero or more entity qualification components, such as negation , family or hypothesis . These qualifiers typically help detect false-positives.","title":"Basic Architecture"},{"location":"pipelines/architecture/#scope","text":"Since the basic usage of EDS-NLP components is to qualify entities, most pipelines can function in two modes: Annotation of the extracted entities (this is the default). To increase throughput, only pre-extracted entities (found in doc.ents ) are processed. Full-text, token-wise annotation. This mode is activated by setting the on_ents_only parameter to False . The possibility to do full-text annotation implies that one could use the pipelines the other way around, eg detecting all negations once and for all in an ETL phase, and reusing the results consequently. However, this is not the intended use of the library, which aims to help researchers downstream as a standalone application.","title":"Scope"},{"location":"pipelines/architecture/#result-persistence","text":"Depending on their purpose (entity extraction, qualification, etc), EDS-NLP pipelines write their results to Doc.ents , Doc.spans or in a custom attribute.","title":"Result persistence"},{"location":"pipelines/architecture/#extraction-pipelines","text":"Extraction pipelines (matchers, the date detector or NER pipelines, for instance) keep their results to the Doc.ents attribute directly. Note that spaCy prohibits overlapping entities within the Doc.ents attribute. To circumvent this limitation, we filter spans , and keep all discarded entities within the discarded key of the Doc.spans attribute. Some pipelines write their output to the Doc.spans dictionary. We enforce the following doctrine: Should the pipe extract entities that are directly informative (typically the output of the eds.matcher component), said entities are stashed in the Doc.ents attribute. On the other hand, should the entity be useful to another pipe, but less so in itself (eg the output of the eds.sections or eds.dates component), it will be stashed in a specific key within the Doc.spans attribute.","title":"Extraction pipelines"},{"location":"pipelines/architecture/#entity-tagging","text":"Moreover, most pipelines declare spaCy extensions , on the Doc , Span and/or Token objects. These extensions are especially useful for qualifier pipelines, but can also be used by other pipelines to persist relevant information. For instance, the eds.dates pipeline: Populates Doc . spans [ \"dates\" ] For each detected item, keeps the normalised date in Span . _ . date","title":"Entity tagging"},{"location":"pipelines/core/","text":"Core Pipelines This section deals with \"core\" functionalities offered by EDS-NLP: Matching a terminology Normalising a text Detecting sentence boundaries","title":"Core Pipelines"},{"location":"pipelines/core/#core-pipelines","text":"This section deals with \"core\" functionalities offered by EDS-NLP: Matching a terminology Normalising a text Detecting sentence boundaries","title":"Core Pipelines"},{"location":"pipelines/core/advanced-matcher/","text":"Advanced Matcher","title":"Advanced Matcher"},{"location":"pipelines/core/advanced-matcher/#advanced-matcher","text":"","title":"Advanced Matcher"},{"location":"pipelines/core/endlines/","text":"Endlines The eds.endlines pipeline classifies newline characters as actual end of lines or mere spaces. In the latter case, the token is removed from the normalised document. Behind the scenes, it uses a endlinesmodel instance, which is an unsupervised algorithm based on the work of Zweigenbaum et al 1 . Usage The following example shows a simple usage. Training import spacy from edsnlp.pipelines.endlines.endlinesmodel import EndLinesModel import pandas as pd from spacy import displacy nlp = spacy . blank ( \"fr\" ) texts = [ \"\"\"Le patient est arriv\u00e9 hier soir. Il est accompagn\u00e9 par son fils ANTECEDENTS Il a fait une TS en 2010; Fumeur, il est arret\u00e9 il a 5 mois Chirurgie de coeur en 2011 CONCLUSION Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. DIAGNOSTIC : Antecedents Familiaux: - 1. P\u00e8re avec diabete \"\"\" , \"\"\"J'aime le \\nfromage...\\n\"\"\" , ] docs = list ( nlp . pipe ( texts )) # Train and predict an EndLinesModel endlines = EndLinesModel ( nlp = nlp ) df = endlines . fit_and_predict ( docs ) df . head () PATH = \"path_to_save\" endlines . save ( PATH ) Inference import spacy nlp = spacy . blank ( \"fr\" ) PATH = \"path_to_save\" nlp . add_pipe ( \"eds.endlines\" , config = dict ( model_path = PATH )) docs = list ( nlp . pipe ( texts )) doc_exemple = docs [ 1 ] doc_exemple doc_exemple . ents = tuple ( s for s in doc_exemple . spans [ \"new_lines\" ] if s . label_ == \"space\" ) displacy . render ( doc_exemple , style = \"ent\" , options = { \"colors\" : { \"space\" : \"red\" }}) Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default model_path Path to the pre-trained pipeline Required Declared extensions The eds.endlines pipeline declares one spaCy extensions , on both Span and Token objects. The end_line attribute is a boolean, set to True if the pipeline predicts that the new line is an end line character. Otherwise, it is set to False if the new line is classified as a space. The pipeline also sets the excluded custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation ) for more detail. Authors and citation The eds.endlines pipeline was developed by AP-HP's Data Science team. Pierre Zweigenbaum, Cyril Grouin, and Thomas Lavergne. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (end-of-line classification with no supervision). In Actes de la conf\u00e9rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters) , 364\u2013371. Paris, France, 7 2016. AFCP - ATALA. URL: https://aclanthology.org/2016.jeptalnrecital-poster.7 . \u21a9","title":"Endlines"},{"location":"pipelines/core/endlines/#endlines","text":"The eds.endlines pipeline classifies newline characters as actual end of lines or mere spaces. In the latter case, the token is removed from the normalised document. Behind the scenes, it uses a endlinesmodel instance, which is an unsupervised algorithm based on the work of Zweigenbaum et al 1 .","title":"Endlines"},{"location":"pipelines/core/endlines/#usage","text":"The following example shows a simple usage.","title":"Usage"},{"location":"pipelines/core/endlines/#training","text":"import spacy from edsnlp.pipelines.endlines.endlinesmodel import EndLinesModel import pandas as pd from spacy import displacy nlp = spacy . blank ( \"fr\" ) texts = [ \"\"\"Le patient est arriv\u00e9 hier soir. Il est accompagn\u00e9 par son fils ANTECEDENTS Il a fait une TS en 2010; Fumeur, il est arret\u00e9 il a 5 mois Chirurgie de coeur en 2011 CONCLUSION Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. DIAGNOSTIC : Antecedents Familiaux: - 1. P\u00e8re avec diabete \"\"\" , \"\"\"J'aime le \\nfromage...\\n\"\"\" , ] docs = list ( nlp . pipe ( texts )) # Train and predict an EndLinesModel endlines = EndLinesModel ( nlp = nlp ) df = endlines . fit_and_predict ( docs ) df . head () PATH = \"path_to_save\" endlines . save ( PATH )","title":"Training"},{"location":"pipelines/core/endlines/#inference","text":"import spacy nlp = spacy . blank ( \"fr\" ) PATH = \"path_to_save\" nlp . add_pipe ( \"eds.endlines\" , config = dict ( model_path = PATH )) docs = list ( nlp . pipe ( texts )) doc_exemple = docs [ 1 ] doc_exemple doc_exemple . ents = tuple ( s for s in doc_exemple . spans [ \"new_lines\" ] if s . label_ == \"space\" ) displacy . render ( doc_exemple , style = \"ent\" , options = { \"colors\" : { \"space\" : \"red\" }})","title":"Inference"},{"location":"pipelines/core/endlines/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default model_path Path to the pre-trained pipeline Required","title":"Configuration"},{"location":"pipelines/core/endlines/#declared-extensions","text":"The eds.endlines pipeline declares one spaCy extensions , on both Span and Token objects. The end_line attribute is a boolean, set to True if the pipeline predicts that the new line is an end line character. Otherwise, it is set to False if the new line is classified as a space. The pipeline also sets the excluded custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation ) for more detail.","title":"Declared extensions"},{"location":"pipelines/core/endlines/#authors-and-citation","text":"The eds.endlines pipeline was developed by AP-HP's Data Science team. Pierre Zweigenbaum, Cyril Grouin, and Thomas Lavergne. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (end-of-line classification with no supervision). In Actes de la conf\u00e9rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters) , 364\u2013371. Paris, France, 7 2016. AFCP - ATALA. URL: https://aclanthology.org/2016.jeptalnrecital-poster.7 . \u21a9","title":"Authors and citation"},{"location":"pipelines/core/matcher/","text":"Matcher EDS-NLP simplifies the matching process by exposing a eds.matcher pipeline that can match on terms or regular expressions. Usage Let us redefine the pipeline : import spacy nlp = spacy . blank ( \"fr\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], # (1) patient = \"patient\" , # (2) ) regex = dict ( covid = r \"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\" , # (3) ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , regex = regex , attr = \"LOWER\" )) Every key in the terms dictionary is mapped to a concept. The eds.matcher pipeline expects a list of expressions, or a single expression. We can also define regular expression patterns. This snippet is complete, and should run as is. Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default terms Terms patterns None (use regex only) regex RegExp patterns. None (use terms only) attr spaCy attribute to match on (eg NORM , LOWER ) \"TEXT\" ignore_excluded Whether to skip excluded tokens during matching False Patterns, be they terms or regex , are defined as dictionaries where keys become the label of the extracted entities. Dictionary values are a either a single expression or a list of expressions that match the concept (see example ). Authors and citation The eds.matcher pipeline was developed by AP-HP's Data Science team.","title":"Matcher"},{"location":"pipelines/core/matcher/#matcher","text":"EDS-NLP simplifies the matching process by exposing a eds.matcher pipeline that can match on terms or regular expressions.","title":"Matcher"},{"location":"pipelines/core/matcher/#usage","text":"Let us redefine the pipeline : import spacy nlp = spacy . blank ( \"fr\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], # (1) patient = \"patient\" , # (2) ) regex = dict ( covid = r \"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\" , # (3) ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , regex = regex , attr = \"LOWER\" )) Every key in the terms dictionary is mapped to a concept. The eds.matcher pipeline expects a list of expressions, or a single expression. We can also define regular expression patterns. This snippet is complete, and should run as is.","title":"Usage"},{"location":"pipelines/core/matcher/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default terms Terms patterns None (use regex only) regex RegExp patterns. None (use terms only) attr spaCy attribute to match on (eg NORM , LOWER ) \"TEXT\" ignore_excluded Whether to skip excluded tokens during matching False Patterns, be they terms or regex , are defined as dictionaries where keys become the label of the extracted entities. Dictionary values are a either a single expression or a list of expressions that match the concept (see example ).","title":"Configuration"},{"location":"pipelines/core/matcher/#authors-and-citation","text":"The eds.matcher pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/core/normalisation/","text":"Normalisation The normalisation scheme used by EDS-NLP adheres to the non-destructive doctrine. In other words, nlp ( text ) . text == text is always true. To achieve this, the input text is never modified. Instead, our normalisation strategy focuses on two axes: Only the NORM attribute is modified by the normalizer pipeline ; Pipelines (eg the pollution pipeline) can mark tokens as excluded by setting the extension Token._.excluded to True . It enables downstream matchers to skip excluded tokens. The normaliser can act on the input text in four dimensions : Move the text to lowercase . Remove accents . We use a deterministic approach to avoid modifying the character-length of the text, which helps for RegEx matching. Normalize apostrophes and quotation marks , which are often coded using special characters. Remove pollutions . Note We recommend you also add an end-of-line classifier to remove excess new line characters (introduced by the PDF layout). We provide a endlines pipeline, which requires training an unsupervised model. Refer to the dedicated page for more information . Usage The normalisation is handled by the single eds.normalizer pipeline. The following code snippet is complete, and should run as is. import spacy from edsnlp.matchers.utils import get_text nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" ) # Notice the special character used for the apostrophe and the quotes text = \"Le patient est admis \u00e0 l'h\u00f4pital le 23 ao\u00fbt 2021 pour une douleur \u02baaffreuse\u201d \u00e0 l`estomac.\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" ) # Out: le patient est admis a l'hopital le 23 aout 2021 pour une douleur \"affreuse\" a l'estomac Utilities To simplify the use of the normalisation output, we provide the get_text utility function. It computes the textual representation for a Span or Doc object. Moreover, every span exposes a normalized_variant extension getter, which computes the normalised representation of an entity on the fly. Configuration Parameter Explanation Default accents Whether to strip accents True lowercase Whether to remove casing True quotes Whether to normalise quotes True pollution Whether to tag pollutions as excluded tokens True Pipelines Let's review each subcomponent. Lowercase The eds.lowercase pipeline transforms every token to lowercase. It is not configurable. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = True , accents = False , quotes = False , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" ) # Out: pneumopathie \u00e0 nbnbwbwbnbwbnbnbnbwbw `coronavirus' Accents The eds.accents pipeline removes accents. To avoid edge cases, the component uses a specified list of accentuated characters and their unaccented representation, making it more predictable than using a library such as unidecode . Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = True , quotes = False , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" ) # Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus' Apostrophes and quotation marks Apostrophes and quotation marks can be encoded using unpredictable special characters. The eds.quotes component transforms every such special character to ' and \" , respectively. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = False , quotes = True , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" ) # Out: Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW 'coronavirus' Pollution The pollution pipeline uses a set of regular expressions to detect pollutions (irrelevant non-medical text that hinders text processing). Corresponding tokens are marked as excluded (by setting Token._.excluded to True ), enabling the use of the phrase matcher. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = True , quotes = False , pollution = True , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" ) # Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus' get_text ( doc , attr = \"TEXT\" , ignore_excluded = True ) # Out: Pneumopathie \u00e0 `coronavirus' This example above shows that the normalisation scheme works on two axes: non-destructive text modification and exclusion of tokens. The two are independent: a matcher can use the NORM attribute but keep excluded tokens, and conversely, match on TEXT while ignoring excluded tokens. Authors and citation The eds.normalizer pipeline was developed by AP-HP's Data Science team.","title":"Normalisation"},{"location":"pipelines/core/normalisation/#normalisation","text":"The normalisation scheme used by EDS-NLP adheres to the non-destructive doctrine. In other words, nlp ( text ) . text == text is always true. To achieve this, the input text is never modified. Instead, our normalisation strategy focuses on two axes: Only the NORM attribute is modified by the normalizer pipeline ; Pipelines (eg the pollution pipeline) can mark tokens as excluded by setting the extension Token._.excluded to True . It enables downstream matchers to skip excluded tokens. The normaliser can act on the input text in four dimensions : Move the text to lowercase . Remove accents . We use a deterministic approach to avoid modifying the character-length of the text, which helps for RegEx matching. Normalize apostrophes and quotation marks , which are often coded using special characters. Remove pollutions . Note We recommend you also add an end-of-line classifier to remove excess new line characters (introduced by the PDF layout). We provide a endlines pipeline, which requires training an unsupervised model. Refer to the dedicated page for more information .","title":"Normalisation"},{"location":"pipelines/core/normalisation/#usage","text":"The normalisation is handled by the single eds.normalizer pipeline. The following code snippet is complete, and should run as is. import spacy from edsnlp.matchers.utils import get_text nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" ) # Notice the special character used for the apostrophe and the quotes text = \"Le patient est admis \u00e0 l'h\u00f4pital le 23 ao\u00fbt 2021 pour une douleur \u02baaffreuse\u201d \u00e0 l`estomac.\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" ) # Out: le patient est admis a l'hopital le 23 aout 2021 pour une douleur \"affreuse\" a l'estomac","title":"Usage"},{"location":"pipelines/core/normalisation/#utilities","text":"To simplify the use of the normalisation output, we provide the get_text utility function. It computes the textual representation for a Span or Doc object. Moreover, every span exposes a normalized_variant extension getter, which computes the normalised representation of an entity on the fly.","title":"Utilities"},{"location":"pipelines/core/normalisation/#configuration","text":"Parameter Explanation Default accents Whether to strip accents True lowercase Whether to remove casing True quotes Whether to normalise quotes True pollution Whether to tag pollutions as excluded tokens True","title":"Configuration"},{"location":"pipelines/core/normalisation/#pipelines","text":"Let's review each subcomponent.","title":"Pipelines"},{"location":"pipelines/core/normalisation/#lowercase","text":"The eds.lowercase pipeline transforms every token to lowercase. It is not configurable. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = True , accents = False , quotes = False , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" ) # Out: pneumopathie \u00e0 nbnbwbwbnbwbnbnbnbwbw `coronavirus'","title":"Lowercase"},{"location":"pipelines/core/normalisation/#accents","text":"The eds.accents pipeline removes accents. To avoid edge cases, the component uses a specified list of accentuated characters and their unaccented representation, making it more predictable than using a library such as unidecode . Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = True , quotes = False , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" ) # Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus'","title":"Accents"},{"location":"pipelines/core/normalisation/#apostrophes-and-quotation-marks","text":"Apostrophes and quotation marks can be encoded using unpredictable special characters. The eds.quotes component transforms every such special character to ' and \" , respectively. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = False , quotes = True , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" ) # Out: Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW 'coronavirus'","title":"Apostrophes and quotation marks"},{"location":"pipelines/core/normalisation/#pollution","text":"The pollution pipeline uses a set of regular expressions to detect pollutions (irrelevant non-medical text that hinders text processing). Corresponding tokens are marked as excluded (by setting Token._.excluded to True ), enabling the use of the phrase matcher. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = True , quotes = False , pollution = True , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" ) # Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus' get_text ( doc , attr = \"TEXT\" , ignore_excluded = True ) # Out: Pneumopathie \u00e0 `coronavirus' This example above shows that the normalisation scheme works on two axes: non-destructive text modification and exclusion of tokens. The two are independent: a matcher can use the NORM attribute but keep excluded tokens, and conversely, match on TEXT while ignoring excluded tokens.","title":"Pollution"},{"location":"pipelines/core/normalisation/#authors-and-citation","text":"The eds.normalizer pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/core/sentences/","text":"Sentences The eds.sentences pipeline provides an alternative to spaCy's default sentencizer , aiming to overcome some of its limitations. Indeed, the sentencizer merely looks at period characters to detect the end of a sentence, a strategy that often fails in a clinical note settings. Our sentences component also classifies end-of-lines as sentence boundaries if the subsequent token begins with an uppercase character, leading to slightly better performances. Moreover, the eds.sentences pipeline can use the output of the eds.normalizer pipeline, and more specifically the end-of-line classification. This is activated by default. Usage EDS-NLP spaCy sentencizer import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac \\n \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\" ) doc = nlp ( text ) for sentence in doc . sents : print ( \"<s>\" , sentence , \"</s>\" ) # Out: <s> Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac # Out: <\\s> # Out: <s> Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. <\\s> import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"sentencizer\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac \\n \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\" ) doc = nlp ( text ) for sentence in doc . sents : print ( \"<s>\" , sentence , \"</s>\" ) # Out: <s> Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac # Out: Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. <\\s> Notice how EDS-NLP's implementation is more robust to ill-defined sentence endings. Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default punct_chars Punctuation patterns None (use pre-defined patterns) use_endlines Whether to use endlines prediction (see documentation ) True Authors and citation The eds.sentences pipeline was developed by AP-HP's Data Science team.","title":"Sentences"},{"location":"pipelines/core/sentences/#sentences","text":"The eds.sentences pipeline provides an alternative to spaCy's default sentencizer , aiming to overcome some of its limitations. Indeed, the sentencizer merely looks at period characters to detect the end of a sentence, a strategy that often fails in a clinical note settings. Our sentences component also classifies end-of-lines as sentence boundaries if the subsequent token begins with an uppercase character, leading to slightly better performances. Moreover, the eds.sentences pipeline can use the output of the eds.normalizer pipeline, and more specifically the end-of-line classification. This is activated by default.","title":"Sentences"},{"location":"pipelines/core/sentences/#usage","text":"EDS-NLP spaCy sentencizer import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac \\n \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\" ) doc = nlp ( text ) for sentence in doc . sents : print ( \"<s>\" , sentence , \"</s>\" ) # Out: <s> Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac # Out: <\\s> # Out: <s> Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. <\\s> import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"sentencizer\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac \\n \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\" ) doc = nlp ( text ) for sentence in doc . sents : print ( \"<s>\" , sentence , \"</s>\" ) # Out: <s> Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac # Out: Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. <\\s> Notice how EDS-NLP's implementation is more robust to ill-defined sentence endings.","title":"Usage"},{"location":"pipelines/core/sentences/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default punct_chars Punctuation patterns None (use pre-defined patterns) use_endlines Whether to use endlines prediction (see documentation ) True","title":"Configuration"},{"location":"pipelines/core/sentences/#authors-and-citation","text":"The eds.sentences pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/misc/","text":"Miscellaneous This section regroups pipelines that extract information that can be used by other components, but have little medical value in itself. For instance, the date detection and normalisation pipeline falls in this category.","title":"Miscellaneous"},{"location":"pipelines/misc/#miscellaneous","text":"This section regroups pipelines that extract information that can be used by other components, but have little medical value in itself. For instance, the date detection and normalisation pipeline falls in this category.","title":"Miscellaneous"},{"location":"pipelines/misc/consultation-dates/","text":"Consultation Dates This pipeline consists of two main parts: A matcher which finds mentions of consultation events (more details below) A date parser (see the corresponding pipeline) that links a date to those events Usage Note It is designed to work ONLY on consultation notes ( CR-CONS ), so please filter accordingly before proceeding. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" , config = dict ( lowercase = True , accents = True , quotes = True , pollution = False , ), ) nlp . add_pipe ( \"eds.consultation_dates\" ) text = \"XXX \\n \" \"Objet : Compte-Rendu de Consultation du 03/10/2018. \\n \" \"XXX \" doc = nlp ( text ) doc . spans [ \"consultation_dates\" ] # Out: [Consultation du 03/10/2018] doc . spans [ \"consultation_dates\" ][ 0 ] . _ . consultation_date # Out: datetime.datetime(2018, 10, 3, 0, 0) Consultation events Three main families of terms are available by default to extract those events. The consultation_mention terms This list contains terms directly referring to consultations, such as \" Consultation du... \" or \" Compte rendu du... \". This list is the only one activated by default since it is fairly precise an not error-prone. The town_mention terms This list contains the towns of each AP-HP's hospital. Its goal is to fetch dates mentioned as \" Paris, le 13 d\u00e9cembre 2015 \". It has a high recall but poor precision, since those dates can often be dates of letter redaction instead of consultation dates. The document_date_mention terms This list contains expressions mentioning the date of creation/edition of a document, such as \" Date du rapport: 13/12/2015 \" or \" Sign\u00e9 le 13/12/2015 \". As for town_mention , it has a high recall but is prone to errors since document date and consultation date aren't necessary similar. Note By default, only the consultation_mention are used Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default consultation_mention Whether to use consultation patterns, or list of patterns True (use pre-defined patterns) town_mention Whether to use town patterns, or list of patterns False document_date_mention Whether to use document date patterns, or list of patterns False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" Declared extensions The eds.consultation_dates pipeline declares one spaCy extensions on the Span object: the consultation_date attribute, which is a Python datetime object. Authors and citation The eds.consultation_dates pipeline was developed by AP-HP's Data Science team.","title":"Consultation Dates"},{"location":"pipelines/misc/consultation-dates/#consultation-dates","text":"This pipeline consists of two main parts: A matcher which finds mentions of consultation events (more details below) A date parser (see the corresponding pipeline) that links a date to those events","title":"Consultation Dates"},{"location":"pipelines/misc/consultation-dates/#usage","text":"Note It is designed to work ONLY on consultation notes ( CR-CONS ), so please filter accordingly before proceeding. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" , config = dict ( lowercase = True , accents = True , quotes = True , pollution = False , ), ) nlp . add_pipe ( \"eds.consultation_dates\" ) text = \"XXX \\n \" \"Objet : Compte-Rendu de Consultation du 03/10/2018. \\n \" \"XXX \" doc = nlp ( text ) doc . spans [ \"consultation_dates\" ] # Out: [Consultation du 03/10/2018] doc . spans [ \"consultation_dates\" ][ 0 ] . _ . consultation_date # Out: datetime.datetime(2018, 10, 3, 0, 0)","title":"Usage"},{"location":"pipelines/misc/consultation-dates/#consultation-events","text":"Three main families of terms are available by default to extract those events.","title":"Consultation events"},{"location":"pipelines/misc/consultation-dates/#the-consultation_mention-terms","text":"This list contains terms directly referring to consultations, such as \" Consultation du... \" or \" Compte rendu du... \". This list is the only one activated by default since it is fairly precise an not error-prone.","title":"The consultation_mention terms"},{"location":"pipelines/misc/consultation-dates/#the-town_mention-terms","text":"This list contains the towns of each AP-HP's hospital. Its goal is to fetch dates mentioned as \" Paris, le 13 d\u00e9cembre 2015 \". It has a high recall but poor precision, since those dates can often be dates of letter redaction instead of consultation dates.","title":"The town_mention terms"},{"location":"pipelines/misc/consultation-dates/#the-document_date_mention-terms","text":"This list contains expressions mentioning the date of creation/edition of a document, such as \" Date du rapport: 13/12/2015 \" or \" Sign\u00e9 le 13/12/2015 \". As for town_mention , it has a high recall but is prone to errors since document date and consultation date aren't necessary similar. Note By default, only the consultation_mention are used","title":"The document_date_mention terms"},{"location":"pipelines/misc/consultation-dates/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default consultation_mention Whether to use consultation patterns, or list of patterns True (use pre-defined patterns) town_mention Whether to use town patterns, or list of patterns False document_date_mention Whether to use document date patterns, or list of patterns False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\"","title":"Configuration"},{"location":"pipelines/misc/consultation-dates/#declared-extensions","text":"The eds.consultation_dates pipeline declares one spaCy extensions on the Span object: the consultation_date attribute, which is a Python datetime object.","title":"Declared extensions"},{"location":"pipelines/misc/consultation-dates/#authors-and-citation","text":"The eds.consultation_dates pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/misc/dates/","text":"Dates The eds.dates pipeline's role is to detect and normalise dates within a medical document. We use simple regular expressions to extract date mentions, and apply the dateparser library for the normalisation. Warning The dates pipeline is still in active development and has not been rigorously validated. If you come across a date expression that goes undetected, please file an issue ! Scope The eds.dates pipeline finds absolute (eg 23/08/2021 ) and relative (eg hier , la semaine derni\u00e8re ) dates alike. If the date of edition (via the doc._.note_datetime extension) is available, relative (and \"year-less\") dates will be normalised using the latter as base. On the other hand, if the base is unknown, the normalisation will follow the pattern : TD\u00b1<number-of-days> , positive values meaning that the relative date mentions the future ( dans trois jours ). Since the extension doc._.note_datetime cannot be set before applying the dates pipeline, we defer the normalisation step until the span._.dates attribute is accessed. See the tutorial for a presentation of a full pipeline featuring the eds.dates component. Usage import spacy from datetime import datetime nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.dates\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac. \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a un an.\" ) doc = nlp ( text ) dates = doc . spans [ \"dates\" ] dates # Out: [23 ao\u00fbt 2021, il y a un an] dates [ 0 ] . _ . date # Out: \"2021-08-23\" dates [ 1 ] . _ . date # Out: \"TD-365\" doc . _ . note_datetime = datetime ( 2021 , 8 , 27 ) dates [ 1 ] . _ . date # Out: \"2020-08-27\" Declared extensions The eds.dates pipeline declares two spaCy extensions on the Span object : The date_parsed attribute is a Python datetime object, used internally by the pipeline. The date attribute is a property that displays a normalised human-readable string for the date. Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default no_year Date patterns without year, eg le 5 ao\u00fbt None (use pre-defined patterns) year_only Date patterns with only the year, eg en 2018 None (use pre-defined patterns) no_day Date patterns without day, eg en mars 2018 None (use pre-defined patterns) absolute Absolute date patterns, eg le 5 ao\u00fbt 2020 None (use pre-defined patterns) relative Relative date patterns, eg hier ) None (use pre-defined patterns) full Full date patterns, eg 2020-10-23 None (use pre-defined patterns) current \"Current\" date patterns, eg ce jour None (use pre-defined patterns) false_positive Some false positive patterns to exclude None (use pre-defined patterns) on_ents_only Whether to look for dates around entities only False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" Authors and citation The eds.dates pipeline was developed by AP-HP's Data Science team.","title":"Dates"},{"location":"pipelines/misc/dates/#dates","text":"The eds.dates pipeline's role is to detect and normalise dates within a medical document. We use simple regular expressions to extract date mentions, and apply the dateparser library for the normalisation. Warning The dates pipeline is still in active development and has not been rigorously validated. If you come across a date expression that goes undetected, please file an issue !","title":"Dates"},{"location":"pipelines/misc/dates/#scope","text":"The eds.dates pipeline finds absolute (eg 23/08/2021 ) and relative (eg hier , la semaine derni\u00e8re ) dates alike. If the date of edition (via the doc._.note_datetime extension) is available, relative (and \"year-less\") dates will be normalised using the latter as base. On the other hand, if the base is unknown, the normalisation will follow the pattern : TD\u00b1<number-of-days> , positive values meaning that the relative date mentions the future ( dans trois jours ). Since the extension doc._.note_datetime cannot be set before applying the dates pipeline, we defer the normalisation step until the span._.dates attribute is accessed. See the tutorial for a presentation of a full pipeline featuring the eds.dates component.","title":"Scope"},{"location":"pipelines/misc/dates/#usage","text":"import spacy from datetime import datetime nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.dates\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac. \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a un an.\" ) doc = nlp ( text ) dates = doc . spans [ \"dates\" ] dates # Out: [23 ao\u00fbt 2021, il y a un an] dates [ 0 ] . _ . date # Out: \"2021-08-23\" dates [ 1 ] . _ . date # Out: \"TD-365\" doc . _ . note_datetime = datetime ( 2021 , 8 , 27 ) dates [ 1 ] . _ . date # Out: \"2020-08-27\"","title":"Usage"},{"location":"pipelines/misc/dates/#declared-extensions","text":"The eds.dates pipeline declares two spaCy extensions on the Span object : The date_parsed attribute is a Python datetime object, used internally by the pipeline. The date attribute is a property that displays a normalised human-readable string for the date.","title":"Declared extensions"},{"location":"pipelines/misc/dates/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default no_year Date patterns without year, eg le 5 ao\u00fbt None (use pre-defined patterns) year_only Date patterns with only the year, eg en 2018 None (use pre-defined patterns) no_day Date patterns without day, eg en mars 2018 None (use pre-defined patterns) absolute Absolute date patterns, eg le 5 ao\u00fbt 2020 None (use pre-defined patterns) relative Relative date patterns, eg hier ) None (use pre-defined patterns) full Full date patterns, eg 2020-10-23 None (use pre-defined patterns) current \"Current\" date patterns, eg ce jour None (use pre-defined patterns) false_positive Some false positive patterns to exclude None (use pre-defined patterns) on_ents_only Whether to look for dates around entities only False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\"","title":"Configuration"},{"location":"pipelines/misc/dates/#authors-and-citation","text":"The eds.dates pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/misc/reason/","text":"Reason The eds.reason pipeline uses a rule-based algorithm to detect spans that relate to the reason of the hospitalisation. It was designed at AP-HP's EDS. Usage The following snippet matches a simple terminology, and looks for spans of hospitalisation reasons. It is complete and can be run as is . import spacy text = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018 MOTIF D'HOSPITALISATION Monsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978, a \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. ANT\u00c9C\u00c9DENTS Ant\u00e9c\u00e9dents m\u00e9dicaux : Premier \u00e9pisode d'asthme en mai 2018.\"\"\" nlp = spacy . blank ( \"fr\" ) # Extraction of entities nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( respiratoire = [ \"asthmatique\" , \"asthme\" , \"toux\" , ] ) ), ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.reason\" , config = dict ( use_sections = True )) doc = nlp ( text ) reason = doc . spans [ \"reasons\" ][ 0 ] reason # Out: 'hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.' reason . _ . is_reason # Out: True entities = reason . _ . ents_reason entities # Out: [asthme] entities [ 0 ] . label_ # Out: 'respiratoire' ent = entities [ 0 ] ent . _ . is_reason # Out: True Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default reasons Reasons patterns None (use pre-defined patterns) attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" use_sections Whether to use sections False ignore_excluded Whether to ignore excluded tokens False Declared extensions The eds.reason pipeline adds the key reasons to doc.spans and declares one spaCy extension , on the Span objects called ents_reason . The ents_reason extension is a list of named entities that overlap the Span , typically entities found in previous pipelines like matcher . It also declares the boolean extension is_reason . This extension is set to True for the Reason Spans but also for the entities that overlap the reason span. Authors and citation The eds.reason pipeline was developed by AP-HP's Data Science team.","title":"Reason"},{"location":"pipelines/misc/reason/#reason","text":"The eds.reason pipeline uses a rule-based algorithm to detect spans that relate to the reason of the hospitalisation. It was designed at AP-HP's EDS.","title":"Reason"},{"location":"pipelines/misc/reason/#usage","text":"The following snippet matches a simple terminology, and looks for spans of hospitalisation reasons. It is complete and can be run as is . import spacy text = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018 MOTIF D'HOSPITALISATION Monsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978, a \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. ANT\u00c9C\u00c9DENTS Ant\u00e9c\u00e9dents m\u00e9dicaux : Premier \u00e9pisode d'asthme en mai 2018.\"\"\" nlp = spacy . blank ( \"fr\" ) # Extraction of entities nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( respiratoire = [ \"asthmatique\" , \"asthme\" , \"toux\" , ] ) ), ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.reason\" , config = dict ( use_sections = True )) doc = nlp ( text ) reason = doc . spans [ \"reasons\" ][ 0 ] reason # Out: 'hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.' reason . _ . is_reason # Out: True entities = reason . _ . ents_reason entities # Out: [asthme] entities [ 0 ] . label_ # Out: 'respiratoire' ent = entities [ 0 ] ent . _ . is_reason # Out: True","title":"Usage"},{"location":"pipelines/misc/reason/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default reasons Reasons patterns None (use pre-defined patterns) attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" use_sections Whether to use sections False ignore_excluded Whether to ignore excluded tokens False","title":"Configuration"},{"location":"pipelines/misc/reason/#declared-extensions","text":"The eds.reason pipeline adds the key reasons to doc.spans and declares one spaCy extension , on the Span objects called ents_reason . The ents_reason extension is a list of named entities that overlap the Span , typically entities found in previous pipelines like matcher . It also declares the boolean extension is_reason . This extension is set to True for the Reason Spans but also for the entities that overlap the reason span.","title":"Declared extensions"},{"location":"pipelines/misc/reason/#authors-and-citation","text":"The eds.reason pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/misc/sections/","text":"Sections Detected sections are : allergies ant\u00e9c\u00e9dents ant\u00e9c\u00e9dents familiaux traitements entr\u00e9e conclusion conclusion entr\u00e9e habitus correspondants diagnostic donn\u00e9es biom\u00e9triques entr\u00e9e examens examens compl\u00e9mentaires facteurs de risques histoire de la maladie actes motif prescriptions traitements sortie The pipeline extracts section title. A \"section\" is then defined as the span of text between two titles. Use at your own risks Should you rely on eds.sections for critical downstream tasks, make sure to validate the pipeline to make sure that the component works. For instance, the eds.history pipeline can use sections to make its predictions, but that possibility is deactivated by default. Usage The following snippet detects section titles. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.sections\" ) text = \"CRU du 10/09/2021 \\n \" \"Motif : \\n \" \"Patient admis pour suspicion de COVID\" doc = nlp ( text ) doc . spans [ \"section_titles\" ] # Out: [Motif :] Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default sections Sections patterns None (use pre-defined patterns) add_patterns Whether add endlines patterns False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" ignore_excluded Whether to ignore excluded tokens True Declared extensions The eds.sections pipeline adds two fields to the doc.spans attribute : The section_titles key contains the list of all section titles extracted using the list declared in the terms.py module. The sections key contains a list of sections, ie spans of text between two section titles (or the last title and the end of the document). Authors and citation The eds.sections pipeline was developed by AP-HP's Data Science team.","title":"Sections"},{"location":"pipelines/misc/sections/#sections","text":"Detected sections are : allergies ant\u00e9c\u00e9dents ant\u00e9c\u00e9dents familiaux traitements entr\u00e9e conclusion conclusion entr\u00e9e habitus correspondants diagnostic donn\u00e9es biom\u00e9triques entr\u00e9e examens examens compl\u00e9mentaires facteurs de risques histoire de la maladie actes motif prescriptions traitements sortie The pipeline extracts section title. A \"section\" is then defined as the span of text between two titles. Use at your own risks Should you rely on eds.sections for critical downstream tasks, make sure to validate the pipeline to make sure that the component works. For instance, the eds.history pipeline can use sections to make its predictions, but that possibility is deactivated by default.","title":"Sections"},{"location":"pipelines/misc/sections/#usage","text":"The following snippet detects section titles. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.sections\" ) text = \"CRU du 10/09/2021 \\n \" \"Motif : \\n \" \"Patient admis pour suspicion de COVID\" doc = nlp ( text ) doc . spans [ \"section_titles\" ] # Out: [Motif :]","title":"Usage"},{"location":"pipelines/misc/sections/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default sections Sections patterns None (use pre-defined patterns) add_patterns Whether add endlines patterns False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" ignore_excluded Whether to ignore excluded tokens True","title":"Configuration"},{"location":"pipelines/misc/sections/#declared-extensions","text":"The eds.sections pipeline adds two fields to the doc.spans attribute : The section_titles key contains the list of all section titles extracted using the list declared in the terms.py module. The sections key contains a list of sections, ie spans of text between two section titles (or the last title and the end of the document).","title":"Declared extensions"},{"location":"pipelines/misc/sections/#authors-and-citation","text":"The eds.sections pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/ner/","text":"Named entity recognition We provide a few Named Entity Recognition (NER) pipelines.","title":"Named entity recognition"},{"location":"pipelines/ner/#named-entity-recognition","text":"We provide a few Named Entity Recognition (NER) pipelines.","title":"Named entity recognition"},{"location":"pipelines/ner/score/","text":"Score The eds.score pipeline allows easy extraction of typical scores (Charlson, SOFA...) that can be found in clinical documents. The pipeline works by Extracting the score's name via the provided regular expressions Extracting the score's raw value via another set of RegEx Normalising the score's value via a normalising function Charlson Comorbidity Index Implementing the eds.score pipeline, the charlson pipeline will extract the Charlson Comorbidity Index : import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.charlson\" ) text = \"Charlson \u00e0 l'admission: 7. \\n \" \"Charlson: \\n \" \"OMS: \\n \" doc = nlp ( text ) doc . ents # Out: (Charlson,) We can see that only one occurrence was extracted. ent = doc . ents [ 0 ] ent . start , ent . end # Out: 0, 1 The second mention of Charlson in the text doesn't contain any numerical value, so it isn't extracted. Each extraction exposes 2 extensions: ent . _ . score_name # Out: 'charlson' ent . _ . score_value # Out: 7 SOFA score The SOFA pipe allows to extract SOFA scores. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.SOFA\" ) text = \"SOFA (\u00e0 24H) : 12. \\n \" \"OMS: \\n \" doc = nlp ( text ) doc . ents # Out: (SOFA,) Each extraction exposes 3 extensions: ent . _ . score_name # Out: 'SOFA' ent . _ . score_value # Out: 12 ent . _ . score_method # Out: \"24H\" Score method can here be \"24H\", \"Maximum\", \"A l'admission\" or \"Non pr\u00e9cis\u00e9e\" Implementing your own score Using the eds.score pipeline, you only have to change its configuration in order to implement a simple score extraction algorithm. As an example, let us see the configuration used for the eds.charlson pipe The configuration consists of 4 items: score_name : The name of the score regex : A list of regular expression to detect the score's mention after_extract : A regular expression to extract the score's value after the score's mention score_normalization : A function name used to normalise the score's raw value Note spaCy doesn't allow to pass functions in the configuration of a pipeline. To circumvent this issue, functions need to be registered, which simply consists in decorating those functions The registration is done as follows: @spacy . registry . misc ( \"score_normalization.charlson\" ) def my_normalization_score ( raw_score : str ): # Implement some filtering here # Return None if you want the score to be discarded return normalized_score The values used for the eds.charlson pipe are the following: @spacy . registry . misc ( \"score_normalization.charlson\" ) def score_normalization ( extracted_score ): \"\"\" Charlson score normalization. If available, returns the integer value of the Charlson score. \"\"\" score_range = list ( range ( 0 , 30 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score ) charlson_config = dict ( score_name = \"charlson\" , regex = [ r \"charlson\" ], after_extract = r \"charlson.*[\\n\\W]*(\\d+)\" , score_normalization = \"score_normalization.charlson\" , )","title":"Score"},{"location":"pipelines/ner/score/#score","text":"The eds.score pipeline allows easy extraction of typical scores (Charlson, SOFA...) that can be found in clinical documents. The pipeline works by Extracting the score's name via the provided regular expressions Extracting the score's raw value via another set of RegEx Normalising the score's value via a normalising function","title":"Score"},{"location":"pipelines/ner/score/#charlson-comorbidity-index","text":"Implementing the eds.score pipeline, the charlson pipeline will extract the Charlson Comorbidity Index : import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.charlson\" ) text = \"Charlson \u00e0 l'admission: 7. \\n \" \"Charlson: \\n \" \"OMS: \\n \" doc = nlp ( text ) doc . ents # Out: (Charlson,) We can see that only one occurrence was extracted. ent = doc . ents [ 0 ] ent . start , ent . end # Out: 0, 1 The second mention of Charlson in the text doesn't contain any numerical value, so it isn't extracted. Each extraction exposes 2 extensions: ent . _ . score_name # Out: 'charlson' ent . _ . score_value # Out: 7","title":"Charlson Comorbidity Index"},{"location":"pipelines/ner/score/#sofa-score","text":"The SOFA pipe allows to extract SOFA scores. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.SOFA\" ) text = \"SOFA (\u00e0 24H) : 12. \\n \" \"OMS: \\n \" doc = nlp ( text ) doc . ents # Out: (SOFA,) Each extraction exposes 3 extensions: ent . _ . score_name # Out: 'SOFA' ent . _ . score_value # Out: 12 ent . _ . score_method # Out: \"24H\" Score method can here be \"24H\", \"Maximum\", \"A l'admission\" or \"Non pr\u00e9cis\u00e9e\"","title":"SOFA score"},{"location":"pipelines/ner/score/#implementing-your-own-score","text":"Using the eds.score pipeline, you only have to change its configuration in order to implement a simple score extraction algorithm. As an example, let us see the configuration used for the eds.charlson pipe The configuration consists of 4 items: score_name : The name of the score regex : A list of regular expression to detect the score's mention after_extract : A regular expression to extract the score's value after the score's mention score_normalization : A function name used to normalise the score's raw value Note spaCy doesn't allow to pass functions in the configuration of a pipeline. To circumvent this issue, functions need to be registered, which simply consists in decorating those functions The registration is done as follows: @spacy . registry . misc ( \"score_normalization.charlson\" ) def my_normalization_score ( raw_score : str ): # Implement some filtering here # Return None if you want the score to be discarded return normalized_score The values used for the eds.charlson pipe are the following: @spacy . registry . misc ( \"score_normalization.charlson\" ) def score_normalization ( extracted_score ): \"\"\" Charlson score normalization. If available, returns the integer value of the Charlson score. \"\"\" score_range = list ( range ( 0 , 30 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score ) charlson_config = dict ( score_name = \"charlson\" , regex = [ r \"charlson\" ], after_extract = r \"charlson.*[\\n\\W]*(\\d+)\" , score_normalization = \"score_normalization.charlson\" , )","title":"Implementing your own score"},{"location":"pipelines/qualifiers/","text":"Qualifier overview In EDS-NLP, we call qualifiers the suite of pipelines designed to qualify a pre-extracted entity for a linguistic modality. Available pipelines Name Description eds.negation Detect negated entities eds.family Detect entities that pertain to a patient's kin rather than themself eds.hypothesis Detect entities subject to speculation eds.reported_speech Detect entities that are quoted from the patient eds.history Detect entities that pertain to the patient's history Rationale In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents. Now, consider the following example: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort. Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. To curb this issue, EDS-NLP proposes rule-based pipelines that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort. Under the hood Our qualifier pipelines all follow the same basic pattern: The pipeline extracts cues. We define three (possibly overlapping) kinds : preceding , ie cues that precede modulated entities ; following , ie cues that follow modulated entities ; in some cases, verbs , ie verbs that convey a modulation (treated as preceding cues). The pipeline splits the text between sentences and propositions, using annotations from a sentencizer pipeline and termination patterns, which define syntagma/proposition terminations. For each pre-extracted entity, the pipeline checks whether there is a cue between the start of the syntagma and the start of the entity, or a following cue between the end of the entity and the end of the proposition. Albeit simple, this algorithm can achieve very good performance depending on the modality. For instance, our eds.negation pipeline reaches 88% F1-score on our dataset. Dealing with pseudo-cues The pipeline can also detect pseudo-cues , ie phrases that contain cues but that are not cues themselves . For instance: sans doute / without doubt contains sans/without , but does not convey negation. Detecting pseudo-cues lets the pipeline filter out any cue that overlaps with a pseudo-cue. Sentence boundaries are required The rule-based algorithm detects cues, and propagate their modulation on the rest of the syntagma . For that reason, a qualifier pipeline needs a sentencizer component to be defined, and will fail otherwise. You may use EDS-NLP's: nlp . add_pipe ( \"eds.sentences\" ) Persisting the results Our qualifier pipelines write their results to a custom spaCy extension , defined on both Span and Token objects. We follow the convention of naming said attribute after the pipeline itself, eg Span._.negation for the eds.negation pipeline. In most cases, that extension is a boolean. We also provide a string representation of the result, computed on the fly by declaring a getter that reads the boolean result of the pipeline. Following spaCy convention, we give this attribute the same name, followed by a _ .","title":"Qualifier overview"},{"location":"pipelines/qualifiers/#qualifier-overview","text":"In EDS-NLP, we call qualifiers the suite of pipelines designed to qualify a pre-extracted entity for a linguistic modality.","title":"Qualifier overview"},{"location":"pipelines/qualifiers/#available-pipelines","text":"Name Description eds.negation Detect negated entities eds.family Detect entities that pertain to a patient's kin rather than themself eds.hypothesis Detect entities subject to speculation eds.reported_speech Detect entities that are quoted from the patient eds.history Detect entities that pertain to the patient's history","title":"Available pipelines"},{"location":"pipelines/qualifiers/#rationale","text":"In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents. Now, consider the following example: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort. Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. To curb this issue, EDS-NLP proposes rule-based pipelines that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort.","title":"Rationale"},{"location":"pipelines/qualifiers/#under-the-hood","text":"Our qualifier pipelines all follow the same basic pattern: The pipeline extracts cues. We define three (possibly overlapping) kinds : preceding , ie cues that precede modulated entities ; following , ie cues that follow modulated entities ; in some cases, verbs , ie verbs that convey a modulation (treated as preceding cues). The pipeline splits the text between sentences and propositions, using annotations from a sentencizer pipeline and termination patterns, which define syntagma/proposition terminations. For each pre-extracted entity, the pipeline checks whether there is a cue between the start of the syntagma and the start of the entity, or a following cue between the end of the entity and the end of the proposition. Albeit simple, this algorithm can achieve very good performance depending on the modality. For instance, our eds.negation pipeline reaches 88% F1-score on our dataset. Dealing with pseudo-cues The pipeline can also detect pseudo-cues , ie phrases that contain cues but that are not cues themselves . For instance: sans doute / without doubt contains sans/without , but does not convey negation. Detecting pseudo-cues lets the pipeline filter out any cue that overlaps with a pseudo-cue. Sentence boundaries are required The rule-based algorithm detects cues, and propagate their modulation on the rest of the syntagma . For that reason, a qualifier pipeline needs a sentencizer component to be defined, and will fail otherwise. You may use EDS-NLP's: nlp . add_pipe ( \"eds.sentences\" )","title":"Under the hood"},{"location":"pipelines/qualifiers/#persisting-the-results","text":"Our qualifier pipelines write their results to a custom spaCy extension , defined on both Span and Token objects. We follow the convention of naming said attribute after the pipeline itself, eg Span._.negation for the eds.negation pipeline. In most cases, that extension is a boolean. We also provide a string representation of the result, computed on the fly by declaring a getter that reads the boolean result of the pipeline. Following spaCy convention, we give this attribute the same name, followed by a _ .","title":"Persisting the results"},{"location":"pipelines/qualifiers/family/","text":"Family The eds.family pipeline uses a simple rule-based algorithm to detect spans that describe a family member (or family history) of the patient rather than the patient themself. Usage The following snippet matches a simple terminology, and checks the family context of the extracted entities. It is complete, and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , ostheoporose = \"osth\u00e9oporose\" )), ) nlp . add_pipe ( \"eds.family\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Il a des ant\u00e9c\u00e9dents familiaux d'osth\u00e9oporose\" ) doc = nlp ( text ) doc . ents # Out: [douleur, osth\u00e9oporose] doc . ents [ 0 ] . _ . family # Out: False doc . ents [ 1 ] . _ . family # Out: True Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" family Family patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) use_sections Whether to use pre-annotated sections (requires the sections pipeline) False on_ents_only Whether to qualify pre-extracted entities only True explain Whether to keep track of the cues for each entity False Declared extensions The eds.family pipeline declares two spaCy extensions , on both Span and Token objects : The family attribute is a boolean, set to True if the pipeline predicts that the span/token relates to a family member. The family_ property is a human-readable string, computed from the family attribute. It implements a simple getter function that outputs PATIENT or FAMILY , depending on the value of family . Authors and citation The eds.family pipeline was developed by AP-HP's Data Science team.","title":"Family"},{"location":"pipelines/qualifiers/family/#family","text":"The eds.family pipeline uses a simple rule-based algorithm to detect spans that describe a family member (or family history) of the patient rather than the patient themself.","title":"Family"},{"location":"pipelines/qualifiers/family/#usage","text":"The following snippet matches a simple terminology, and checks the family context of the extracted entities. It is complete, and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , ostheoporose = \"osth\u00e9oporose\" )), ) nlp . add_pipe ( \"eds.family\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Il a des ant\u00e9c\u00e9dents familiaux d'osth\u00e9oporose\" ) doc = nlp ( text ) doc . ents # Out: [douleur, osth\u00e9oporose] doc . ents [ 0 ] . _ . family # Out: False doc . ents [ 1 ] . _ . family # Out: True","title":"Usage"},{"location":"pipelines/qualifiers/family/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" family Family patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) use_sections Whether to use pre-annotated sections (requires the sections pipeline) False on_ents_only Whether to qualify pre-extracted entities only True explain Whether to keep track of the cues for each entity False","title":"Configuration"},{"location":"pipelines/qualifiers/family/#declared-extensions","text":"The eds.family pipeline declares two spaCy extensions , on both Span and Token objects : The family attribute is a boolean, set to True if the pipeline predicts that the span/token relates to a family member. The family_ property is a human-readable string, computed from the family attribute. It implements a simple getter function that outputs PATIENT or FAMILY , depending on the value of family .","title":"Declared extensions"},{"location":"pipelines/qualifiers/family/#authors-and-citation","text":"The eds.family pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/qualifiers/history/","text":"Medical History The eds.history pipeline uses a simple rule-based algorithm to detect spans that describe medical history rather than the diagnostic of a given visit. The mere definition of an medical history is not straightforward. Hence, this component only tags entities that are explicitly described as part of the medical history , eg preceded by a synonym of \"medical history\". This component may also use the output of the eds.sections pipeline . In that case, the entire ant\u00e9c\u00e9dent section is tagged as a medical history. Warning Be careful, the eds.sections component may oversize the ant\u00e9c\u00e9dents section. Indeed, it detects section titles and tags the entire text between a title and the next as a section. Hence, should a section title goes undetected after the ant\u00e9c\u00e9dents title, some parts of the document will erroneously be tagged as a medical history. To curb that possibility, using the output of the eds.sections component is deactivated by default. Usage The following snippet matches a simple terminology, and checks whether the extracted entities are history or not. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , malaise = \"malaises\" )), ) nlp . add_pipe ( \"eds.history\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Il a des ant\u00e9c\u00e9dents de malaises.\" ) doc = nlp ( text ) doc . ents # Out: [douleur, malaises] doc . ents [ 0 ] . _ . history # Out: False doc . ents [ 1 ] . _ . history # Out: True Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" history History patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) use_sections Whether to use pre-annotated sections (requires the sections pipeline) False on_ents_only Whether to qualify pre-extracted entities only True explain Whether to keep track of the cues for each entity False Declared extensions The eds.history pipeline declares two spaCy extensions , on both Span and Token objects : The history attribute is a boolean, set to True if the pipeline predicts that the span/token is a medical history. The history_ property is a human-readable string, computed from the history attribute. It implements a simple getter function that outputs CURRENT or ATCD , depending on the value of history . Authors and citation The eds.history pipeline was developed by AP-HP's Data Science team.","title":"Medical History"},{"location":"pipelines/qualifiers/history/#medical-history","text":"The eds.history pipeline uses a simple rule-based algorithm to detect spans that describe medical history rather than the diagnostic of a given visit. The mere definition of an medical history is not straightforward. Hence, this component only tags entities that are explicitly described as part of the medical history , eg preceded by a synonym of \"medical history\". This component may also use the output of the eds.sections pipeline . In that case, the entire ant\u00e9c\u00e9dent section is tagged as a medical history. Warning Be careful, the eds.sections component may oversize the ant\u00e9c\u00e9dents section. Indeed, it detects section titles and tags the entire text between a title and the next as a section. Hence, should a section title goes undetected after the ant\u00e9c\u00e9dents title, some parts of the document will erroneously be tagged as a medical history. To curb that possibility, using the output of the eds.sections component is deactivated by default.","title":"Medical History"},{"location":"pipelines/qualifiers/history/#usage","text":"The following snippet matches a simple terminology, and checks whether the extracted entities are history or not. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , malaise = \"malaises\" )), ) nlp . add_pipe ( \"eds.history\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Il a des ant\u00e9c\u00e9dents de malaises.\" ) doc = nlp ( text ) doc . ents # Out: [douleur, malaises] doc . ents [ 0 ] . _ . history # Out: False doc . ents [ 1 ] . _ . history # Out: True","title":"Usage"},{"location":"pipelines/qualifiers/history/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" history History patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) use_sections Whether to use pre-annotated sections (requires the sections pipeline) False on_ents_only Whether to qualify pre-extracted entities only True explain Whether to keep track of the cues for each entity False","title":"Configuration"},{"location":"pipelines/qualifiers/history/#declared-extensions","text":"The eds.history pipeline declares two spaCy extensions , on both Span and Token objects : The history attribute is a boolean, set to True if the pipeline predicts that the span/token is a medical history. The history_ property is a human-readable string, computed from the history attribute. It implements a simple getter function that outputs CURRENT or ATCD , depending on the value of history .","title":"Declared extensions"},{"location":"pipelines/qualifiers/history/#authors-and-citation","text":"The eds.history pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/qualifiers/hypothesis/","text":"Hypothesis The eds.hypothesis pipeline uses a simple rule-based algorithm to detect spans that are speculations rather than certain statements. Usage The following snippet matches a simple terminology, and checks whether the extracted entities are part of a speculation. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , fracture = \"fracture\" )), ) nlp . add_pipe ( \"eds.hypothesis\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Possible fracture du radius.\" ) doc = nlp ( text ) doc . ents # Out: [douleur, fracture] doc . ents [ 0 ] . _ . hypothesis # Out: False doc . ents [ 1 ] . _ . hypothesis # Out: True Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-hypothesis patterns None (use pre-defined patterns) preceding Preceding hypothesis patterns None (use pre-defined patterns) following Following hypothesis patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs_hyp Patterns for verbs that imply a hypothesis None (use pre-defined patterns) verbs_eds Common verb patterns, checked for conditional mode None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for hypothesis within entities False explain Whether to keep track of the cues for each entity False Declared extensions The eds.hypothesis pipeline declares two spaCy extensions , on both Span and Token objects : The hypothesis attribute is a boolean, set to True if the pipeline predicts that the span/token is a speculation. The hypothesis_ property is a human-readable string, computed from the hypothesis attribute. It implements a simple getter function that outputs HYP or CERT , depending on the value of hypothesis . Performance The pipeline's performance is measured on three datasets : The ESSAI 1 and CAS 2 datasets were developed at the CNRS. The two are concatenated. The NegParHyp corpus was specifically developed at EDS to test the pipeline on actual clinical notes, using pseudonymised notes from the EDS. Dataset Hypothesis F1 CAS/ESSAI 49% NegParHyp 52% NegParHyp corpus The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context. Authors and citation The eds.hypothesis pipeline was developed by AP-HP's Data Science team. Cl\u00e9ment Dalloux, Vincent Claveau, and Natalia Grabar. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. In SIIM 2017 - Symposium sur l'Ing\u00e9nierie de l'Information M\u00e9dicale , 1\u20138. Toulouse, France, November 2017. URL: https://hal.archives-ouvertes.fr/hal-01659637 . \u21a9 Natalia Grabar, Vincent Claveau, and Cl\u00e9ment Dalloux. CAS: French Corpus with Clinical Cases. In LOUHI 2018 - The Ninth International Workshop on Health Text Mining and Information Analysis , Ninth International Workshop on Health Text Mining and Information Analysis (LOUHI) Proceedings of the Workshop, 1\u20137. Bruxelles, France, October 2018. URL: https://hal.archives-ouvertes.fr/hal-01937096 . \u21a9","title":"Hypothesis"},{"location":"pipelines/qualifiers/hypothesis/#hypothesis","text":"The eds.hypothesis pipeline uses a simple rule-based algorithm to detect spans that are speculations rather than certain statements.","title":"Hypothesis"},{"location":"pipelines/qualifiers/hypothesis/#usage","text":"The following snippet matches a simple terminology, and checks whether the extracted entities are part of a speculation. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , fracture = \"fracture\" )), ) nlp . add_pipe ( \"eds.hypothesis\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Possible fracture du radius.\" ) doc = nlp ( text ) doc . ents # Out: [douleur, fracture] doc . ents [ 0 ] . _ . hypothesis # Out: False doc . ents [ 1 ] . _ . hypothesis # Out: True","title":"Usage"},{"location":"pipelines/qualifiers/hypothesis/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-hypothesis patterns None (use pre-defined patterns) preceding Preceding hypothesis patterns None (use pre-defined patterns) following Following hypothesis patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs_hyp Patterns for verbs that imply a hypothesis None (use pre-defined patterns) verbs_eds Common verb patterns, checked for conditional mode None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for hypothesis within entities False explain Whether to keep track of the cues for each entity False","title":"Configuration"},{"location":"pipelines/qualifiers/hypothesis/#declared-extensions","text":"The eds.hypothesis pipeline declares two spaCy extensions , on both Span and Token objects : The hypothesis attribute is a boolean, set to True if the pipeline predicts that the span/token is a speculation. The hypothesis_ property is a human-readable string, computed from the hypothesis attribute. It implements a simple getter function that outputs HYP or CERT , depending on the value of hypothesis .","title":"Declared extensions"},{"location":"pipelines/qualifiers/hypothesis/#performance","text":"The pipeline's performance is measured on three datasets : The ESSAI 1 and CAS 2 datasets were developed at the CNRS. The two are concatenated. The NegParHyp corpus was specifically developed at EDS to test the pipeline on actual clinical notes, using pseudonymised notes from the EDS. Dataset Hypothesis F1 CAS/ESSAI 49% NegParHyp 52% NegParHyp corpus The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.","title":"Performance"},{"location":"pipelines/qualifiers/hypothesis/#authors-and-citation","text":"The eds.hypothesis pipeline was developed by AP-HP's Data Science team. Cl\u00e9ment Dalloux, Vincent Claveau, and Natalia Grabar. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. In SIIM 2017 - Symposium sur l'Ing\u00e9nierie de l'Information M\u00e9dicale , 1\u20138. Toulouse, France, November 2017. URL: https://hal.archives-ouvertes.fr/hal-01659637 . \u21a9 Natalia Grabar, Vincent Claveau, and Cl\u00e9ment Dalloux. CAS: French Corpus with Clinical Cases. In LOUHI 2018 - The Ninth International Workshop on Health Text Mining and Information Analysis , Ninth International Workshop on Health Text Mining and Information Analysis (LOUHI) Proceedings of the Workshop, 1\u20137. Bruxelles, France, October 2018. URL: https://hal.archives-ouvertes.fr/hal-01937096 . \u21a9","title":"Authors and citation"},{"location":"pipelines/qualifiers/negation/","text":"Negation The eds.negation pipeline uses a simple rule-based algorithm to detect negated spans. It was designed at AP-HP's EDS, following the insights of the NegEx algorithm by Chapman et al 1 . Usage The following snippet matches a simple terminology, and checks the polarity of the extracted entities. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( patient = \"patient\" , fracture = \"fracture\" )), ) nlp . add_pipe ( \"eds.negation\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Le scanner ne d\u00e9tecte aucune fracture.\" ) doc = nlp ( text ) doc . ents # Out: [patient, fracture] doc . ents [ 0 ] . _ . negation # (1) # Out: False doc . ents [ 1 ] . _ . negation # Out: True The result of the pipeline is kept in the negation custom extension. Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-negation patterns None (use pre-defined patterns) preceding Preceding negation patterns None (use pre-defined patterns) following Following negation patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs Patterns for verbs that imply a negation None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for negations within entities False explain Whether to keep track of the cues for each entity False Declared extensions The eds.negation pipeline declares two spaCy extensions , on both Span and Token objects : The negation attribute is a boolean, set to True if the pipeline predicts that the span/token is negated. The negation_ property is a human-readable string, computed from the negation attribute. It implements a simple getter function that outputs AFF or NEG , depending on the value of negation . Performance The pipeline's performance is measured on three datasets : The ESSAI 2 and CAS 3 datasets were developed at the CNRS. The two are concatenated. The NegParHyp corpus was specifically developed at AP-HP to test the pipeline on actual clinical notes, using pseudonymised notes from the AP-HP. Dataset Negation F1 CAS/ESSAI 71% NegParHyp 88% NegParHyp corpus The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context. Authors and citation The eds.negation pipeline was developed by AP-HP's Data Science team. Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gregory F. Cooper, and Bruce G. Buchanan. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics , 34(5):301\u2013310, October 2001. URL: https://linkinghub.elsevier.com/retrieve/pii/S1532046401910299 (visited on 2020-12-31), doi:10.1006/jbin.2001.1029 . \u21a9 Cl\u00e9ment Dalloux, Vincent Claveau, and Natalia Grabar. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. In SIIM 2017 - Symposium sur l'Ing\u00e9nierie de l'Information M\u00e9dicale , 1\u20138. Toulouse, France, November 2017. URL: https://hal.archives-ouvertes.fr/hal-01659637 . \u21a9 Natalia Grabar, Vincent Claveau, and Cl\u00e9ment Dalloux. CAS: French Corpus with Clinical Cases. In LOUHI 2018 - The Ninth International Workshop on Health Text Mining and Information Analysis , Ninth International Workshop on Health Text Mining and Information Analysis (LOUHI) Proceedings of the Workshop, 1\u20137. Bruxelles, France, October 2018. URL: https://hal.archives-ouvertes.fr/hal-01937096 . \u21a9","title":"Negation"},{"location":"pipelines/qualifiers/negation/#negation","text":"The eds.negation pipeline uses a simple rule-based algorithm to detect negated spans. It was designed at AP-HP's EDS, following the insights of the NegEx algorithm by Chapman et al 1 .","title":"Negation"},{"location":"pipelines/qualifiers/negation/#usage","text":"The following snippet matches a simple terminology, and checks the polarity of the extracted entities. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( patient = \"patient\" , fracture = \"fracture\" )), ) nlp . add_pipe ( \"eds.negation\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Le scanner ne d\u00e9tecte aucune fracture.\" ) doc = nlp ( text ) doc . ents # Out: [patient, fracture] doc . ents [ 0 ] . _ . negation # (1) # Out: False doc . ents [ 1 ] . _ . negation # Out: True The result of the pipeline is kept in the negation custom extension.","title":"Usage"},{"location":"pipelines/qualifiers/negation/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-negation patterns None (use pre-defined patterns) preceding Preceding negation patterns None (use pre-defined patterns) following Following negation patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs Patterns for verbs that imply a negation None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for negations within entities False explain Whether to keep track of the cues for each entity False","title":"Configuration"},{"location":"pipelines/qualifiers/negation/#declared-extensions","text":"The eds.negation pipeline declares two spaCy extensions , on both Span and Token objects : The negation attribute is a boolean, set to True if the pipeline predicts that the span/token is negated. The negation_ property is a human-readable string, computed from the negation attribute. It implements a simple getter function that outputs AFF or NEG , depending on the value of negation .","title":"Declared extensions"},{"location":"pipelines/qualifiers/negation/#performance","text":"The pipeline's performance is measured on three datasets : The ESSAI 2 and CAS 3 datasets were developed at the CNRS. The two are concatenated. The NegParHyp corpus was specifically developed at AP-HP to test the pipeline on actual clinical notes, using pseudonymised notes from the AP-HP. Dataset Negation F1 CAS/ESSAI 71% NegParHyp 88% NegParHyp corpus The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.","title":"Performance"},{"location":"pipelines/qualifiers/negation/#authors-and-citation","text":"The eds.negation pipeline was developed by AP-HP's Data Science team. Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gregory F. Cooper, and Bruce G. Buchanan. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics , 34(5):301\u2013310, October 2001. URL: https://linkinghub.elsevier.com/retrieve/pii/S1532046401910299 (visited on 2020-12-31), doi:10.1006/jbin.2001.1029 . \u21a9 Cl\u00e9ment Dalloux, Vincent Claveau, and Natalia Grabar. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. In SIIM 2017 - Symposium sur l'Ing\u00e9nierie de l'Information M\u00e9dicale , 1\u20138. Toulouse, France, November 2017. URL: https://hal.archives-ouvertes.fr/hal-01659637 . \u21a9 Natalia Grabar, Vincent Claveau, and Cl\u00e9ment Dalloux. CAS: French Corpus with Clinical Cases. In LOUHI 2018 - The Ninth International Workshop on Health Text Mining and Information Analysis , Ninth International Workshop on Health Text Mining and Information Analysis (LOUHI) Proceedings of the Workshop, 1\u20137. Bruxelles, France, October 2018. URL: https://hal.archives-ouvertes.fr/hal-01937096 . \u21a9","title":"Authors and citation"},{"location":"pipelines/qualifiers/reported-speech/","text":"Reported Speech The eds.reported_speech pipeline uses a simple rule-based algorithm to detect spans that relate to reported speech (eg when the doctor quotes the patient). It was designed at AP-HP's EDS. Usage The following snippet matches a simple terminology, and checks whether the extracted entities are part of a reported speech. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( patient = \"patient\" , alcool = \"alcoolis\u00e9\" )), ) nlp . add_pipe ( \"eds.reported_speech\" ) text = ( \"Le patient est admis aux urgences ce soir pour une douleur au bras. \" \"Il nie \u00eatre alcoolis\u00e9.\" ) doc = nlp ( text ) doc . ents # Out: [patient, alcoolis\u00e9] doc . ents [ 0 ] . _ . reported_speech # Out: False doc . ents [ 1 ] . _ . reported_speech # Out: True Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-reported speech patterns None (use pre-defined patterns) preceding Preceding reported speech patterns None (use pre-defined patterns) following Following reported speech patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs Patterns for verbs that imply a reported speech None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for reported speech within entities False explain Whether to keep track of the cues for each entity False Declared extensions The eds.reported_speech pipeline declares two spaCy extensions , on both Span and Token objects : The reported_speech attribute is a boolean, set to True if the pipeline predicts that the span/token is reported. The reported_speech_ property is a human-readable string, computed from the reported_speech attribute. It implements a simple getter function that outputs DIRECT or REPORTED , depending on the value of reported_speech . Authors and citation The eds.reported_speech pipeline was developed by AP-HP's Data Science team.","title":"Reported Speech"},{"location":"pipelines/qualifiers/reported-speech/#reported-speech","text":"The eds.reported_speech pipeline uses a simple rule-based algorithm to detect spans that relate to reported speech (eg when the doctor quotes the patient). It was designed at AP-HP's EDS.","title":"Reported Speech"},{"location":"pipelines/qualifiers/reported-speech/#usage","text":"The following snippet matches a simple terminology, and checks whether the extracted entities are part of a reported speech. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( patient = \"patient\" , alcool = \"alcoolis\u00e9\" )), ) nlp . add_pipe ( \"eds.reported_speech\" ) text = ( \"Le patient est admis aux urgences ce soir pour une douleur au bras. \" \"Il nie \u00eatre alcoolis\u00e9.\" ) doc = nlp ( text ) doc . ents # Out: [patient, alcoolis\u00e9] doc . ents [ 0 ] . _ . reported_speech # Out: False doc . ents [ 1 ] . _ . reported_speech # Out: True","title":"Usage"},{"location":"pipelines/qualifiers/reported-speech/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-reported speech patterns None (use pre-defined patterns) preceding Preceding reported speech patterns None (use pre-defined patterns) following Following reported speech patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs Patterns for verbs that imply a reported speech None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for reported speech within entities False explain Whether to keep track of the cues for each entity False","title":"Configuration"},{"location":"pipelines/qualifiers/reported-speech/#declared-extensions","text":"The eds.reported_speech pipeline declares two spaCy extensions , on both Span and Token objects : The reported_speech attribute is a boolean, set to True if the pipeline predicts that the span/token is reported. The reported_speech_ property is a human-readable string, computed from the reported_speech attribute. It implements a simple getter function that outputs DIRECT or REPORTED , depending on the value of reported_speech .","title":"Declared extensions"},{"location":"pipelines/qualifiers/reported-speech/#authors-and-citation","text":"The eds.reported_speech pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"reference/","text":"edsnlp EDS-NLP conjugator conjugate_verb ( verb , conjugator ) Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas DataFrame . PARAMETER DESCRIPTION verb Verb to conjugate. TYPE: str conjugator mlconjug3 instance for conjugating. TYPE: mlconjug3.Conjugator RETURNS DESCRIPTION pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. Source code in edsnlp/conjugator.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def conjugate_verb ( verb : str , conjugator : mlconjug3 . Conjugator , ) -> pd . DataFrame : \"\"\" Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas `DataFrame`. Parameters ---------- verb : str Verb to conjugate. conjugator : mlconjug3.Conjugator mlconjug3 instance for conjugating. Returns ------- pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. \"\"\" df = pd . DataFrame ( conjugator . conjugate ( verb ) . iterate (), columns = [ \"mode\" , \"tense\" , \"person\" , \"term\" ], ) df . term = df . term . fillna ( df . person ) df . loc [ df . person == df . term , \"person\" ] = None df . insert ( 0 , \"verb\" , verb ) return df conjugate ( verbs , language = 'fr' ) Conjugate a list of verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate TYPE: Union[str, List[str]] language Language to conjugate. Defaults to French ( fr ). TYPE: str DEFAULT: 'fr' RETURNS DESCRIPTION pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: verb , mode , tense , person , term Source code in edsnlp/conjugator.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def conjugate ( verbs : Union [ str , List [ str ]], language : str = \"fr\" , ) -> pd . DataFrame : \"\"\" Conjugate a list of verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate language: str Language to conjugate. Defaults to French (`fr`). Returns ------- pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: `verb`, `mode`, `tense`, `person`, `term` \"\"\" if isinstance ( verbs , str ): verbs = [ verbs ] conjugator = mlconjug3 . Conjugator ( language = language ) df = pd . concat ([ conjugate_verb ( verb , conjugator = conjugator ) for verb in verbs ]) df = df . reset_index ( drop = True ) return df get_conjugated_verbs ( verbs , matches , language = 'fr' ) Get a list of conjugated verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate. TYPE: Union[str, List[str]] matches List of dictionary describing the mode/tense/persons to keep. TYPE: Union[List[Dict[str, str]], Dict[str, str]] language [description], by default \"fr\" (French) TYPE: str, optional DEFAULT: 'fr' RETURNS DESCRIPTION List[str] List of terms to look for. Examples: >>> get_conjugated_verbs ( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] Source code in edsnlp/conjugator.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_conjugated_verbs ( verbs : Union [ str , List [ str ]], matches : Union [ List [ Dict [ str , str ]], Dict [ str , str ]], language : str = \"fr\" , ) -> List [ str ]: \"\"\" Get a list of conjugated verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate. matches : Union[List[Dict[str, str]], Dict[str, str]] List of dictionary describing the mode/tense/persons to keep. language : str, optional [description], by default \"fr\" (French) Returns ------- List[str] List of terms to look for. Examples -------- >>> get_conjugated_verbs( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] \"\"\" if isinstance ( matches , dict ): matches = [ matches ] terms = [] df = conjugate ( verbs = verbs , language = language , ) for match in matches : q = \" & \" . join ([ f ' { k } == \" { v } \"' for k , v in match . items ()]) terms . extend ( df . query ( q ) . term . unique ()) return list ( set ( terms )) components","title":"`edsnlp`"},{"location":"reference/#edsnlp","text":"EDS-NLP","title":"edsnlp"},{"location":"reference/#edsnlp.conjugator","text":"","title":"conjugator"},{"location":"reference/#edsnlp.conjugator.conjugate_verb","text":"Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas DataFrame . PARAMETER DESCRIPTION verb Verb to conjugate. TYPE: str conjugator mlconjug3 instance for conjugating. TYPE: mlconjug3.Conjugator RETURNS DESCRIPTION pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. Source code in edsnlp/conjugator.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def conjugate_verb ( verb : str , conjugator : mlconjug3 . Conjugator , ) -> pd . DataFrame : \"\"\" Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas `DataFrame`. Parameters ---------- verb : str Verb to conjugate. conjugator : mlconjug3.Conjugator mlconjug3 instance for conjugating. Returns ------- pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. \"\"\" df = pd . DataFrame ( conjugator . conjugate ( verb ) . iterate (), columns = [ \"mode\" , \"tense\" , \"person\" , \"term\" ], ) df . term = df . term . fillna ( df . person ) df . loc [ df . person == df . term , \"person\" ] = None df . insert ( 0 , \"verb\" , verb ) return df","title":"conjugate_verb()"},{"location":"reference/#edsnlp.conjugator.conjugate","text":"Conjugate a list of verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate TYPE: Union[str, List[str]] language Language to conjugate. Defaults to French ( fr ). TYPE: str DEFAULT: 'fr' RETURNS DESCRIPTION pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: verb , mode , tense , person , term Source code in edsnlp/conjugator.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def conjugate ( verbs : Union [ str , List [ str ]], language : str = \"fr\" , ) -> pd . DataFrame : \"\"\" Conjugate a list of verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate language: str Language to conjugate. Defaults to French (`fr`). Returns ------- pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: `verb`, `mode`, `tense`, `person`, `term` \"\"\" if isinstance ( verbs , str ): verbs = [ verbs ] conjugator = mlconjug3 . Conjugator ( language = language ) df = pd . concat ([ conjugate_verb ( verb , conjugator = conjugator ) for verb in verbs ]) df = df . reset_index ( drop = True ) return df","title":"conjugate()"},{"location":"reference/#edsnlp.conjugator.get_conjugated_verbs","text":"Get a list of conjugated verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate. TYPE: Union[str, List[str]] matches List of dictionary describing the mode/tense/persons to keep. TYPE: Union[List[Dict[str, str]], Dict[str, str]] language [description], by default \"fr\" (French) TYPE: str, optional DEFAULT: 'fr' RETURNS DESCRIPTION List[str] List of terms to look for. Examples: >>> get_conjugated_verbs ( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] Source code in edsnlp/conjugator.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_conjugated_verbs ( verbs : Union [ str , List [ str ]], matches : Union [ List [ Dict [ str , str ]], Dict [ str , str ]], language : str = \"fr\" , ) -> List [ str ]: \"\"\" Get a list of conjugated verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate. matches : Union[List[Dict[str, str]], Dict[str, str]] List of dictionary describing the mode/tense/persons to keep. language : str, optional [description], by default \"fr\" (French) Returns ------- List[str] List of terms to look for. Examples -------- >>> get_conjugated_verbs( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] \"\"\" if isinstance ( matches , dict ): matches = [ matches ] terms = [] df = conjugate ( verbs = verbs , language = language , ) for match in matches : q = \" & \" . join ([ f ' { k } == \" { v } \"' for k , v in match . items ()]) terms . extend ( df . query ( q ) . term . unique ()) return list ( set ( terms ))","title":"get_conjugated_verbs()"},{"location":"reference/#edsnlp.components","text":"","title":"components"},{"location":"reference/SUMMARY/","text":"edsnlp components conjugator connectors brat labeltool omop extensions matchers phrase regex utils offset text pipelines base core advanced advanced factory endlines endlines endlinesmodel factory functional matcher factory matcher normalizer accents accents factory patterns factory lowercase factory normalizer pollution factory patterns pollution quotes factory patterns quotes utils sentences factory sentences terms factories misc consultation_dates consultation_dates factory patterns dates dates factory parsing patterns atomic days months time years current relative reason factory patterns reason sections factory patterns sections ner covid factory patterns scores base_score charlson factory patterns emergency ccmu factory patterns gemsa factory patterns priority factory patterns factory sofa factory patterns sofa qualifiers base factories family factory family patterns history factory history patterns hypothesis factory hypothesis patterns negation factory negation patterns reported_speech factory patterns reported_speech terminations processing parallel simple spark wrapper utils deprecation examples filter inclusion regex resources","title":"SUMMARY"},{"location":"reference/components/","text":"edsnlp.components","title":"components"},{"location":"reference/components/#edsnlpcomponents","text":"","title":"edsnlp.components"},{"location":"reference/conjugator/","text":"edsnlp.conjugator conjugate_verb ( verb , conjugator ) Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas DataFrame . PARAMETER DESCRIPTION verb Verb to conjugate. TYPE: str conjugator mlconjug3 instance for conjugating. TYPE: mlconjug3.Conjugator RETURNS DESCRIPTION pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. Source code in edsnlp/conjugator.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def conjugate_verb ( verb : str , conjugator : mlconjug3 . Conjugator , ) -> pd . DataFrame : \"\"\" Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas `DataFrame`. Parameters ---------- verb : str Verb to conjugate. conjugator : mlconjug3.Conjugator mlconjug3 instance for conjugating. Returns ------- pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. \"\"\" df = pd . DataFrame ( conjugator . conjugate ( verb ) . iterate (), columns = [ \"mode\" , \"tense\" , \"person\" , \"term\" ], ) df . term = df . term . fillna ( df . person ) df . loc [ df . person == df . term , \"person\" ] = None df . insert ( 0 , \"verb\" , verb ) return df conjugate ( verbs , language = 'fr' ) Conjugate a list of verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate TYPE: Union[str, List[str]] language Language to conjugate. Defaults to French ( fr ). TYPE: str DEFAULT: 'fr' RETURNS DESCRIPTION pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: verb , mode , tense , person , term Source code in edsnlp/conjugator.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def conjugate ( verbs : Union [ str , List [ str ]], language : str = \"fr\" , ) -> pd . DataFrame : \"\"\" Conjugate a list of verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate language: str Language to conjugate. Defaults to French (`fr`). Returns ------- pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: `verb`, `mode`, `tense`, `person`, `term` \"\"\" if isinstance ( verbs , str ): verbs = [ verbs ] conjugator = mlconjug3 . Conjugator ( language = language ) df = pd . concat ([ conjugate_verb ( verb , conjugator = conjugator ) for verb in verbs ]) df = df . reset_index ( drop = True ) return df get_conjugated_verbs ( verbs , matches , language = 'fr' ) Get a list of conjugated verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate. TYPE: Union[str, List[str]] matches List of dictionary describing the mode/tense/persons to keep. TYPE: Union[List[Dict[str, str]], Dict[str, str]] language [description], by default \"fr\" (French) TYPE: str, optional DEFAULT: 'fr' RETURNS DESCRIPTION List[str] List of terms to look for. Examples: >>> get_conjugated_verbs ( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] Source code in edsnlp/conjugator.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_conjugated_verbs ( verbs : Union [ str , List [ str ]], matches : Union [ List [ Dict [ str , str ]], Dict [ str , str ]], language : str = \"fr\" , ) -> List [ str ]: \"\"\" Get a list of conjugated verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate. matches : Union[List[Dict[str, str]], Dict[str, str]] List of dictionary describing the mode/tense/persons to keep. language : str, optional [description], by default \"fr\" (French) Returns ------- List[str] List of terms to look for. Examples -------- >>> get_conjugated_verbs( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] \"\"\" if isinstance ( matches , dict ): matches = [ matches ] terms = [] df = conjugate ( verbs = verbs , language = language , ) for match in matches : q = \" & \" . join ([ f ' { k } == \" { v } \"' for k , v in match . items ()]) terms . extend ( df . query ( q ) . term . unique ()) return list ( set ( terms ))","title":"conjugator"},{"location":"reference/conjugator/#edsnlpconjugator","text":"","title":"edsnlp.conjugator"},{"location":"reference/conjugator/#edsnlp.conjugator.conjugate_verb","text":"Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas DataFrame . PARAMETER DESCRIPTION verb Verb to conjugate. TYPE: str conjugator mlconjug3 instance for conjugating. TYPE: mlconjug3.Conjugator RETURNS DESCRIPTION pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. Source code in edsnlp/conjugator.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def conjugate_verb ( verb : str , conjugator : mlconjug3 . Conjugator , ) -> pd . DataFrame : \"\"\" Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas `DataFrame`. Parameters ---------- verb : str Verb to conjugate. conjugator : mlconjug3.Conjugator mlconjug3 instance for conjugating. Returns ------- pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. \"\"\" df = pd . DataFrame ( conjugator . conjugate ( verb ) . iterate (), columns = [ \"mode\" , \"tense\" , \"person\" , \"term\" ], ) df . term = df . term . fillna ( df . person ) df . loc [ df . person == df . term , \"person\" ] = None df . insert ( 0 , \"verb\" , verb ) return df","title":"conjugate_verb()"},{"location":"reference/conjugator/#edsnlp.conjugator.conjugate","text":"Conjugate a list of verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate TYPE: Union[str, List[str]] language Language to conjugate. Defaults to French ( fr ). TYPE: str DEFAULT: 'fr' RETURNS DESCRIPTION pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: verb , mode , tense , person , term Source code in edsnlp/conjugator.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def conjugate ( verbs : Union [ str , List [ str ]], language : str = \"fr\" , ) -> pd . DataFrame : \"\"\" Conjugate a list of verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate language: str Language to conjugate. Defaults to French (`fr`). Returns ------- pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: `verb`, `mode`, `tense`, `person`, `term` \"\"\" if isinstance ( verbs , str ): verbs = [ verbs ] conjugator = mlconjug3 . Conjugator ( language = language ) df = pd . concat ([ conjugate_verb ( verb , conjugator = conjugator ) for verb in verbs ]) df = df . reset_index ( drop = True ) return df","title":"conjugate()"},{"location":"reference/conjugator/#edsnlp.conjugator.get_conjugated_verbs","text":"Get a list of conjugated verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate. TYPE: Union[str, List[str]] matches List of dictionary describing the mode/tense/persons to keep. TYPE: Union[List[Dict[str, str]], Dict[str, str]] language [description], by default \"fr\" (French) TYPE: str, optional DEFAULT: 'fr' RETURNS DESCRIPTION List[str] List of terms to look for. Examples: >>> get_conjugated_verbs ( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] Source code in edsnlp/conjugator.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_conjugated_verbs ( verbs : Union [ str , List [ str ]], matches : Union [ List [ Dict [ str , str ]], Dict [ str , str ]], language : str = \"fr\" , ) -> List [ str ]: \"\"\" Get a list of conjugated verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate. matches : Union[List[Dict[str, str]], Dict[str, str]] List of dictionary describing the mode/tense/persons to keep. language : str, optional [description], by default \"fr\" (French) Returns ------- List[str] List of terms to look for. Examples -------- >>> get_conjugated_verbs( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] \"\"\" if isinstance ( matches , dict ): matches = [ matches ] terms = [] df = conjugate ( verbs = verbs , language = language , ) for match in matches : q = \" & \" . join ([ f ' { k } == \" { v } \"' for k , v in match . items ()]) terms . extend ( df . query ( q ) . term . unique ()) return list ( set ( terms ))","title":"get_conjugated_verbs()"},{"location":"reference/extensions/","text":"edsnlp.extensions","title":"extensions"},{"location":"reference/extensions/#edsnlpextensions","text":"","title":"edsnlp.extensions"},{"location":"reference/connectors/","text":"edsnlp.connectors","title":"`edsnlp.connectors`"},{"location":"reference/connectors/#edsnlpconnectors","text":"","title":"edsnlp.connectors"},{"location":"reference/connectors/brat/","text":"edsnlp.connectors.brat BratConnector Bases: object Two-way connector with BRAT. Supports entities only. PARAMETER DESCRIPTION directory Directory containing the BRAT files. TYPE: str n_jobs Number of jobs for multiprocessing, by default 1 TYPE: int, optional Source code in edsnlp/connectors/brat.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 class BratConnector ( object ): \"\"\" Two-way connector with BRAT. Supports entities only. Parameters ---------- directory : str Directory containing the BRAT files. n_jobs : int, optional Number of jobs for multiprocessing, by default 1 \"\"\" def __init__ ( self , directory : str , n_jobs : int = 1 ): self . directory = directory self . n_jobs = n_jobs os . makedirs ( directory , exist_ok = True ) def full_path ( self , filename : str ) -> str : return os . path . join ( self . directory , filename ) def read_file ( self , filename : str ) -> str : \"\"\" Reads a file within the BRAT directory. Parameters ---------- filename: The path to the file within the BRAT directory. Returns ------- text: The text content of the file. \"\"\" with open ( self . full_path ( filename ), \"r\" , encoding = \"utf-8\" ) as f : return f . read () def read_texts ( self ) -> pd . DataFrame : \"\"\" Reads all texts from the BRAT folder. Returns ------- texts: DataFrame containing all texts in the BRAT directory. \"\"\" files = os . listdir ( self . directory ) filenames = [ f [: - 4 ] for f in files if f . endswith ( \".txt\" )] assert filenames , f \"BRAT directory { self . directory } is empty!\" logger . info ( f \"The BRAT directory contains { len ( filenames ) } annotated documents.\" ) texts = pd . DataFrame ( dict ( note_id = filenames )) with tqdm ( texts . note_id , ascii = True , ncols = 100 , desc = \"Text extraction\" ) as iterator : texts [ \"note_text\" ] = [ self . read_file ( note_id + \".txt\" ) for note_id in iterator ] return texts def read_brat_annotation ( self , note_id : Union [ str , int ]) -> pd . DataFrame : \"\"\" Reads BRAT annotation inside the BRAT directory. Parameters ---------- note_id: Note ID within the BRAT directory. Returns ------- annotations: DataFrame containing the annotations for the given note. \"\"\" filename = f \" { note_id } .ann\" annotations = read_brat_annotation ( self . full_path ( filename )) return annotations def read_annotations ( self , texts : pd . DataFrame ) -> pd . DataFrame : dfs = [] with tqdm ( texts . note_id , ascii = True , ncols = 100 , desc = \"Annotation extraction\" ) as iterator : dfs = Parallel ( n_jobs = self . n_jobs )( delayed ( self . read_brat_annotation )( note_id ) for note_id in iterator ) # for note_id in iterator: # dfs.append(self.read_brat_annotation(note_id)) annotations = pd . concat ( dfs , keys = texts . note_id , names = [ \"note_id\" ]) annotations = annotations . droplevel ( 1 ) . reset_index () return annotations def get_brat ( self ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Reads texts and annotations, and returns two DataFrame objects. Returns ------- texts: A DataFrame containing two fields, `note_id` and `note_text` annotations: A DataFrame containing the annotations. \"\"\" texts = self . read_texts () annotations = self . read_annotations ( texts ) return texts , annotations def brat2docs ( self , nlp : Language ) -> List [ Doc ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: A spaCy pipeline. Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" texts , annotations = self . get_brat () docs = [] with tqdm ( zip ( texts . note_id , nlp . pipe ( texts . note_text , batch_size = 50 , n_process = self . n_jobs ), ), ascii = True , ncols = 100 , desc = \"spaCy conversion\" , total = len ( texts ), ) as iterator : for note_id , doc in iterator : doc . _ . note_id = note_id ann = annotations . query ( \"note_id == @note_id\" ) spans = [] for _ , row in ann . iterrows (): span = doc . char_span ( row . start , row . end , label = row . label , alignment_mode = \"expand\" , ) spans . append ( span ) doc . ents = filter_spans ( spans ) docs . append ( doc ) return docs def doc2brat ( self , doc : Doc ) -> None : \"\"\" Writes a spaCy document to file in the BRAT directory. Parameters ---------- doc: spaCy Doc object. The spans in `ents` will populate the `note_id.ann` file. \"\"\" filename = str ( doc . _ . note_id ) with open ( self . full_path ( f \" { filename } .txt\" ), \"w\" , encoding = \"utf-8\" ) as f : f . write ( doc . text ) annotations = pd . DataFrame . from_records ( [ dict ( label = ann . label_ , lexical_variant = ann . text , start = ann . start_char , end = ann . end_char , ) for ann in doc . ents ] ) if len ( annotations ) > 0 : annotations [ \"annot\" ] = ( annotations . label + \" \" + annotations . start . astype ( str ) + \" \" + annotations . end . astype ( str ) ) annotations [ \"index\" ] = [ f \"T { i + 1 } \" for i in range ( len ( annotations ))] annotations = annotations [[ \"index\" , \"annot\" , \"lexical_variant\" ]] annotations . to_csv ( self . full_path ( f \" { filename } .ann\" ), sep = \" \\t \" , header = None , index = False , encoding = \"utf-8\" , ) else : open ( self . full_path ( f \" { filename } .ann\" ), \"w\" , encoding = \"utf-8\" ) . close () def docs2brat ( self , docs : List [ Doc ]) -> None : \"\"\" Writes a list of spaCy documents to file. Parameters ---------- docs: List of spaCy documents. \"\"\" for doc in docs : self . doc2brat ( doc ) read_file ( filename ) Reads a file within the BRAT directory. PARAMETER DESCRIPTION filename The path to the file within the BRAT directory. TYPE: str RETURNS DESCRIPTION text The text content of the file. Source code in edsnlp/connectors/brat.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def read_file ( self , filename : str ) -> str : \"\"\" Reads a file within the BRAT directory. Parameters ---------- filename: The path to the file within the BRAT directory. Returns ------- text: The text content of the file. \"\"\" with open ( self . full_path ( filename ), \"r\" , encoding = \"utf-8\" ) as f : return f . read () read_texts () Reads all texts from the BRAT folder. RETURNS DESCRIPTION texts DataFrame containing all texts in the BRAT directory. Source code in edsnlp/connectors/brat.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def read_texts ( self ) -> pd . DataFrame : \"\"\" Reads all texts from the BRAT folder. Returns ------- texts: DataFrame containing all texts in the BRAT directory. \"\"\" files = os . listdir ( self . directory ) filenames = [ f [: - 4 ] for f in files if f . endswith ( \".txt\" )] assert filenames , f \"BRAT directory { self . directory } is empty!\" logger . info ( f \"The BRAT directory contains { len ( filenames ) } annotated documents.\" ) texts = pd . DataFrame ( dict ( note_id = filenames )) with tqdm ( texts . note_id , ascii = True , ncols = 100 , desc = \"Text extraction\" ) as iterator : texts [ \"note_text\" ] = [ self . read_file ( note_id + \".txt\" ) for note_id in iterator ] return texts read_brat_annotation ( note_id ) Reads BRAT annotation inside the BRAT directory. PARAMETER DESCRIPTION note_id Note ID within the BRAT directory. TYPE: Union [ str , int ] RETURNS DESCRIPTION annotations DataFrame containing the annotations for the given note. Source code in edsnlp/connectors/brat.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def read_brat_annotation ( self , note_id : Union [ str , int ]) -> pd . DataFrame : \"\"\" Reads BRAT annotation inside the BRAT directory. Parameters ---------- note_id: Note ID within the BRAT directory. Returns ------- annotations: DataFrame containing the annotations for the given note. \"\"\" filename = f \" { note_id } .ann\" annotations = read_brat_annotation ( self . full_path ( filename )) return annotations get_brat () Reads texts and annotations, and returns two DataFrame objects. RETURNS DESCRIPTION texts A DataFrame containing two fields, note_id and note_text annotations A DataFrame containing the annotations. Source code in edsnlp/connectors/brat.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def get_brat ( self ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Reads texts and annotations, and returns two DataFrame objects. Returns ------- texts: A DataFrame containing two fields, `note_id` and `note_text` annotations: A DataFrame containing the annotations. \"\"\" texts = self . read_texts () annotations = self . read_annotations ( texts ) return texts , annotations brat2docs ( nlp ) Transforms a BRAT folder to a list of spaCy documents. PARAMETER DESCRIPTION nlp A spaCy pipeline. TYPE: Language RETURNS DESCRIPTION docs List of spaCy documents, with annotations in the ents attribute. Source code in edsnlp/connectors/brat.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def brat2docs ( self , nlp : Language ) -> List [ Doc ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: A spaCy pipeline. Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" texts , annotations = self . get_brat () docs = [] with tqdm ( zip ( texts . note_id , nlp . pipe ( texts . note_text , batch_size = 50 , n_process = self . n_jobs ), ), ascii = True , ncols = 100 , desc = \"spaCy conversion\" , total = len ( texts ), ) as iterator : for note_id , doc in iterator : doc . _ . note_id = note_id ann = annotations . query ( \"note_id == @note_id\" ) spans = [] for _ , row in ann . iterrows (): span = doc . char_span ( row . start , row . end , label = row . label , alignment_mode = \"expand\" , ) spans . append ( span ) doc . ents = filter_spans ( spans ) docs . append ( doc ) return docs doc2brat ( doc ) Writes a spaCy document to file in the BRAT directory. PARAMETER DESCRIPTION doc spaCy Doc object. The spans in ents will populate the note_id.ann file. TYPE: Doc Source code in edsnlp/connectors/brat.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def doc2brat ( self , doc : Doc ) -> None : \"\"\" Writes a spaCy document to file in the BRAT directory. Parameters ---------- doc: spaCy Doc object. The spans in `ents` will populate the `note_id.ann` file. \"\"\" filename = str ( doc . _ . note_id ) with open ( self . full_path ( f \" { filename } .txt\" ), \"w\" , encoding = \"utf-8\" ) as f : f . write ( doc . text ) annotations = pd . DataFrame . from_records ( [ dict ( label = ann . label_ , lexical_variant = ann . text , start = ann . start_char , end = ann . end_char , ) for ann in doc . ents ] ) if len ( annotations ) > 0 : annotations [ \"annot\" ] = ( annotations . label + \" \" + annotations . start . astype ( str ) + \" \" + annotations . end . astype ( str ) ) annotations [ \"index\" ] = [ f \"T { i + 1 } \" for i in range ( len ( annotations ))] annotations = annotations [[ \"index\" , \"annot\" , \"lexical_variant\" ]] annotations . to_csv ( self . full_path ( f \" { filename } .ann\" ), sep = \" \\t \" , header = None , index = False , encoding = \"utf-8\" , ) else : open ( self . full_path ( f \" { filename } .ann\" ), \"w\" , encoding = \"utf-8\" ) . close () docs2brat ( docs ) Writes a list of spaCy documents to file. PARAMETER DESCRIPTION docs List of spaCy documents. TYPE: List [ Doc ] Source code in edsnlp/connectors/brat.py 275 276 277 278 279 280 281 282 283 284 285 def docs2brat ( self , docs : List [ Doc ]) -> None : \"\"\" Writes a list of spaCy documents to file. Parameters ---------- docs: List of spaCy documents. \"\"\" for doc in docs : self . doc2brat ( doc ) read_brat_annotation ( filename ) Read BRAT annotation file and returns a pandas DataFrame. PARAMETER DESCRIPTION filename Path to the annotation file. TYPE: str RETURNS DESCRIPTION annotations DataFrame containing the annotations. Source code in edsnlp/connectors/brat.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def read_brat_annotation ( filename : str ) -> pd . DataFrame : \"\"\" Read BRAT annotation file and returns a pandas DataFrame. Parameters ---------- filename: Path to the annotation file. Returns ------- annotations: DataFrame containing the annotations. \"\"\" lines = [] with open ( filename , \"r\" ) as f : for line in f . readlines (): lines . append ( tuple ( line . rstrip ( \" \\n \" ) . split ( \" \\t \" , 2 ))) if not lines or len ( lines [ 0 ]) == 1 : return pd . DataFrame ( columns = [ \"index\" , \"start\" , \"end\" , \"label\" , \"lexical_variant\" ] ) annotations = pd . DataFrame ( lines , columns = [ \"index\" , \"annot\" , \"lexical_variant\" ]) annotations [ \"end\" ] = annotations . annot . str . split () . str [ - 1 ] annotations [ \"annot\" ] = annotations . annot . str . split ( \";\" ) . str [ 0 ] annotations [ \"label\" ] = annotations . annot . str . split () . str [: - 2 ] . str . join ( \" \" ) annotations [ \"start\" ] = annotations . annot . str . split () . str [ - 2 ] annotations [[ \"start\" , \"end\" ]] = annotations [[ \"start\" , \"end\" ]] . astype ( int ) annotations = annotations . drop ( columns = [ \"annot\" ]) return annotations","title":"brat"},{"location":"reference/connectors/brat/#edsnlpconnectorsbrat","text":"","title":"edsnlp.connectors.brat"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector","text":"Bases: object Two-way connector with BRAT. Supports entities only. PARAMETER DESCRIPTION directory Directory containing the BRAT files. TYPE: str n_jobs Number of jobs for multiprocessing, by default 1 TYPE: int, optional Source code in edsnlp/connectors/brat.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 class BratConnector ( object ): \"\"\" Two-way connector with BRAT. Supports entities only. Parameters ---------- directory : str Directory containing the BRAT files. n_jobs : int, optional Number of jobs for multiprocessing, by default 1 \"\"\" def __init__ ( self , directory : str , n_jobs : int = 1 ): self . directory = directory self . n_jobs = n_jobs os . makedirs ( directory , exist_ok = True ) def full_path ( self , filename : str ) -> str : return os . path . join ( self . directory , filename ) def read_file ( self , filename : str ) -> str : \"\"\" Reads a file within the BRAT directory. Parameters ---------- filename: The path to the file within the BRAT directory. Returns ------- text: The text content of the file. \"\"\" with open ( self . full_path ( filename ), \"r\" , encoding = \"utf-8\" ) as f : return f . read () def read_texts ( self ) -> pd . DataFrame : \"\"\" Reads all texts from the BRAT folder. Returns ------- texts: DataFrame containing all texts in the BRAT directory. \"\"\" files = os . listdir ( self . directory ) filenames = [ f [: - 4 ] for f in files if f . endswith ( \".txt\" )] assert filenames , f \"BRAT directory { self . directory } is empty!\" logger . info ( f \"The BRAT directory contains { len ( filenames ) } annotated documents.\" ) texts = pd . DataFrame ( dict ( note_id = filenames )) with tqdm ( texts . note_id , ascii = True , ncols = 100 , desc = \"Text extraction\" ) as iterator : texts [ \"note_text\" ] = [ self . read_file ( note_id + \".txt\" ) for note_id in iterator ] return texts def read_brat_annotation ( self , note_id : Union [ str , int ]) -> pd . DataFrame : \"\"\" Reads BRAT annotation inside the BRAT directory. Parameters ---------- note_id: Note ID within the BRAT directory. Returns ------- annotations: DataFrame containing the annotations for the given note. \"\"\" filename = f \" { note_id } .ann\" annotations = read_brat_annotation ( self . full_path ( filename )) return annotations def read_annotations ( self , texts : pd . DataFrame ) -> pd . DataFrame : dfs = [] with tqdm ( texts . note_id , ascii = True , ncols = 100 , desc = \"Annotation extraction\" ) as iterator : dfs = Parallel ( n_jobs = self . n_jobs )( delayed ( self . read_brat_annotation )( note_id ) for note_id in iterator ) # for note_id in iterator: # dfs.append(self.read_brat_annotation(note_id)) annotations = pd . concat ( dfs , keys = texts . note_id , names = [ \"note_id\" ]) annotations = annotations . droplevel ( 1 ) . reset_index () return annotations def get_brat ( self ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Reads texts and annotations, and returns two DataFrame objects. Returns ------- texts: A DataFrame containing two fields, `note_id` and `note_text` annotations: A DataFrame containing the annotations. \"\"\" texts = self . read_texts () annotations = self . read_annotations ( texts ) return texts , annotations def brat2docs ( self , nlp : Language ) -> List [ Doc ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: A spaCy pipeline. Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" texts , annotations = self . get_brat () docs = [] with tqdm ( zip ( texts . note_id , nlp . pipe ( texts . note_text , batch_size = 50 , n_process = self . n_jobs ), ), ascii = True , ncols = 100 , desc = \"spaCy conversion\" , total = len ( texts ), ) as iterator : for note_id , doc in iterator : doc . _ . note_id = note_id ann = annotations . query ( \"note_id == @note_id\" ) spans = [] for _ , row in ann . iterrows (): span = doc . char_span ( row . start , row . end , label = row . label , alignment_mode = \"expand\" , ) spans . append ( span ) doc . ents = filter_spans ( spans ) docs . append ( doc ) return docs def doc2brat ( self , doc : Doc ) -> None : \"\"\" Writes a spaCy document to file in the BRAT directory. Parameters ---------- doc: spaCy Doc object. The spans in `ents` will populate the `note_id.ann` file. \"\"\" filename = str ( doc . _ . note_id ) with open ( self . full_path ( f \" { filename } .txt\" ), \"w\" , encoding = \"utf-8\" ) as f : f . write ( doc . text ) annotations = pd . DataFrame . from_records ( [ dict ( label = ann . label_ , lexical_variant = ann . text , start = ann . start_char , end = ann . end_char , ) for ann in doc . ents ] ) if len ( annotations ) > 0 : annotations [ \"annot\" ] = ( annotations . label + \" \" + annotations . start . astype ( str ) + \" \" + annotations . end . astype ( str ) ) annotations [ \"index\" ] = [ f \"T { i + 1 } \" for i in range ( len ( annotations ))] annotations = annotations [[ \"index\" , \"annot\" , \"lexical_variant\" ]] annotations . to_csv ( self . full_path ( f \" { filename } .ann\" ), sep = \" \\t \" , header = None , index = False , encoding = \"utf-8\" , ) else : open ( self . full_path ( f \" { filename } .ann\" ), \"w\" , encoding = \"utf-8\" ) . close () def docs2brat ( self , docs : List [ Doc ]) -> None : \"\"\" Writes a list of spaCy documents to file. Parameters ---------- docs: List of spaCy documents. \"\"\" for doc in docs : self . doc2brat ( doc )","title":"BratConnector"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.read_file","text":"Reads a file within the BRAT directory. PARAMETER DESCRIPTION filename The path to the file within the BRAT directory. TYPE: str RETURNS DESCRIPTION text The text content of the file. Source code in edsnlp/connectors/brat.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def read_file ( self , filename : str ) -> str : \"\"\" Reads a file within the BRAT directory. Parameters ---------- filename: The path to the file within the BRAT directory. Returns ------- text: The text content of the file. \"\"\" with open ( self . full_path ( filename ), \"r\" , encoding = \"utf-8\" ) as f : return f . read ()","title":"read_file()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.read_texts","text":"Reads all texts from the BRAT folder. RETURNS DESCRIPTION texts DataFrame containing all texts in the BRAT directory. Source code in edsnlp/connectors/brat.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def read_texts ( self ) -> pd . DataFrame : \"\"\" Reads all texts from the BRAT folder. Returns ------- texts: DataFrame containing all texts in the BRAT directory. \"\"\" files = os . listdir ( self . directory ) filenames = [ f [: - 4 ] for f in files if f . endswith ( \".txt\" )] assert filenames , f \"BRAT directory { self . directory } is empty!\" logger . info ( f \"The BRAT directory contains { len ( filenames ) } annotated documents.\" ) texts = pd . DataFrame ( dict ( note_id = filenames )) with tqdm ( texts . note_id , ascii = True , ncols = 100 , desc = \"Text extraction\" ) as iterator : texts [ \"note_text\" ] = [ self . read_file ( note_id + \".txt\" ) for note_id in iterator ] return texts","title":"read_texts()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.read_brat_annotation","text":"Reads BRAT annotation inside the BRAT directory. PARAMETER DESCRIPTION note_id Note ID within the BRAT directory. TYPE: Union [ str , int ] RETURNS DESCRIPTION annotations DataFrame containing the annotations for the given note. Source code in edsnlp/connectors/brat.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def read_brat_annotation ( self , note_id : Union [ str , int ]) -> pd . DataFrame : \"\"\" Reads BRAT annotation inside the BRAT directory. Parameters ---------- note_id: Note ID within the BRAT directory. Returns ------- annotations: DataFrame containing the annotations for the given note. \"\"\" filename = f \" { note_id } .ann\" annotations = read_brat_annotation ( self . full_path ( filename )) return annotations","title":"read_brat_annotation()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.get_brat","text":"Reads texts and annotations, and returns two DataFrame objects. RETURNS DESCRIPTION texts A DataFrame containing two fields, note_id and note_text annotations A DataFrame containing the annotations. Source code in edsnlp/connectors/brat.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def get_brat ( self ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Reads texts and annotations, and returns two DataFrame objects. Returns ------- texts: A DataFrame containing two fields, `note_id` and `note_text` annotations: A DataFrame containing the annotations. \"\"\" texts = self . read_texts () annotations = self . read_annotations ( texts ) return texts , annotations","title":"get_brat()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.brat2docs","text":"Transforms a BRAT folder to a list of spaCy documents. PARAMETER DESCRIPTION nlp A spaCy pipeline. TYPE: Language RETURNS DESCRIPTION docs List of spaCy documents, with annotations in the ents attribute. Source code in edsnlp/connectors/brat.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def brat2docs ( self , nlp : Language ) -> List [ Doc ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: A spaCy pipeline. Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" texts , annotations = self . get_brat () docs = [] with tqdm ( zip ( texts . note_id , nlp . pipe ( texts . note_text , batch_size = 50 , n_process = self . n_jobs ), ), ascii = True , ncols = 100 , desc = \"spaCy conversion\" , total = len ( texts ), ) as iterator : for note_id , doc in iterator : doc . _ . note_id = note_id ann = annotations . query ( \"note_id == @note_id\" ) spans = [] for _ , row in ann . iterrows (): span = doc . char_span ( row . start , row . end , label = row . label , alignment_mode = \"expand\" , ) spans . append ( span ) doc . ents = filter_spans ( spans ) docs . append ( doc ) return docs","title":"brat2docs()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.doc2brat","text":"Writes a spaCy document to file in the BRAT directory. PARAMETER DESCRIPTION doc spaCy Doc object. The spans in ents will populate the note_id.ann file. TYPE: Doc Source code in edsnlp/connectors/brat.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def doc2brat ( self , doc : Doc ) -> None : \"\"\" Writes a spaCy document to file in the BRAT directory. Parameters ---------- doc: spaCy Doc object. The spans in `ents` will populate the `note_id.ann` file. \"\"\" filename = str ( doc . _ . note_id ) with open ( self . full_path ( f \" { filename } .txt\" ), \"w\" , encoding = \"utf-8\" ) as f : f . write ( doc . text ) annotations = pd . DataFrame . from_records ( [ dict ( label = ann . label_ , lexical_variant = ann . text , start = ann . start_char , end = ann . end_char , ) for ann in doc . ents ] ) if len ( annotations ) > 0 : annotations [ \"annot\" ] = ( annotations . label + \" \" + annotations . start . astype ( str ) + \" \" + annotations . end . astype ( str ) ) annotations [ \"index\" ] = [ f \"T { i + 1 } \" for i in range ( len ( annotations ))] annotations = annotations [[ \"index\" , \"annot\" , \"lexical_variant\" ]] annotations . to_csv ( self . full_path ( f \" { filename } .ann\" ), sep = \" \\t \" , header = None , index = False , encoding = \"utf-8\" , ) else : open ( self . full_path ( f \" { filename } .ann\" ), \"w\" , encoding = \"utf-8\" ) . close ()","title":"doc2brat()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.docs2brat","text":"Writes a list of spaCy documents to file. PARAMETER DESCRIPTION docs List of spaCy documents. TYPE: List [ Doc ] Source code in edsnlp/connectors/brat.py 275 276 277 278 279 280 281 282 283 284 285 def docs2brat ( self , docs : List [ Doc ]) -> None : \"\"\" Writes a list of spaCy documents to file. Parameters ---------- docs: List of spaCy documents. \"\"\" for doc in docs : self . doc2brat ( doc )","title":"docs2brat()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.read_brat_annotation","text":"Read BRAT annotation file and returns a pandas DataFrame. PARAMETER DESCRIPTION filename Path to the annotation file. TYPE: str RETURNS DESCRIPTION annotations DataFrame containing the annotations. Source code in edsnlp/connectors/brat.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def read_brat_annotation ( filename : str ) -> pd . DataFrame : \"\"\" Read BRAT annotation file and returns a pandas DataFrame. Parameters ---------- filename: Path to the annotation file. Returns ------- annotations: DataFrame containing the annotations. \"\"\" lines = [] with open ( filename , \"r\" ) as f : for line in f . readlines (): lines . append ( tuple ( line . rstrip ( \" \\n \" ) . split ( \" \\t \" , 2 ))) if not lines or len ( lines [ 0 ]) == 1 : return pd . DataFrame ( columns = [ \"index\" , \"start\" , \"end\" , \"label\" , \"lexical_variant\" ] ) annotations = pd . DataFrame ( lines , columns = [ \"index\" , \"annot\" , \"lexical_variant\" ]) annotations [ \"end\" ] = annotations . annot . str . split () . str [ - 1 ] annotations [ \"annot\" ] = annotations . annot . str . split ( \";\" ) . str [ 0 ] annotations [ \"label\" ] = annotations . annot . str . split () . str [: - 2 ] . str . join ( \" \" ) annotations [ \"start\" ] = annotations . annot . str . split () . str [ - 2 ] annotations [[ \"start\" , \"end\" ]] = annotations [[ \"start\" , \"end\" ]] . astype ( int ) annotations = annotations . drop ( columns = [ \"annot\" ]) return annotations","title":"read_brat_annotation()"},{"location":"reference/connectors/labeltool/","text":"edsnlp.connectors.labeltool docs2labeltool ( docs , extensions = None ) Returns a labeltool-ready dataframe from a list of annotated document. PARAMETER DESCRIPTION docs List of annotated spacy docs. TYPE: List [ Doc ] extensions List of extensions to use by labeltool. TYPE: Optional [ List [ str ]] DEFAULT: None RETURNS DESCRIPTION df DataFrame tailored for labeltool. Source code in edsnlp/connectors/labeltool.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def docs2labeltool ( docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> pd . DataFrame : \"\"\" Returns a labeltool-ready dataframe from a list of annotated document. Parameters ---------- docs: list of spaCy Doc List of annotated spacy docs. extensions: list of extensions List of extensions to use by labeltool. Returns ------- df: pd.DataFrame DataFrame tailored for labeltool. \"\"\" if extensions is None : extensions = [] entities = [] for i , doc in enumerate ( tqdm ( docs , ascii = True , ncols = 100 )): for ent in doc . ents : d = dict ( note_text = doc . text , offset_begin = ent . start_char , offset_end = ent . end_char , label_name = ent . label_ , label_value = ent . text , ) d [ \"note_id\" ] = doc . _ . note_id or i for ext in extensions : d [ ext ] = getattr ( ent . _ , ext ) entities . append ( d ) df = pd . DataFrame . from_records ( entities ) columns = [ \"note_id\" , \"note_text\" , \"offset_begin\" , \"offset_end\" , \"label_name\" , \"label_value\" , ] df = df [ columns + extensions ] return df","title":"labeltool"},{"location":"reference/connectors/labeltool/#edsnlpconnectorslabeltool","text":"","title":"edsnlp.connectors.labeltool"},{"location":"reference/connectors/labeltool/#edsnlp.connectors.labeltool.docs2labeltool","text":"Returns a labeltool-ready dataframe from a list of annotated document. PARAMETER DESCRIPTION docs List of annotated spacy docs. TYPE: List [ Doc ] extensions List of extensions to use by labeltool. TYPE: Optional [ List [ str ]] DEFAULT: None RETURNS DESCRIPTION df DataFrame tailored for labeltool. Source code in edsnlp/connectors/labeltool.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def docs2labeltool ( docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> pd . DataFrame : \"\"\" Returns a labeltool-ready dataframe from a list of annotated document. Parameters ---------- docs: list of spaCy Doc List of annotated spacy docs. extensions: list of extensions List of extensions to use by labeltool. Returns ------- df: pd.DataFrame DataFrame tailored for labeltool. \"\"\" if extensions is None : extensions = [] entities = [] for i , doc in enumerate ( tqdm ( docs , ascii = True , ncols = 100 )): for ent in doc . ents : d = dict ( note_text = doc . text , offset_begin = ent . start_char , offset_end = ent . end_char , label_name = ent . label_ , label_value = ent . text , ) d [ \"note_id\" ] = doc . _ . note_id or i for ext in extensions : d [ ext ] = getattr ( ent . _ , ext ) entities . append ( d ) df = pd . DataFrame . from_records ( entities ) columns = [ \"note_id\" , \"note_text\" , \"offset_begin\" , \"offset_end\" , \"label_name\" , \"label_value\" , ] df = df [ columns + extensions ] return df","title":"docs2labeltool()"},{"location":"reference/connectors/omop/","text":"edsnlp.connectors.omop OmopConnector Bases: object [summary] PARAMETER DESCRIPTION nlp spaCy language object. TYPE: Language start_char Name of the column containing the start character index of the entity, by default \"start_char\" TYPE: str, optional end_char Name of the column containing the end character index of the entity, by default \"end_char\" TYPE: str, optional Source code in edsnlp/connectors/omop.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 class OmopConnector ( object ): \"\"\" [summary] Parameters ---------- nlp : Language spaCy language object. start_char : str, optional Name of the column containing the start character index of the entity, by default \"start_char\" end_char : str, optional Name of the column containing the end character index of the entity, by default \"end_char\" \"\"\" def __init__ ( self , nlp : Language , start_char : str = \"start_char\" , end_char : str = \"end_char\" , ): self . start_char = start_char self . end_char = end_char self . nlp = nlp def preprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Preprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { self . start_char : \"start_char\" , self . end_char : \"end_char\" , } ) return note , note_nlp def postprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Postprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { \"start_char\" : self . start_char , \"end_char\" : self . end_char , } ) return note , note_nlp def omop2docs ( self , note : pd . DataFrame , note_nlp : pd . DataFrame , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms OMOP tables to a list of spaCy documents. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] List of spaCy documents. \"\"\" note , note_nlp = self . preprocess ( note , note_nlp ) return omop2docs ( note , note_nlp , self . nlp , extensions ) def docs2omop ( self , docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy documents to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of spaCy documents. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note , note_nlp = docs2omop ( docs , extensions = extensions ) note , note_nlp = self . postprocess ( note , note_nlp ) return note , note_nlp preprocess ( note , note_nlp ) Preprocess the input OMOP tables: modification of the column names. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def preprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Preprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { self . start_char : \"start_char\" , self . end_char : \"end_char\" , } ) return note , note_nlp postprocess ( note , note_nlp ) Postprocess the input OMOP tables: modification of the column names. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def postprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Postprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { \"start_char\" : self . start_char , \"end_char\" : self . end_char , } ) return note , note_nlp omop2docs ( note , note_nlp , extensions = None ) Transforms OMOP tables to a list of spaCy documents. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION List[Doc] List of spaCy documents. Source code in edsnlp/connectors/omop.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def omop2docs ( self , note : pd . DataFrame , note_nlp : pd . DataFrame , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms OMOP tables to a list of spaCy documents. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] List of spaCy documents. \"\"\" note , note_nlp = self . preprocess ( note , note_nlp ) return omop2docs ( note , note_nlp , self . nlp , extensions ) docs2omop ( docs , extensions = None ) Transforms a list of spaCy documents to a pair of OMOP tables. PARAMETER DESCRIPTION docs List of spaCy documents. TYPE: List[Doc] extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def docs2omop ( self , docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy documents to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of spaCy documents. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note , note_nlp = docs2omop ( docs , extensions = extensions ) note , note_nlp = self . postprocess ( note , note_nlp ) return note , note_nlp omop2docs ( note , note_nlp , nlp , extensions = None ) Transforms an OMOP-formatted pair of dataframes into a list of documents. PARAMETER DESCRIPTION note The OMOP note table. TYPE: pd.DataFrame note_nlp The OMOP note_nlp table TYPE: pd.DataFrame nlp spaCy language object. TYPE: Language extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION List[Doc] List of spaCy documents Source code in edsnlp/connectors/omop.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def omop2docs ( note : pd . DataFrame , note_nlp : pd . DataFrame , nlp : Language , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms an OMOP-formatted pair of dataframes into a list of documents. Parameters ---------- note : pd.DataFrame The OMOP `note` table. note_nlp : pd.DataFrame The OMOP `note_nlp` table nlp : Language spaCy language object. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] : List of spaCy documents \"\"\" note = note . copy () note_nlp = note_nlp . copy () extensions = extensions or [] def row2ent ( row ): d = dict ( start_char = row . start_char , end_char = row . end_char , label = row . get ( \"note_nlp_source_value\" ), extensions = { ext : row . get ( ext ) for ext in extensions }, ) return d # Create entities note_nlp [ \"ents\" ] = note_nlp . apply ( row2ent , axis = 1 ) note_nlp = note_nlp . groupby ( \"note_id\" , as_index = False )[ \"ents\" ] . agg ( list ) note = note . merge ( note_nlp , on = [ \"note_id\" ], how = \"left\" ) # Generate documents note [ \"doc\" ] = note . note_text . apply ( nlp ) # Process documents for _ , row in note . iterrows (): doc = row . doc doc . _ . note_id = row . note_id doc . _ . note_datetime = row . get ( \"note_datetime\" ) ents = [] if not isinstance ( row . ents , list ): continue for ent in row . ents : span = doc . char_span ( ent [ \"start_char\" ], ent [ \"end_char\" ], ent [ \"label\" ], alignment_mode = \"expand\" , ) for k , v in ent [ \"extensions\" ] . items (): setattr ( span . _ , k , v ) ents . append ( span ) if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [ span ] else : doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return list ( note . doc ) docs2omop ( docs , extensions = None ) Transforms a list of spaCy docs to a pair of OMOP tables. PARAMETER DESCRIPTION docs List of documents to transform. TYPE: List[Doc] extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION Tuple[pd.DataFrame, pd.DataFrame] Pair of OMOP tables ( note and note_nlp ) Source code in edsnlp/connectors/omop.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def docs2omop ( docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy docs to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of documents to transform. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- Tuple[pd.DataFrame, pd.DataFrame] Pair of OMOP tables (`note` and `note_nlp`) \"\"\" df = pd . DataFrame ( dict ( doc = docs )) df [ \"note_text\" ] = df . doc . apply ( lambda doc : doc . text ) df [ \"note_id\" ] = df . doc . apply ( lambda doc : doc . _ . note_id ) df [ \"note_datetime\" ] = df . doc . apply ( lambda doc : doc . _ . note_datetime ) if df . note_id . isna () . any (): df [ \"note_id\" ] = range ( len ( df )) df [ \"ents\" ] = df . doc . apply ( lambda doc : list ( doc . ents )) df [ \"ents\" ] += df . doc . apply ( lambda doc : list ( doc . spans [ \"discarded\" ])) note = df [[ \"note_id\" , \"note_text\" , \"note_datetime\" ]] df = df [[ \"note_id\" , \"ents\" ]] . explode ( \"ents\" ) extensions = extensions or [] def ent2dict ( ent : Span , ) -> Dict [ str , Any ]: d = dict ( start_char = ent . start_char , end_char = ent . end_char , note_nlp_source_value = ent . label_ , lexical_variant = ent . text , # normalized_variant=ent._.normalized.text, ) for ext in extensions : d [ ext ] = getattr ( ent . _ , ext ) return d df [ \"ents\" ] = df . ents . apply ( ent2dict ) columns = [ \"start_char\" , \"end_char\" , \"note_nlp_source_value\" , \"lexical_variant\" , # \"normalized_variant\", ] columns += extensions df [ columns ] = df . ents . apply ( pd . Series ) df [ \"term_modifiers\" ] = \"\" for i , ext in enumerate ( extensions ): if i > 0 : df . term_modifiers += \";\" df . term_modifiers += ext + \"=\" + df [ ext ] . astype ( str ) df [ \"note_nlp_id\" ] = range ( len ( df )) note_nlp = df [[ \"note_nlp_id\" , \"note_id\" ] + columns ] return note , note_nlp","title":"omop"},{"location":"reference/connectors/omop/#edsnlpconnectorsomop","text":"","title":"edsnlp.connectors.omop"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector","text":"Bases: object [summary] PARAMETER DESCRIPTION nlp spaCy language object. TYPE: Language start_char Name of the column containing the start character index of the entity, by default \"start_char\" TYPE: str, optional end_char Name of the column containing the end character index of the entity, by default \"end_char\" TYPE: str, optional Source code in edsnlp/connectors/omop.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 class OmopConnector ( object ): \"\"\" [summary] Parameters ---------- nlp : Language spaCy language object. start_char : str, optional Name of the column containing the start character index of the entity, by default \"start_char\" end_char : str, optional Name of the column containing the end character index of the entity, by default \"end_char\" \"\"\" def __init__ ( self , nlp : Language , start_char : str = \"start_char\" , end_char : str = \"end_char\" , ): self . start_char = start_char self . end_char = end_char self . nlp = nlp def preprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Preprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { self . start_char : \"start_char\" , self . end_char : \"end_char\" , } ) return note , note_nlp def postprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Postprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { \"start_char\" : self . start_char , \"end_char\" : self . end_char , } ) return note , note_nlp def omop2docs ( self , note : pd . DataFrame , note_nlp : pd . DataFrame , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms OMOP tables to a list of spaCy documents. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] List of spaCy documents. \"\"\" note , note_nlp = self . preprocess ( note , note_nlp ) return omop2docs ( note , note_nlp , self . nlp , extensions ) def docs2omop ( self , docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy documents to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of spaCy documents. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note , note_nlp = docs2omop ( docs , extensions = extensions ) note , note_nlp = self . postprocess ( note , note_nlp ) return note , note_nlp","title":"OmopConnector"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.preprocess","text":"Preprocess the input OMOP tables: modification of the column names. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def preprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Preprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { self . start_char : \"start_char\" , self . end_char : \"end_char\" , } ) return note , note_nlp","title":"preprocess()"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.postprocess","text":"Postprocess the input OMOP tables: modification of the column names. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def postprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Postprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { \"start_char\" : self . start_char , \"end_char\" : self . end_char , } ) return note , note_nlp","title":"postprocess()"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.omop2docs","text":"Transforms OMOP tables to a list of spaCy documents. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION List[Doc] List of spaCy documents. Source code in edsnlp/connectors/omop.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def omop2docs ( self , note : pd . DataFrame , note_nlp : pd . DataFrame , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms OMOP tables to a list of spaCy documents. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] List of spaCy documents. \"\"\" note , note_nlp = self . preprocess ( note , note_nlp ) return omop2docs ( note , note_nlp , self . nlp , extensions )","title":"omop2docs()"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.docs2omop","text":"Transforms a list of spaCy documents to a pair of OMOP tables. PARAMETER DESCRIPTION docs List of spaCy documents. TYPE: List[Doc] extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def docs2omop ( self , docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy documents to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of spaCy documents. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note , note_nlp = docs2omop ( docs , extensions = extensions ) note , note_nlp = self . postprocess ( note , note_nlp ) return note , note_nlp","title":"docs2omop()"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.omop2docs","text":"Transforms an OMOP-formatted pair of dataframes into a list of documents. PARAMETER DESCRIPTION note The OMOP note table. TYPE: pd.DataFrame note_nlp The OMOP note_nlp table TYPE: pd.DataFrame nlp spaCy language object. TYPE: Language extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION List[Doc] List of spaCy documents Source code in edsnlp/connectors/omop.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def omop2docs ( note : pd . DataFrame , note_nlp : pd . DataFrame , nlp : Language , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms an OMOP-formatted pair of dataframes into a list of documents. Parameters ---------- note : pd.DataFrame The OMOP `note` table. note_nlp : pd.DataFrame The OMOP `note_nlp` table nlp : Language spaCy language object. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] : List of spaCy documents \"\"\" note = note . copy () note_nlp = note_nlp . copy () extensions = extensions or [] def row2ent ( row ): d = dict ( start_char = row . start_char , end_char = row . end_char , label = row . get ( \"note_nlp_source_value\" ), extensions = { ext : row . get ( ext ) for ext in extensions }, ) return d # Create entities note_nlp [ \"ents\" ] = note_nlp . apply ( row2ent , axis = 1 ) note_nlp = note_nlp . groupby ( \"note_id\" , as_index = False )[ \"ents\" ] . agg ( list ) note = note . merge ( note_nlp , on = [ \"note_id\" ], how = \"left\" ) # Generate documents note [ \"doc\" ] = note . note_text . apply ( nlp ) # Process documents for _ , row in note . iterrows (): doc = row . doc doc . _ . note_id = row . note_id doc . _ . note_datetime = row . get ( \"note_datetime\" ) ents = [] if not isinstance ( row . ents , list ): continue for ent in row . ents : span = doc . char_span ( ent [ \"start_char\" ], ent [ \"end_char\" ], ent [ \"label\" ], alignment_mode = \"expand\" , ) for k , v in ent [ \"extensions\" ] . items (): setattr ( span . _ , k , v ) ents . append ( span ) if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [ span ] else : doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return list ( note . doc )","title":"omop2docs()"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.docs2omop","text":"Transforms a list of spaCy docs to a pair of OMOP tables. PARAMETER DESCRIPTION docs List of documents to transform. TYPE: List[Doc] extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION Tuple[pd.DataFrame, pd.DataFrame] Pair of OMOP tables ( note and note_nlp ) Source code in edsnlp/connectors/omop.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def docs2omop ( docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy docs to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of documents to transform. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- Tuple[pd.DataFrame, pd.DataFrame] Pair of OMOP tables (`note` and `note_nlp`) \"\"\" df = pd . DataFrame ( dict ( doc = docs )) df [ \"note_text\" ] = df . doc . apply ( lambda doc : doc . text ) df [ \"note_id\" ] = df . doc . apply ( lambda doc : doc . _ . note_id ) df [ \"note_datetime\" ] = df . doc . apply ( lambda doc : doc . _ . note_datetime ) if df . note_id . isna () . any (): df [ \"note_id\" ] = range ( len ( df )) df [ \"ents\" ] = df . doc . apply ( lambda doc : list ( doc . ents )) df [ \"ents\" ] += df . doc . apply ( lambda doc : list ( doc . spans [ \"discarded\" ])) note = df [[ \"note_id\" , \"note_text\" , \"note_datetime\" ]] df = df [[ \"note_id\" , \"ents\" ]] . explode ( \"ents\" ) extensions = extensions or [] def ent2dict ( ent : Span , ) -> Dict [ str , Any ]: d = dict ( start_char = ent . start_char , end_char = ent . end_char , note_nlp_source_value = ent . label_ , lexical_variant = ent . text , # normalized_variant=ent._.normalized.text, ) for ext in extensions : d [ ext ] = getattr ( ent . _ , ext ) return d df [ \"ents\" ] = df . ents . apply ( ent2dict ) columns = [ \"start_char\" , \"end_char\" , \"note_nlp_source_value\" , \"lexical_variant\" , # \"normalized_variant\", ] columns += extensions df [ columns ] = df . ents . apply ( pd . Series ) df [ \"term_modifiers\" ] = \"\" for i , ext in enumerate ( extensions ): if i > 0 : df . term_modifiers += \";\" df . term_modifiers += ext + \"=\" + df [ ext ] . astype ( str ) df [ \"note_nlp_id\" ] = range ( len ( df )) note_nlp = df [[ \"note_nlp_id\" , \"note_id\" ] + columns ] return note , note_nlp","title":"docs2omop()"},{"location":"reference/matchers/","text":"edsnlp.matchers","title":"`edsnlp.matchers`"},{"location":"reference/matchers/#edsnlpmatchers","text":"","title":"edsnlp.matchers"},{"location":"reference/matchers/phrase/","text":"edsnlp.matchers.phrase EDSPhraseMatcher Bases: object PhraseMatcher that matches \"over\" excluded tokens. PARAMETER DESCRIPTION vocab spaCy vocabulary to match on. TYPE: Vocab attr Default attribute to match on, by default \"TEXT\". Can be overiden in the add method. To match on a custom attribute, prepend the attribute name with _ . TYPE: str ignore_excluded Whether to ignore excluded tokens, by default True TYPE: bool, optional exclude_newlines Whether to exclude new lines, by default False TYPE: bool, optional Source code in edsnlp/matchers/phrase.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class EDSPhraseMatcher ( object ): \"\"\" PhraseMatcher that matches \"over\" excluded tokens. Parameters ---------- vocab : Vocab spaCy vocabulary to match on. attr : str Default attribute to match on, by default \"TEXT\". Can be overiden in the `add` method. To match on a custom attribute, prepend the attribute name with `_`. ignore_excluded : bool, optional Whether to ignore excluded tokens, by default True exclude_newlines : bool, optional Whether to exclude new lines, by default False \"\"\" def __init__ ( self , vocab : Vocab , attr : str = \"TEXT\" , ignore_excluded : bool = True , exclude_newlines : bool = False , ): self . matcher = Matcher ( vocab , validate = True ) self . attr = attr self . ignore_excluded = ignore_excluded self . exclusion_attribute = ( \"excluded_or_space\" if exclude_newlines else \"excluded\" ) @staticmethod def get_attr ( token : Token , attr : str , custom_attr : bool = False ) -> str : if custom_attr : return getattr ( token . _ , attr ) else : attr = ATTRIBUTES . get ( attr ) return getattr ( token , attr ) def create_pattern ( self , match_pattern : Doc , attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , ) -> List [ PatternDict ]: \"\"\" Create a pattern Parameters ---------- match_pattern : Doc A spaCy doc object, to use as match model. attr : str, optional Overwrite attribute to match on. ignore_excluded: bool, optional Whether to skip excluded tokens. Returns ------- List[PatternDict] A spaCy rule-based pattern. \"\"\" ignore_excluded = ignore_excluded or self . ignore_excluded attr = attr or self . attr custom_attr = attr . startswith ( \"_\" ) if custom_attr : attr = attr . lstrip ( \"_\" ) . lower () pattern = [] for token in match_pattern : pattern . append ({ \"_\" : { attr : self . get_attr ( token , attr , True )}}) if ignore_excluded and token . whitespace_ : # If the token is followed by a whitespace, # we let it match on a pollution pattern . append ({ \"_\" : { self . exclusion_attribute : True }, \"OP\" : \"*\" }) return pattern else : pattern = [] for token in match_pattern : pattern . append ({ attr : self . get_attr ( token , attr , False )}) if ignore_excluded and token . whitespace_ : # If the token is followed by a whitespace, # we let it match on a pollution pattern . append ({ \"_\" : { self . exclusion_attribute : True }, \"OP\" : \"*\" }) return pattern def build_patterns ( self , nlp : Language , terms : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- nlp : Language The instance of the spaCy language class. terms : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not terms : terms = dict () for key , expressions in terms . items (): if isinstance ( expressions , dict ): attr = expressions . get ( \"attr\" ) expressions = expressions . get ( \"patterns\" ) else : attr = None if isinstance ( expressions , str ): expressions = [ expressions ] patterns = list ( nlp . pipe ( expressions )) self . add ( key , patterns , attr ) def add ( self , key : str , patterns : List [ Doc ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , ) -> None : \"\"\" Add a pattern. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Overwrite the attribute to match on for this specific pattern. ignore_excluded : bool, optional Overwrite the parameter for this specific pattern. \"\"\" patterns = [ self . create_pattern ( pattern , attr = attr , ignore_excluded = ignore_excluded ) for pattern in patterns ] self . matcher . add ( key , patterns ) def remove ( self , key : str , ) -> None : \"\"\" Remove a pattern. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError Should the key not be contained in the registry. \"\"\" self . matcher . remove ( key ) def __len__ ( self ): return len ( self . matcher ) def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , ) -> Generator : \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Whether to return matches as spans. Yields ------- match: Span A match. \"\"\" if len ( self . matcher ): for match in self . matcher ( doclike , as_spans = as_spans ): yield match create_pattern ( match_pattern , attr = None , ignore_excluded = None ) Create a pattern PARAMETER DESCRIPTION match_pattern A spaCy doc object, to use as match model. TYPE: Doc attr Overwrite attribute to match on. TYPE: str, optional DEFAULT: None ignore_excluded Whether to skip excluded tokens. TYPE: Optional [ bool ] DEFAULT: None RETURNS DESCRIPTION List[PatternDict] A spaCy rule-based pattern. Source code in edsnlp/matchers/phrase.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def create_pattern ( self , match_pattern : Doc , attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , ) -> List [ PatternDict ]: \"\"\" Create a pattern Parameters ---------- match_pattern : Doc A spaCy doc object, to use as match model. attr : str, optional Overwrite attribute to match on. ignore_excluded: bool, optional Whether to skip excluded tokens. Returns ------- List[PatternDict] A spaCy rule-based pattern. \"\"\" ignore_excluded = ignore_excluded or self . ignore_excluded attr = attr or self . attr custom_attr = attr . startswith ( \"_\" ) if custom_attr : attr = attr . lstrip ( \"_\" ) . lower () pattern = [] for token in match_pattern : pattern . append ({ \"_\" : { attr : self . get_attr ( token , attr , True )}}) if ignore_excluded and token . whitespace_ : # If the token is followed by a whitespace, # we let it match on a pollution pattern . append ({ \"_\" : { self . exclusion_attribute : True }, \"OP\" : \"*\" }) return pattern else : pattern = [] for token in match_pattern : pattern . append ({ attr : self . get_attr ( token , attr , False )}) if ignore_excluded and token . whitespace_ : # If the token is followed by a whitespace, # we let it match on a pollution pattern . append ({ \"_\" : { self . exclusion_attribute : True }, \"OP\" : \"*\" }) return pattern build_patterns ( nlp , terms ) Build patterns and adds them for matching. Helper function for pipelines using this matcher. PARAMETER DESCRIPTION nlp The instance of the spaCy language class. TYPE: Language terms Dictionary of label/terms, or label/dictionary of terms/attribute. TYPE: Patterns Source code in edsnlp/matchers/phrase.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def build_patterns ( self , nlp : Language , terms : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- nlp : Language The instance of the spaCy language class. terms : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not terms : terms = dict () for key , expressions in terms . items (): if isinstance ( expressions , dict ): attr = expressions . get ( \"attr\" ) expressions = expressions . get ( \"patterns\" ) else : attr = None if isinstance ( expressions , str ): expressions = [ expressions ] patterns = list ( nlp . pipe ( expressions )) self . add ( key , patterns , attr ) add ( key , patterns , attr = None , ignore_excluded = None ) Add a pattern. PARAMETER DESCRIPTION key Key of the new/updated pattern. TYPE: str patterns List of patterns to add. TYPE: List[str] attr Overwrite the attribute to match on for this specific pattern. TYPE: str, optional DEFAULT: None ignore_excluded Overwrite the parameter for this specific pattern. TYPE: bool, optional DEFAULT: None Source code in edsnlp/matchers/phrase.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def add ( self , key : str , patterns : List [ Doc ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , ) -> None : \"\"\" Add a pattern. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Overwrite the attribute to match on for this specific pattern. ignore_excluded : bool, optional Overwrite the parameter for this specific pattern. \"\"\" patterns = [ self . create_pattern ( pattern , attr = attr , ignore_excluded = ignore_excluded ) for pattern in patterns ] self . matcher . add ( key , patterns ) remove ( key ) Remove a pattern. PARAMETER DESCRIPTION key key of the pattern to remove. TYPE: str RAISES DESCRIPTION ValueError Should the key not be contained in the registry. Source code in edsnlp/matchers/phrase.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def remove ( self , key : str , ) -> None : \"\"\" Remove a pattern. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError Should the key not be contained in the registry. \"\"\" self . matcher . remove ( key ) __call__ ( doclike , as_spans = False ) Performs matching. Yields matches. PARAMETER DESCRIPTION doclike spaCy Doc or Span object. TYPE: Union [ Doc , Span ] as_spans Whether to return matches as spans. DEFAULT: False YIELDS DESCRIPTION match A match. Source code in edsnlp/matchers/phrase.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , ) -> Generator : \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Whether to return matches as spans. Yields ------- match: Span A match. \"\"\" if len ( self . matcher ): for match in self . matcher ( doclike , as_spans = as_spans ): yield match","title":"phrase"},{"location":"reference/matchers/phrase/#edsnlpmatchersphrase","text":"","title":"edsnlp.matchers.phrase"},{"location":"reference/matchers/phrase/#edsnlp.matchers.phrase.EDSPhraseMatcher","text":"Bases: object PhraseMatcher that matches \"over\" excluded tokens. PARAMETER DESCRIPTION vocab spaCy vocabulary to match on. TYPE: Vocab attr Default attribute to match on, by default \"TEXT\". Can be overiden in the add method. To match on a custom attribute, prepend the attribute name with _ . TYPE: str ignore_excluded Whether to ignore excluded tokens, by default True TYPE: bool, optional exclude_newlines Whether to exclude new lines, by default False TYPE: bool, optional Source code in edsnlp/matchers/phrase.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class EDSPhraseMatcher ( object ): \"\"\" PhraseMatcher that matches \"over\" excluded tokens. Parameters ---------- vocab : Vocab spaCy vocabulary to match on. attr : str Default attribute to match on, by default \"TEXT\". Can be overiden in the `add` method. To match on a custom attribute, prepend the attribute name with `_`. ignore_excluded : bool, optional Whether to ignore excluded tokens, by default True exclude_newlines : bool, optional Whether to exclude new lines, by default False \"\"\" def __init__ ( self , vocab : Vocab , attr : str = \"TEXT\" , ignore_excluded : bool = True , exclude_newlines : bool = False , ): self . matcher = Matcher ( vocab , validate = True ) self . attr = attr self . ignore_excluded = ignore_excluded self . exclusion_attribute = ( \"excluded_or_space\" if exclude_newlines else \"excluded\" ) @staticmethod def get_attr ( token : Token , attr : str , custom_attr : bool = False ) -> str : if custom_attr : return getattr ( token . _ , attr ) else : attr = ATTRIBUTES . get ( attr ) return getattr ( token , attr ) def create_pattern ( self , match_pattern : Doc , attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , ) -> List [ PatternDict ]: \"\"\" Create a pattern Parameters ---------- match_pattern : Doc A spaCy doc object, to use as match model. attr : str, optional Overwrite attribute to match on. ignore_excluded: bool, optional Whether to skip excluded tokens. Returns ------- List[PatternDict] A spaCy rule-based pattern. \"\"\" ignore_excluded = ignore_excluded or self . ignore_excluded attr = attr or self . attr custom_attr = attr . startswith ( \"_\" ) if custom_attr : attr = attr . lstrip ( \"_\" ) . lower () pattern = [] for token in match_pattern : pattern . append ({ \"_\" : { attr : self . get_attr ( token , attr , True )}}) if ignore_excluded and token . whitespace_ : # If the token is followed by a whitespace, # we let it match on a pollution pattern . append ({ \"_\" : { self . exclusion_attribute : True }, \"OP\" : \"*\" }) return pattern else : pattern = [] for token in match_pattern : pattern . append ({ attr : self . get_attr ( token , attr , False )}) if ignore_excluded and token . whitespace_ : # If the token is followed by a whitespace, # we let it match on a pollution pattern . append ({ \"_\" : { self . exclusion_attribute : True }, \"OP\" : \"*\" }) return pattern def build_patterns ( self , nlp : Language , terms : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- nlp : Language The instance of the spaCy language class. terms : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not terms : terms = dict () for key , expressions in terms . items (): if isinstance ( expressions , dict ): attr = expressions . get ( \"attr\" ) expressions = expressions . get ( \"patterns\" ) else : attr = None if isinstance ( expressions , str ): expressions = [ expressions ] patterns = list ( nlp . pipe ( expressions )) self . add ( key , patterns , attr ) def add ( self , key : str , patterns : List [ Doc ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , ) -> None : \"\"\" Add a pattern. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Overwrite the attribute to match on for this specific pattern. ignore_excluded : bool, optional Overwrite the parameter for this specific pattern. \"\"\" patterns = [ self . create_pattern ( pattern , attr = attr , ignore_excluded = ignore_excluded ) for pattern in patterns ] self . matcher . add ( key , patterns ) def remove ( self , key : str , ) -> None : \"\"\" Remove a pattern. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError Should the key not be contained in the registry. \"\"\" self . matcher . remove ( key ) def __len__ ( self ): return len ( self . matcher ) def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , ) -> Generator : \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Whether to return matches as spans. Yields ------- match: Span A match. \"\"\" if len ( self . matcher ): for match in self . matcher ( doclike , as_spans = as_spans ): yield match","title":"EDSPhraseMatcher"},{"location":"reference/matchers/phrase/#edsnlp.matchers.phrase.EDSPhraseMatcher.create_pattern","text":"Create a pattern PARAMETER DESCRIPTION match_pattern A spaCy doc object, to use as match model. TYPE: Doc attr Overwrite attribute to match on. TYPE: str, optional DEFAULT: None ignore_excluded Whether to skip excluded tokens. TYPE: Optional [ bool ] DEFAULT: None RETURNS DESCRIPTION List[PatternDict] A spaCy rule-based pattern. Source code in edsnlp/matchers/phrase.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def create_pattern ( self , match_pattern : Doc , attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , ) -> List [ PatternDict ]: \"\"\" Create a pattern Parameters ---------- match_pattern : Doc A spaCy doc object, to use as match model. attr : str, optional Overwrite attribute to match on. ignore_excluded: bool, optional Whether to skip excluded tokens. Returns ------- List[PatternDict] A spaCy rule-based pattern. \"\"\" ignore_excluded = ignore_excluded or self . ignore_excluded attr = attr or self . attr custom_attr = attr . startswith ( \"_\" ) if custom_attr : attr = attr . lstrip ( \"_\" ) . lower () pattern = [] for token in match_pattern : pattern . append ({ \"_\" : { attr : self . get_attr ( token , attr , True )}}) if ignore_excluded and token . whitespace_ : # If the token is followed by a whitespace, # we let it match on a pollution pattern . append ({ \"_\" : { self . exclusion_attribute : True }, \"OP\" : \"*\" }) return pattern else : pattern = [] for token in match_pattern : pattern . append ({ attr : self . get_attr ( token , attr , False )}) if ignore_excluded and token . whitespace_ : # If the token is followed by a whitespace, # we let it match on a pollution pattern . append ({ \"_\" : { self . exclusion_attribute : True }, \"OP\" : \"*\" }) return pattern","title":"create_pattern()"},{"location":"reference/matchers/phrase/#edsnlp.matchers.phrase.EDSPhraseMatcher.build_patterns","text":"Build patterns and adds them for matching. Helper function for pipelines using this matcher. PARAMETER DESCRIPTION nlp The instance of the spaCy language class. TYPE: Language terms Dictionary of label/terms, or label/dictionary of terms/attribute. TYPE: Patterns Source code in edsnlp/matchers/phrase.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def build_patterns ( self , nlp : Language , terms : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- nlp : Language The instance of the spaCy language class. terms : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not terms : terms = dict () for key , expressions in terms . items (): if isinstance ( expressions , dict ): attr = expressions . get ( \"attr\" ) expressions = expressions . get ( \"patterns\" ) else : attr = None if isinstance ( expressions , str ): expressions = [ expressions ] patterns = list ( nlp . pipe ( expressions )) self . add ( key , patterns , attr )","title":"build_patterns()"},{"location":"reference/matchers/phrase/#edsnlp.matchers.phrase.EDSPhraseMatcher.add","text":"Add a pattern. PARAMETER DESCRIPTION key Key of the new/updated pattern. TYPE: str patterns List of patterns to add. TYPE: List[str] attr Overwrite the attribute to match on for this specific pattern. TYPE: str, optional DEFAULT: None ignore_excluded Overwrite the parameter for this specific pattern. TYPE: bool, optional DEFAULT: None Source code in edsnlp/matchers/phrase.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def add ( self , key : str , patterns : List [ Doc ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , ) -> None : \"\"\" Add a pattern. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Overwrite the attribute to match on for this specific pattern. ignore_excluded : bool, optional Overwrite the parameter for this specific pattern. \"\"\" patterns = [ self . create_pattern ( pattern , attr = attr , ignore_excluded = ignore_excluded ) for pattern in patterns ] self . matcher . add ( key , patterns )","title":"add()"},{"location":"reference/matchers/phrase/#edsnlp.matchers.phrase.EDSPhraseMatcher.remove","text":"Remove a pattern. PARAMETER DESCRIPTION key key of the pattern to remove. TYPE: str RAISES DESCRIPTION ValueError Should the key not be contained in the registry. Source code in edsnlp/matchers/phrase.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def remove ( self , key : str , ) -> None : \"\"\" Remove a pattern. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError Should the key not be contained in the registry. \"\"\" self . matcher . remove ( key )","title":"remove()"},{"location":"reference/matchers/phrase/#edsnlp.matchers.phrase.EDSPhraseMatcher.__call__","text":"Performs matching. Yields matches. PARAMETER DESCRIPTION doclike spaCy Doc or Span object. TYPE: Union [ Doc , Span ] as_spans Whether to return matches as spans. DEFAULT: False YIELDS DESCRIPTION match A match. Source code in edsnlp/matchers/phrase.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , ) -> Generator : \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Whether to return matches as spans. Yields ------- match: Span A match. \"\"\" if len ( self . matcher ): for match in self . matcher ( doclike , as_spans = as_spans ): yield match","title":"__call__()"},{"location":"reference/matchers/regex/","text":"edsnlp.matchers.regex RegexMatcher Bases: object Simple RegExp matcher. PARAMETER DESCRIPTION alignment_mode How spans should be aligned with tokens. Possible values are strict (character indices must be aligned with token boundaries), \"contract\" (span of all tokens completely within the character span), \"expand\" (span of all tokens at least partially covered by the character span). Defaults to expand . TYPE: str attr Default attribute to match on, by default \"TEXT\". Can be overiden in the add method. TYPE: str ignore_excluded Whether to skip exclusions TYPE: bool Source code in edsnlp/matchers/regex.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 class RegexMatcher ( object ): \"\"\" Simple RegExp matcher. Parameters ---------- alignment_mode : str How spans should be aligned with tokens. Possible values are `strict` (character indices must be aligned with token boundaries), \"contract\" (span of all tokens completely within the character span), \"expand\" (span of all tokens at least partially covered by the character span). Defaults to `expand`. attr : str Default attribute to match on, by default \"TEXT\". Can be overiden in the `add` method. ignore_excluded : bool Whether to skip exclusions \"\"\" def __init__ ( self , alignment_mode : str = \"expand\" , attr : str = \"TEXT\" , ignore_excluded : bool = False , ): self . alignment_mode = alignment_mode self . regex = [] self . default_attr = attr self . ignore_excluded = ignore_excluded def build_patterns ( self , regex : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- regex : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not regex : regex = dict () for key , patterns in regex . items (): if isinstance ( patterns , dict ): attr = patterns . get ( \"attr\" ) alignment_mode = patterns . get ( \"alignment_mode\" ) patterns = patterns . get ( \"regex\" ) else : attr = None alignment_mode = None if isinstance ( patterns , str ): patterns = [ patterns ] self . add ( key = key , patterns = patterns , attr = attr , alignment_mode = alignment_mode ) def add ( self , key : str , patterns : List [ str ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , alignment_mode : Optional [ str ] = None , ): \"\"\" Add a pattern to the registry. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Attribute to use for matching. By default uses the `default_attr` attribute ignore_excluded : bool, optional Whether to skip excluded tokens during matching. alignment_mode : str, optional Overwrite alignment mode. \"\"\" if attr is None : attr = self . default_attr if ignore_excluded is None : ignore_excluded = self . ignore_excluded if alignment_mode is None : alignment_mode = self . alignment_mode patterns = [ re . compile ( pattern ) for pattern in patterns ] self . regex . append (( key , patterns , attr , ignore_excluded , alignment_mode )) def remove ( self , key : str , ): \"\"\" Remove a pattern for the registry. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError If the key is not present in the registered patterns. \"\"\" n = len ( self . regex ) self . regex = [( k , p , a , i , am ) for k , p , a , i , am in self . regex if k != key ] if len ( self . regex ) == n : raise ValueError ( f \"` { key } ` is not referenced in the matcher\" ) def __len__ ( self ): return len ( set ([ regex [ 0 ] for regex in self . regex ])) def match ( self , doclike : Union [ Doc , Span ], ) -> Tuple [ Span , re . Match ]: \"\"\" Iterates on the matches. Parameters ---------- doclike: spaCy Doc or Span object to match on. Yields ------- span: A match. \"\"\" for key , patterns , attr , ignore_excluded , alignment_mode in self . regex : text = get_text ( doclike , attr , ignore_excluded ) for pattern in patterns : for match in pattern . finditer ( text ): logger . trace ( f \"Matched a regex from { key } : { repr ( match . group ()) } \" ) span = create_span ( doclike = doclike , start_char = match . start (), end_char = match . end (), key = key , attr = attr , alignment_mode = alignment_mode , ignore_excluded = ignore_excluded , ) if span is None : continue yield span , match def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , return_groupdict = False , ) -> Union [ Span , Tuple [ Span , Dict [ str , Any ]]]: \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Returns matches as spans. Yields ------ span: A match. groupdict: Additional information coming from the named patterns in the regular expression. \"\"\" for span , match in self . match ( doclike ): if not as_spans : offset = doclike [ 0 ] . i span = ( span . label , span . start - offset , span . end - offset ) if return_groupdict : yield span , match . groupdict () else : yield span build_patterns ( regex ) Build patterns and adds them for matching. Helper function for pipelines using this matcher. PARAMETER DESCRIPTION regex Dictionary of label/terms, or label/dictionary of terms/attribute. TYPE: Patterns Source code in edsnlp/matchers/regex.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def build_patterns ( self , regex : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- regex : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not regex : regex = dict () for key , patterns in regex . items (): if isinstance ( patterns , dict ): attr = patterns . get ( \"attr\" ) alignment_mode = patterns . get ( \"alignment_mode\" ) patterns = patterns . get ( \"regex\" ) else : attr = None alignment_mode = None if isinstance ( patterns , str ): patterns = [ patterns ] self . add ( key = key , patterns = patterns , attr = attr , alignment_mode = alignment_mode ) add ( key , patterns , attr = None , ignore_excluded = None , alignment_mode = None ) Add a pattern to the registry. PARAMETER DESCRIPTION key Key of the new/updated pattern. TYPE: str patterns List of patterns to add. TYPE: List[str] attr Attribute to use for matching. By default uses the default_attr attribute TYPE: str, optional DEFAULT: None ignore_excluded Whether to skip excluded tokens during matching. TYPE: bool, optional DEFAULT: None alignment_mode Overwrite alignment mode. TYPE: str, optional DEFAULT: None Source code in edsnlp/matchers/regex.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def add ( self , key : str , patterns : List [ str ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , alignment_mode : Optional [ str ] = None , ): \"\"\" Add a pattern to the registry. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Attribute to use for matching. By default uses the `default_attr` attribute ignore_excluded : bool, optional Whether to skip excluded tokens during matching. alignment_mode : str, optional Overwrite alignment mode. \"\"\" if attr is None : attr = self . default_attr if ignore_excluded is None : ignore_excluded = self . ignore_excluded if alignment_mode is None : alignment_mode = self . alignment_mode patterns = [ re . compile ( pattern ) for pattern in patterns ] self . regex . append (( key , patterns , attr , ignore_excluded , alignment_mode )) remove ( key ) Remove a pattern for the registry. PARAMETER DESCRIPTION key key of the pattern to remove. TYPE: str RAISES DESCRIPTION ValueError If the key is not present in the registered patterns. Source code in edsnlp/matchers/regex.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def remove ( self , key : str , ): \"\"\" Remove a pattern for the registry. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError If the key is not present in the registered patterns. \"\"\" n = len ( self . regex ) self . regex = [( k , p , a , i , am ) for k , p , a , i , am in self . regex if k != key ] if len ( self . regex ) == n : raise ValueError ( f \"` { key } ` is not referenced in the matcher\" ) match ( doclike ) Iterates on the matches. PARAMETER DESCRIPTION doclike spaCy Doc or Span object to match on. TYPE: Union [ Doc , Span ] YIELDS DESCRIPTION span A match. Source code in edsnlp/matchers/regex.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def match ( self , doclike : Union [ Doc , Span ], ) -> Tuple [ Span , re . Match ]: \"\"\" Iterates on the matches. Parameters ---------- doclike: spaCy Doc or Span object to match on. Yields ------- span: A match. \"\"\" for key , patterns , attr , ignore_excluded , alignment_mode in self . regex : text = get_text ( doclike , attr , ignore_excluded ) for pattern in patterns : for match in pattern . finditer ( text ): logger . trace ( f \"Matched a regex from { key } : { repr ( match . group ()) } \" ) span = create_span ( doclike = doclike , start_char = match . start (), end_char = match . end (), key = key , attr = attr , alignment_mode = alignment_mode , ignore_excluded = ignore_excluded , ) if span is None : continue yield span , match __call__ ( doclike , as_spans = False , return_groupdict = False ) Performs matching. Yields matches. PARAMETER DESCRIPTION doclike spaCy Doc or Span object. TYPE: Union [ Doc , Span ] as_spans Returns matches as spans. DEFAULT: False YIELDS DESCRIPTION span A match. groupdict Additional information coming from the named patterns in the regular expression. Source code in edsnlp/matchers/regex.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , return_groupdict = False , ) -> Union [ Span , Tuple [ Span , Dict [ str , Any ]]]: \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Returns matches as spans. Yields ------ span: A match. groupdict: Additional information coming from the named patterns in the regular expression. \"\"\" for span , match in self . match ( doclike ): if not as_spans : offset = doclike [ 0 ] . i span = ( span . label , span . start - offset , span . end - offset ) if return_groupdict : yield span , match . groupdict () else : yield span create_span ( doclike , start_char , end_char , key , attr , alignment_mode , ignore_excluded ) spaCy only allows strict alignment mode for char_span on Spans. This method circumvents this. PARAMETER DESCRIPTION doclike Doc or Span . TYPE: Union[Doc, Span] start_char Character index within the Doc-like object. TYPE: int end_char Character index of the end, within the Doc-like object. TYPE: int key The key used to match. TYPE: str alignment_mode The alignment mode. TYPE: str ignore_excluded Whether to skip excluded tokens. TYPE: bool RETURNS DESCRIPTION span A span matched on the Doc-like object. Source code in edsnlp/matchers/regex.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def create_span ( doclike : Union [ Doc , Span ], start_char : int , end_char : int , key : str , attr : str , alignment_mode : str , ignore_excluded : bool , ) -> Span : \"\"\" spaCy only allows strict alignment mode for char_span on Spans. This method circumvents this. Parameters ---------- doclike : Union[Doc, Span] `Doc` or `Span`. start_char : int Character index within the Doc-like object. end_char : int Character index of the end, within the Doc-like object. key : str The key used to match. alignment_mode : str The alignment mode. ignore_excluded : bool Whether to skip excluded tokens. Returns ------- span: A span matched on the Doc-like object. \"\"\" doc = doclike if isinstance ( doclike , Doc ) else doclike . doc # Handle the simple case immediately if attr in { \"TEXT\" , \"LOWER\" } and not ignore_excluded : off = doclike [ 0 ] . idx return doc . char_span ( start_char + off , end_char + off , label = key , alignment_mode = alignment_mode , ) # If doclike is a Span, we need to get the clean # index of the first included token if ignore_excluded : original , clean = alignment ( doc = doc , attr = attr , ignore_excluded = ignore_excluded , ) first_included = get_first_included ( doclike ) i = bisect_left ( original , first_included . idx ) first = clean [ i ] else : first = doclike [ 0 ] . idx start_char += offset ( doc , attr = attr , ignore_excluded = ignore_excluded , index = first + start_char , ) end_char += offset ( doc , attr = attr , ignore_excluded = ignore_excluded , index = first + end_char , ) span = doc . char_span ( start_char , end_char , label = key , alignment_mode = alignment_mode , ) return span","title":"regex"},{"location":"reference/matchers/regex/#edsnlpmatchersregex","text":"","title":"edsnlp.matchers.regex"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher","text":"Bases: object Simple RegExp matcher. PARAMETER DESCRIPTION alignment_mode How spans should be aligned with tokens. Possible values are strict (character indices must be aligned with token boundaries), \"contract\" (span of all tokens completely within the character span), \"expand\" (span of all tokens at least partially covered by the character span). Defaults to expand . TYPE: str attr Default attribute to match on, by default \"TEXT\". Can be overiden in the add method. TYPE: str ignore_excluded Whether to skip exclusions TYPE: bool Source code in edsnlp/matchers/regex.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 class RegexMatcher ( object ): \"\"\" Simple RegExp matcher. Parameters ---------- alignment_mode : str How spans should be aligned with tokens. Possible values are `strict` (character indices must be aligned with token boundaries), \"contract\" (span of all tokens completely within the character span), \"expand\" (span of all tokens at least partially covered by the character span). Defaults to `expand`. attr : str Default attribute to match on, by default \"TEXT\". Can be overiden in the `add` method. ignore_excluded : bool Whether to skip exclusions \"\"\" def __init__ ( self , alignment_mode : str = \"expand\" , attr : str = \"TEXT\" , ignore_excluded : bool = False , ): self . alignment_mode = alignment_mode self . regex = [] self . default_attr = attr self . ignore_excluded = ignore_excluded def build_patterns ( self , regex : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- regex : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not regex : regex = dict () for key , patterns in regex . items (): if isinstance ( patterns , dict ): attr = patterns . get ( \"attr\" ) alignment_mode = patterns . get ( \"alignment_mode\" ) patterns = patterns . get ( \"regex\" ) else : attr = None alignment_mode = None if isinstance ( patterns , str ): patterns = [ patterns ] self . add ( key = key , patterns = patterns , attr = attr , alignment_mode = alignment_mode ) def add ( self , key : str , patterns : List [ str ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , alignment_mode : Optional [ str ] = None , ): \"\"\" Add a pattern to the registry. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Attribute to use for matching. By default uses the `default_attr` attribute ignore_excluded : bool, optional Whether to skip excluded tokens during matching. alignment_mode : str, optional Overwrite alignment mode. \"\"\" if attr is None : attr = self . default_attr if ignore_excluded is None : ignore_excluded = self . ignore_excluded if alignment_mode is None : alignment_mode = self . alignment_mode patterns = [ re . compile ( pattern ) for pattern in patterns ] self . regex . append (( key , patterns , attr , ignore_excluded , alignment_mode )) def remove ( self , key : str , ): \"\"\" Remove a pattern for the registry. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError If the key is not present in the registered patterns. \"\"\" n = len ( self . regex ) self . regex = [( k , p , a , i , am ) for k , p , a , i , am in self . regex if k != key ] if len ( self . regex ) == n : raise ValueError ( f \"` { key } ` is not referenced in the matcher\" ) def __len__ ( self ): return len ( set ([ regex [ 0 ] for regex in self . regex ])) def match ( self , doclike : Union [ Doc , Span ], ) -> Tuple [ Span , re . Match ]: \"\"\" Iterates on the matches. Parameters ---------- doclike: spaCy Doc or Span object to match on. Yields ------- span: A match. \"\"\" for key , patterns , attr , ignore_excluded , alignment_mode in self . regex : text = get_text ( doclike , attr , ignore_excluded ) for pattern in patterns : for match in pattern . finditer ( text ): logger . trace ( f \"Matched a regex from { key } : { repr ( match . group ()) } \" ) span = create_span ( doclike = doclike , start_char = match . start (), end_char = match . end (), key = key , attr = attr , alignment_mode = alignment_mode , ignore_excluded = ignore_excluded , ) if span is None : continue yield span , match def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , return_groupdict = False , ) -> Union [ Span , Tuple [ Span , Dict [ str , Any ]]]: \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Returns matches as spans. Yields ------ span: A match. groupdict: Additional information coming from the named patterns in the regular expression. \"\"\" for span , match in self . match ( doclike ): if not as_spans : offset = doclike [ 0 ] . i span = ( span . label , span . start - offset , span . end - offset ) if return_groupdict : yield span , match . groupdict () else : yield span","title":"RegexMatcher"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.build_patterns","text":"Build patterns and adds them for matching. Helper function for pipelines using this matcher. PARAMETER DESCRIPTION regex Dictionary of label/terms, or label/dictionary of terms/attribute. TYPE: Patterns Source code in edsnlp/matchers/regex.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def build_patterns ( self , regex : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- regex : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not regex : regex = dict () for key , patterns in regex . items (): if isinstance ( patterns , dict ): attr = patterns . get ( \"attr\" ) alignment_mode = patterns . get ( \"alignment_mode\" ) patterns = patterns . get ( \"regex\" ) else : attr = None alignment_mode = None if isinstance ( patterns , str ): patterns = [ patterns ] self . add ( key = key , patterns = patterns , attr = attr , alignment_mode = alignment_mode )","title":"build_patterns()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.add","text":"Add a pattern to the registry. PARAMETER DESCRIPTION key Key of the new/updated pattern. TYPE: str patterns List of patterns to add. TYPE: List[str] attr Attribute to use for matching. By default uses the default_attr attribute TYPE: str, optional DEFAULT: None ignore_excluded Whether to skip excluded tokens during matching. TYPE: bool, optional DEFAULT: None alignment_mode Overwrite alignment mode. TYPE: str, optional DEFAULT: None Source code in edsnlp/matchers/regex.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def add ( self , key : str , patterns : List [ str ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , alignment_mode : Optional [ str ] = None , ): \"\"\" Add a pattern to the registry. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Attribute to use for matching. By default uses the `default_attr` attribute ignore_excluded : bool, optional Whether to skip excluded tokens during matching. alignment_mode : str, optional Overwrite alignment mode. \"\"\" if attr is None : attr = self . default_attr if ignore_excluded is None : ignore_excluded = self . ignore_excluded if alignment_mode is None : alignment_mode = self . alignment_mode patterns = [ re . compile ( pattern ) for pattern in patterns ] self . regex . append (( key , patterns , attr , ignore_excluded , alignment_mode ))","title":"add()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.remove","text":"Remove a pattern for the registry. PARAMETER DESCRIPTION key key of the pattern to remove. TYPE: str RAISES DESCRIPTION ValueError If the key is not present in the registered patterns. Source code in edsnlp/matchers/regex.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def remove ( self , key : str , ): \"\"\" Remove a pattern for the registry. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError If the key is not present in the registered patterns. \"\"\" n = len ( self . regex ) self . regex = [( k , p , a , i , am ) for k , p , a , i , am in self . regex if k != key ] if len ( self . regex ) == n : raise ValueError ( f \"` { key } ` is not referenced in the matcher\" )","title":"remove()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match","text":"Iterates on the matches. PARAMETER DESCRIPTION doclike spaCy Doc or Span object to match on. TYPE: Union [ Doc , Span ] YIELDS DESCRIPTION span A match. Source code in edsnlp/matchers/regex.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def match ( self , doclike : Union [ Doc , Span ], ) -> Tuple [ Span , re . Match ]: \"\"\" Iterates on the matches. Parameters ---------- doclike: spaCy Doc or Span object to match on. Yields ------- span: A match. \"\"\" for key , patterns , attr , ignore_excluded , alignment_mode in self . regex : text = get_text ( doclike , attr , ignore_excluded ) for pattern in patterns : for match in pattern . finditer ( text ): logger . trace ( f \"Matched a regex from { key } : { repr ( match . group ()) } \" ) span = create_span ( doclike = doclike , start_char = match . start (), end_char = match . end (), key = key , attr = attr , alignment_mode = alignment_mode , ignore_excluded = ignore_excluded , ) if span is None : continue yield span , match","title":"match()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.__call__","text":"Performs matching. Yields matches. PARAMETER DESCRIPTION doclike spaCy Doc or Span object. TYPE: Union [ Doc , Span ] as_spans Returns matches as spans. DEFAULT: False YIELDS DESCRIPTION span A match. groupdict Additional information coming from the named patterns in the regular expression. Source code in edsnlp/matchers/regex.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , return_groupdict = False , ) -> Union [ Span , Tuple [ Span , Dict [ str , Any ]]]: \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Returns matches as spans. Yields ------ span: A match. groupdict: Additional information coming from the named patterns in the regular expression. \"\"\" for span , match in self . match ( doclike ): if not as_spans : offset = doclike [ 0 ] . i span = ( span . label , span . start - offset , span . end - offset ) if return_groupdict : yield span , match . groupdict () else : yield span","title":"__call__()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.create_span","text":"spaCy only allows strict alignment mode for char_span on Spans. This method circumvents this. PARAMETER DESCRIPTION doclike Doc or Span . TYPE: Union[Doc, Span] start_char Character index within the Doc-like object. TYPE: int end_char Character index of the end, within the Doc-like object. TYPE: int key The key used to match. TYPE: str alignment_mode The alignment mode. TYPE: str ignore_excluded Whether to skip excluded tokens. TYPE: bool RETURNS DESCRIPTION span A span matched on the Doc-like object. Source code in edsnlp/matchers/regex.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def create_span ( doclike : Union [ Doc , Span ], start_char : int , end_char : int , key : str , attr : str , alignment_mode : str , ignore_excluded : bool , ) -> Span : \"\"\" spaCy only allows strict alignment mode for char_span on Spans. This method circumvents this. Parameters ---------- doclike : Union[Doc, Span] `Doc` or `Span`. start_char : int Character index within the Doc-like object. end_char : int Character index of the end, within the Doc-like object. key : str The key used to match. alignment_mode : str The alignment mode. ignore_excluded : bool Whether to skip excluded tokens. Returns ------- span: A span matched on the Doc-like object. \"\"\" doc = doclike if isinstance ( doclike , Doc ) else doclike . doc # Handle the simple case immediately if attr in { \"TEXT\" , \"LOWER\" } and not ignore_excluded : off = doclike [ 0 ] . idx return doc . char_span ( start_char + off , end_char + off , label = key , alignment_mode = alignment_mode , ) # If doclike is a Span, we need to get the clean # index of the first included token if ignore_excluded : original , clean = alignment ( doc = doc , attr = attr , ignore_excluded = ignore_excluded , ) first_included = get_first_included ( doclike ) i = bisect_left ( original , first_included . idx ) first = clean [ i ] else : first = doclike [ 0 ] . idx start_char += offset ( doc , attr = attr , ignore_excluded = ignore_excluded , index = first + start_char , ) end_char += offset ( doc , attr = attr , ignore_excluded = ignore_excluded , index = first + end_char , ) span = doc . char_span ( start_char , end_char , label = key , alignment_mode = alignment_mode , ) return span","title":"create_span()"},{"location":"reference/matchers/utils/","text":"edsnlp.matchers.utils offset alignment ( doc , attr = 'TEXT' , ignore_excluded = True ) Align different representations of a Doc or Span object. PARAMETER DESCRIPTION doc spaCy Doc or Span object TYPE: Doc attr Attribute to use, by default \"TEXT\" TYPE: str, optional DEFAULT: 'TEXT' ignore_excluded Whether to remove excluded tokens, by default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION Tuple[List[int], List[int]] An alignment tuple: original and clean lists. Source code in edsnlp/matchers/utils/offset.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @lru_cache ( maxsize = 32 ) def alignment ( doc : Doc , attr : str = \"TEXT\" , ignore_excluded : bool = True , ) -> Tuple [ List [ int ], List [ int ]]: \"\"\" Align different representations of a `Doc` or `Span` object. Parameters ---------- doc : Doc spaCy `Doc` or `Span` object attr : str, optional Attribute to use, by default `\"TEXT\"` ignore_excluded : bool, optional Whether to remove excluded tokens, by default True Returns ------- Tuple[List[int], List[int]] An alignment tuple: original and clean lists. \"\"\" assert isinstance ( doc , Doc ) attr = attr . upper () attr = ATTRIBUTES . get ( attr , attr ) custom = attr . startswith ( \"_\" ) if custom : attr = attr [ 1 :] . lower () # Define the length function length = partial ( token_length , custom = custom , attr = attr ) original = [] clean = [] cursor = 0 for token in doc : if not ignore_excluded or not token . _ . excluded : # The token is not excluded, we add its extremities to the list original . append ( token . idx ) # We add the cursor clean . append ( cursor ) cursor += length ( token ) if token . whitespace_ : cursor += 1 return original , clean offset ( doc , attr , ignore_excluded , index ) Compute offset between the original text and a given representation (defined by the couple attr , ignore_excluded ). The alignment itself is computed with alignment . PARAMETER DESCRIPTION doc The spaCy Doc object TYPE: Doc attr The attribute used by the RegexMatcher (eg NORM ) TYPE: str ignore_excluded Whether the RegexMatcher ignores excluded tokens. TYPE: bool index The index in the pre-processed text. TYPE: int RETURNS DESCRIPTION int The offset. To get the character index in the original document, just do: original = index + offset ( doc , attr , ignore_excluded , index ) Source code in edsnlp/matchers/utils/offset.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def offset ( doc : Doc , attr : str , ignore_excluded : bool , index : int , ) -> int : \"\"\" Compute offset between the original text and a given representation (defined by the couple `attr`, `ignore_excluded`). The alignment itself is computed with [`alignment`][edsnlp.matchers.utils.offset.alignment]. Parameters ---------- doc : Doc The spaCy `Doc` object attr : str The attribute used by the [`RegexMatcher`][edsnlp.matchers.regex.RegexMatcher] (eg `NORM`) ignore_excluded : bool Whether the RegexMatcher ignores excluded tokens. index : int The index in the pre-processed text. Returns ------- int The offset. To get the character index in the original document, just do: `#!python original = index + offset(doc, attr, ignore_excluded, index)` \"\"\" original , clean = alignment ( doc = doc , attr = attr , ignore_excluded = ignore_excluded , ) # We use bisect to efficiently find the correct rightmost-lower index i = bisect_left ( clean , index ) i = min ( i , len ( original ) - 1 ) return original [ i ] - clean [ i ] text get_text ( doclike , attr , ignore_excluded ) Get text using a custom attribute, possibly ignoring excluded tokens. PARAMETER DESCRIPTION doclike Doc or Span to get text from. TYPE: Union[Doc, Span] attr Attribute to use. TYPE: str ignore_excluded Whether to skip excluded tokens, by default False TYPE: bool RETURNS DESCRIPTION str Extracted text. Source code in edsnlp/matchers/utils/text.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @lru_cache ( 32 ) def get_text ( doclike : Union [ Doc , Span ], attr : str , ignore_excluded : bool , ) -> str : \"\"\" Get text using a custom attribute, possibly ignoring excluded tokens. Parameters ---------- doclike : Union[Doc, Span] Doc or Span to get text from. attr : str Attribute to use. ignore_excluded : bool Whether to skip excluded tokens, by default False Returns ------- str Extracted text. \"\"\" attr = attr . upper () if not ignore_excluded : if attr == \"TEXT\" : return doclike . text elif attr == \"LOWER\" : return doclike . text . lower () else : tokens = doclike else : tokens = [ t for t in doclike if not t . _ . excluded ] attr = ATTRIBUTES . get ( attr , attr ) if attr . startswith ( \"_\" ): attr = attr [ 1 :] . lower () return \"\" . join ([ getattr ( t . _ , attr ) + t . whitespace_ for t in tokens ]) else : return \"\" . join ([ getattr ( t , attr ) + t . whitespace_ for t in tokens ])","title":"`edsnlp.matchers.utils`"},{"location":"reference/matchers/utils/#edsnlpmatchersutils","text":"","title":"edsnlp.matchers.utils"},{"location":"reference/matchers/utils/#edsnlp.matchers.utils.offset","text":"","title":"offset"},{"location":"reference/matchers/utils/#edsnlp.matchers.utils.offset.alignment","text":"Align different representations of a Doc or Span object. PARAMETER DESCRIPTION doc spaCy Doc or Span object TYPE: Doc attr Attribute to use, by default \"TEXT\" TYPE: str, optional DEFAULT: 'TEXT' ignore_excluded Whether to remove excluded tokens, by default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION Tuple[List[int], List[int]] An alignment tuple: original and clean lists. Source code in edsnlp/matchers/utils/offset.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @lru_cache ( maxsize = 32 ) def alignment ( doc : Doc , attr : str = \"TEXT\" , ignore_excluded : bool = True , ) -> Tuple [ List [ int ], List [ int ]]: \"\"\" Align different representations of a `Doc` or `Span` object. Parameters ---------- doc : Doc spaCy `Doc` or `Span` object attr : str, optional Attribute to use, by default `\"TEXT\"` ignore_excluded : bool, optional Whether to remove excluded tokens, by default True Returns ------- Tuple[List[int], List[int]] An alignment tuple: original and clean lists. \"\"\" assert isinstance ( doc , Doc ) attr = attr . upper () attr = ATTRIBUTES . get ( attr , attr ) custom = attr . startswith ( \"_\" ) if custom : attr = attr [ 1 :] . lower () # Define the length function length = partial ( token_length , custom = custom , attr = attr ) original = [] clean = [] cursor = 0 for token in doc : if not ignore_excluded or not token . _ . excluded : # The token is not excluded, we add its extremities to the list original . append ( token . idx ) # We add the cursor clean . append ( cursor ) cursor += length ( token ) if token . whitespace_ : cursor += 1 return original , clean","title":"alignment()"},{"location":"reference/matchers/utils/#edsnlp.matchers.utils.offset.offset","text":"Compute offset between the original text and a given representation (defined by the couple attr , ignore_excluded ). The alignment itself is computed with alignment . PARAMETER DESCRIPTION doc The spaCy Doc object TYPE: Doc attr The attribute used by the RegexMatcher (eg NORM ) TYPE: str ignore_excluded Whether the RegexMatcher ignores excluded tokens. TYPE: bool index The index in the pre-processed text. TYPE: int RETURNS DESCRIPTION int The offset. To get the character index in the original document, just do: original = index + offset ( doc , attr , ignore_excluded , index ) Source code in edsnlp/matchers/utils/offset.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def offset ( doc : Doc , attr : str , ignore_excluded : bool , index : int , ) -> int : \"\"\" Compute offset between the original text and a given representation (defined by the couple `attr`, `ignore_excluded`). The alignment itself is computed with [`alignment`][edsnlp.matchers.utils.offset.alignment]. Parameters ---------- doc : Doc The spaCy `Doc` object attr : str The attribute used by the [`RegexMatcher`][edsnlp.matchers.regex.RegexMatcher] (eg `NORM`) ignore_excluded : bool Whether the RegexMatcher ignores excluded tokens. index : int The index in the pre-processed text. Returns ------- int The offset. To get the character index in the original document, just do: `#!python original = index + offset(doc, attr, ignore_excluded, index)` \"\"\" original , clean = alignment ( doc = doc , attr = attr , ignore_excluded = ignore_excluded , ) # We use bisect to efficiently find the correct rightmost-lower index i = bisect_left ( clean , index ) i = min ( i , len ( original ) - 1 ) return original [ i ] - clean [ i ]","title":"offset()"},{"location":"reference/matchers/utils/#edsnlp.matchers.utils.text","text":"","title":"text"},{"location":"reference/matchers/utils/#edsnlp.matchers.utils.text.get_text","text":"Get text using a custom attribute, possibly ignoring excluded tokens. PARAMETER DESCRIPTION doclike Doc or Span to get text from. TYPE: Union[Doc, Span] attr Attribute to use. TYPE: str ignore_excluded Whether to skip excluded tokens, by default False TYPE: bool RETURNS DESCRIPTION str Extracted text. Source code in edsnlp/matchers/utils/text.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @lru_cache ( 32 ) def get_text ( doclike : Union [ Doc , Span ], attr : str , ignore_excluded : bool , ) -> str : \"\"\" Get text using a custom attribute, possibly ignoring excluded tokens. Parameters ---------- doclike : Union[Doc, Span] Doc or Span to get text from. attr : str Attribute to use. ignore_excluded : bool Whether to skip excluded tokens, by default False Returns ------- str Extracted text. \"\"\" attr = attr . upper () if not ignore_excluded : if attr == \"TEXT\" : return doclike . text elif attr == \"LOWER\" : return doclike . text . lower () else : tokens = doclike else : tokens = [ t for t in doclike if not t . _ . excluded ] attr = ATTRIBUTES . get ( attr , attr ) if attr . startswith ( \"_\" ): attr = attr [ 1 :] . lower () return \"\" . join ([ getattr ( t . _ , attr ) + t . whitespace_ for t in tokens ]) else : return \"\" . join ([ getattr ( t , attr ) + t . whitespace_ for t in tokens ])","title":"get_text()"},{"location":"reference/matchers/utils/offset/","text":"edsnlp.matchers.utils.offset alignment ( doc , attr = 'TEXT' , ignore_excluded = True ) Align different representations of a Doc or Span object. PARAMETER DESCRIPTION doc spaCy Doc or Span object TYPE: Doc attr Attribute to use, by default \"TEXT\" TYPE: str, optional DEFAULT: 'TEXT' ignore_excluded Whether to remove excluded tokens, by default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION Tuple[List[int], List[int]] An alignment tuple: original and clean lists. Source code in edsnlp/matchers/utils/offset.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @lru_cache ( maxsize = 32 ) def alignment ( doc : Doc , attr : str = \"TEXT\" , ignore_excluded : bool = True , ) -> Tuple [ List [ int ], List [ int ]]: \"\"\" Align different representations of a `Doc` or `Span` object. Parameters ---------- doc : Doc spaCy `Doc` or `Span` object attr : str, optional Attribute to use, by default `\"TEXT\"` ignore_excluded : bool, optional Whether to remove excluded tokens, by default True Returns ------- Tuple[List[int], List[int]] An alignment tuple: original and clean lists. \"\"\" assert isinstance ( doc , Doc ) attr = attr . upper () attr = ATTRIBUTES . get ( attr , attr ) custom = attr . startswith ( \"_\" ) if custom : attr = attr [ 1 :] . lower () # Define the length function length = partial ( token_length , custom = custom , attr = attr ) original = [] clean = [] cursor = 0 for token in doc : if not ignore_excluded or not token . _ . excluded : # The token is not excluded, we add its extremities to the list original . append ( token . idx ) # We add the cursor clean . append ( cursor ) cursor += length ( token ) if token . whitespace_ : cursor += 1 return original , clean offset ( doc , attr , ignore_excluded , index ) Compute offset between the original text and a given representation (defined by the couple attr , ignore_excluded ). The alignment itself is computed with alignment . PARAMETER DESCRIPTION doc The spaCy Doc object TYPE: Doc attr The attribute used by the RegexMatcher (eg NORM ) TYPE: str ignore_excluded Whether the RegexMatcher ignores excluded tokens. TYPE: bool index The index in the pre-processed text. TYPE: int RETURNS DESCRIPTION int The offset. To get the character index in the original document, just do: original = index + offset ( doc , attr , ignore_excluded , index ) Source code in edsnlp/matchers/utils/offset.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def offset ( doc : Doc , attr : str , ignore_excluded : bool , index : int , ) -> int : \"\"\" Compute offset between the original text and a given representation (defined by the couple `attr`, `ignore_excluded`). The alignment itself is computed with [`alignment`][edsnlp.matchers.utils.offset.alignment]. Parameters ---------- doc : Doc The spaCy `Doc` object attr : str The attribute used by the [`RegexMatcher`][edsnlp.matchers.regex.RegexMatcher] (eg `NORM`) ignore_excluded : bool Whether the RegexMatcher ignores excluded tokens. index : int The index in the pre-processed text. Returns ------- int The offset. To get the character index in the original document, just do: `#!python original = index + offset(doc, attr, ignore_excluded, index)` \"\"\" original , clean = alignment ( doc = doc , attr = attr , ignore_excluded = ignore_excluded , ) # We use bisect to efficiently find the correct rightmost-lower index i = bisect_left ( clean , index ) i = min ( i , len ( original ) - 1 ) return original [ i ] - clean [ i ]","title":"offset"},{"location":"reference/matchers/utils/offset/#edsnlpmatchersutilsoffset","text":"","title":"edsnlp.matchers.utils.offset"},{"location":"reference/matchers/utils/offset/#edsnlp.matchers.utils.offset.alignment","text":"Align different representations of a Doc or Span object. PARAMETER DESCRIPTION doc spaCy Doc or Span object TYPE: Doc attr Attribute to use, by default \"TEXT\" TYPE: str, optional DEFAULT: 'TEXT' ignore_excluded Whether to remove excluded tokens, by default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION Tuple[List[int], List[int]] An alignment tuple: original and clean lists. Source code in edsnlp/matchers/utils/offset.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @lru_cache ( maxsize = 32 ) def alignment ( doc : Doc , attr : str = \"TEXT\" , ignore_excluded : bool = True , ) -> Tuple [ List [ int ], List [ int ]]: \"\"\" Align different representations of a `Doc` or `Span` object. Parameters ---------- doc : Doc spaCy `Doc` or `Span` object attr : str, optional Attribute to use, by default `\"TEXT\"` ignore_excluded : bool, optional Whether to remove excluded tokens, by default True Returns ------- Tuple[List[int], List[int]] An alignment tuple: original and clean lists. \"\"\" assert isinstance ( doc , Doc ) attr = attr . upper () attr = ATTRIBUTES . get ( attr , attr ) custom = attr . startswith ( \"_\" ) if custom : attr = attr [ 1 :] . lower () # Define the length function length = partial ( token_length , custom = custom , attr = attr ) original = [] clean = [] cursor = 0 for token in doc : if not ignore_excluded or not token . _ . excluded : # The token is not excluded, we add its extremities to the list original . append ( token . idx ) # We add the cursor clean . append ( cursor ) cursor += length ( token ) if token . whitespace_ : cursor += 1 return original , clean","title":"alignment()"},{"location":"reference/matchers/utils/offset/#edsnlp.matchers.utils.offset.offset","text":"Compute offset between the original text and a given representation (defined by the couple attr , ignore_excluded ). The alignment itself is computed with alignment . PARAMETER DESCRIPTION doc The spaCy Doc object TYPE: Doc attr The attribute used by the RegexMatcher (eg NORM ) TYPE: str ignore_excluded Whether the RegexMatcher ignores excluded tokens. TYPE: bool index The index in the pre-processed text. TYPE: int RETURNS DESCRIPTION int The offset. To get the character index in the original document, just do: original = index + offset ( doc , attr , ignore_excluded , index ) Source code in edsnlp/matchers/utils/offset.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def offset ( doc : Doc , attr : str , ignore_excluded : bool , index : int , ) -> int : \"\"\" Compute offset between the original text and a given representation (defined by the couple `attr`, `ignore_excluded`). The alignment itself is computed with [`alignment`][edsnlp.matchers.utils.offset.alignment]. Parameters ---------- doc : Doc The spaCy `Doc` object attr : str The attribute used by the [`RegexMatcher`][edsnlp.matchers.regex.RegexMatcher] (eg `NORM`) ignore_excluded : bool Whether the RegexMatcher ignores excluded tokens. index : int The index in the pre-processed text. Returns ------- int The offset. To get the character index in the original document, just do: `#!python original = index + offset(doc, attr, ignore_excluded, index)` \"\"\" original , clean = alignment ( doc = doc , attr = attr , ignore_excluded = ignore_excluded , ) # We use bisect to efficiently find the correct rightmost-lower index i = bisect_left ( clean , index ) i = min ( i , len ( original ) - 1 ) return original [ i ] - clean [ i ]","title":"offset()"},{"location":"reference/matchers/utils/text/","text":"edsnlp.matchers.utils.text get_text ( doclike , attr , ignore_excluded ) Get text using a custom attribute, possibly ignoring excluded tokens. PARAMETER DESCRIPTION doclike Doc or Span to get text from. TYPE: Union[Doc, Span] attr Attribute to use. TYPE: str ignore_excluded Whether to skip excluded tokens, by default False TYPE: bool RETURNS DESCRIPTION str Extracted text. Source code in edsnlp/matchers/utils/text.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @lru_cache ( 32 ) def get_text ( doclike : Union [ Doc , Span ], attr : str , ignore_excluded : bool , ) -> str : \"\"\" Get text using a custom attribute, possibly ignoring excluded tokens. Parameters ---------- doclike : Union[Doc, Span] Doc or Span to get text from. attr : str Attribute to use. ignore_excluded : bool Whether to skip excluded tokens, by default False Returns ------- str Extracted text. \"\"\" attr = attr . upper () if not ignore_excluded : if attr == \"TEXT\" : return doclike . text elif attr == \"LOWER\" : return doclike . text . lower () else : tokens = doclike else : tokens = [ t for t in doclike if not t . _ . excluded ] attr = ATTRIBUTES . get ( attr , attr ) if attr . startswith ( \"_\" ): attr = attr [ 1 :] . lower () return \"\" . join ([ getattr ( t . _ , attr ) + t . whitespace_ for t in tokens ]) else : return \"\" . join ([ getattr ( t , attr ) + t . whitespace_ for t in tokens ])","title":"text"},{"location":"reference/matchers/utils/text/#edsnlpmatchersutilstext","text":"","title":"edsnlp.matchers.utils.text"},{"location":"reference/matchers/utils/text/#edsnlp.matchers.utils.text.get_text","text":"Get text using a custom attribute, possibly ignoring excluded tokens. PARAMETER DESCRIPTION doclike Doc or Span to get text from. TYPE: Union[Doc, Span] attr Attribute to use. TYPE: str ignore_excluded Whether to skip excluded tokens, by default False TYPE: bool RETURNS DESCRIPTION str Extracted text. Source code in edsnlp/matchers/utils/text.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @lru_cache ( 32 ) def get_text ( doclike : Union [ Doc , Span ], attr : str , ignore_excluded : bool , ) -> str : \"\"\" Get text using a custom attribute, possibly ignoring excluded tokens. Parameters ---------- doclike : Union[Doc, Span] Doc or Span to get text from. attr : str Attribute to use. ignore_excluded : bool Whether to skip excluded tokens, by default False Returns ------- str Extracted text. \"\"\" attr = attr . upper () if not ignore_excluded : if attr == \"TEXT\" : return doclike . text elif attr == \"LOWER\" : return doclike . text . lower () else : tokens = doclike else : tokens = [ t for t in doclike if not t . _ . excluded ] attr = ATTRIBUTES . get ( attr , attr ) if attr . startswith ( \"_\" ): attr = attr [ 1 :] . lower () return \"\" . join ([ getattr ( t . _ , attr ) + t . whitespace_ for t in tokens ]) else : return \"\" . join ([ getattr ( t , attr ) + t . whitespace_ for t in tokens ])","title":"get_text()"},{"location":"reference/pipelines/","text":"edsnlp.pipelines","title":"`edsnlp.pipelines`"},{"location":"reference/pipelines/#edsnlppipelines","text":"","title":"edsnlp.pipelines"},{"location":"reference/pipelines/base/","text":"edsnlp.pipelines.base BaseComponent Bases: object The BaseComponent adds a set_extensions method, called at the creation of the object. It helps decouple the initialisation of the pipeline from the creation of extensions, and is particularly usefull when distributing EDSNLP on a cluster, since the serialisation mechanism imposes that the extensions be reset. Source code in edsnlp/pipelines/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseComponent ( object ): \"\"\" The `BaseComponent` adds a `set_extensions` method, called at the creation of the object. It helps decouple the initialisation of the pipeline from the creation of extensions, and is particularly usefull when distributing EDSNLP on a cluster, since the serialisation mechanism imposes that the extensions be reset. \"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_extensions () @staticmethod def set_extensions () -> None : \"\"\" Set `Doc`, `Span` and `Token` extensions. \"\"\" pass def _boundaries ( self , doc : Doc , terminations : Optional [ List [ Span ]] = None ) -> List [ Tuple [ int , int ]]: \"\"\" Create sub sentences based sentences and terminations found in text. Parameters ---------- doc: spaCy Doc object terminations: List of tuples with (match_id, start, end) Returns ------- boundaries: List of tuples with (start, end) of spans \"\"\" if terminations is None : terminations = [] sent_starts = [ sent . start for sent in doc . sents ] termination_starts = [ t . start for t in terminations ] starts = sent_starts + termination_starts + [ len ( doc )] # Remove duplicates starts = list ( set ( starts )) # Sort starts starts . sort () boundaries = [( start , end ) for start , end in zip ( starts [: - 1 ], starts [ 1 :])] return boundaries set_extensions () Set Doc , Span and Token extensions. Source code in edsnlp/pipelines/base.py 22 23 24 25 26 27 @staticmethod def set_extensions () -> None : \"\"\" Set `Doc`, `Span` and `Token` extensions. \"\"\" pass _boundaries ( doc , terminations = None ) Create sub sentences based sentences and terminations found in text. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc terminations List of tuples with (match_id, start, end) TYPE: Optional [ List [ Span ]] DEFAULT: None RETURNS DESCRIPTION boundaries List of tuples with (start, end) of spans Source code in edsnlp/pipelines/base.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def _boundaries ( self , doc : Doc , terminations : Optional [ List [ Span ]] = None ) -> List [ Tuple [ int , int ]]: \"\"\" Create sub sentences based sentences and terminations found in text. Parameters ---------- doc: spaCy Doc object terminations: List of tuples with (match_id, start, end) Returns ------- boundaries: List of tuples with (start, end) of spans \"\"\" if terminations is None : terminations = [] sent_starts = [ sent . start for sent in doc . sents ] termination_starts = [ t . start for t in terminations ] starts = sent_starts + termination_starts + [ len ( doc )] # Remove duplicates starts = list ( set ( starts )) # Sort starts starts . sort () boundaries = [( start , end ) for start , end in zip ( starts [: - 1 ], starts [ 1 :])] return boundaries","title":"base"},{"location":"reference/pipelines/base/#edsnlppipelinesbase","text":"","title":"edsnlp.pipelines.base"},{"location":"reference/pipelines/base/#edsnlp.pipelines.base.BaseComponent","text":"Bases: object The BaseComponent adds a set_extensions method, called at the creation of the object. It helps decouple the initialisation of the pipeline from the creation of extensions, and is particularly usefull when distributing EDSNLP on a cluster, since the serialisation mechanism imposes that the extensions be reset. Source code in edsnlp/pipelines/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseComponent ( object ): \"\"\" The `BaseComponent` adds a `set_extensions` method, called at the creation of the object. It helps decouple the initialisation of the pipeline from the creation of extensions, and is particularly usefull when distributing EDSNLP on a cluster, since the serialisation mechanism imposes that the extensions be reset. \"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_extensions () @staticmethod def set_extensions () -> None : \"\"\" Set `Doc`, `Span` and `Token` extensions. \"\"\" pass def _boundaries ( self , doc : Doc , terminations : Optional [ List [ Span ]] = None ) -> List [ Tuple [ int , int ]]: \"\"\" Create sub sentences based sentences and terminations found in text. Parameters ---------- doc: spaCy Doc object terminations: List of tuples with (match_id, start, end) Returns ------- boundaries: List of tuples with (start, end) of spans \"\"\" if terminations is None : terminations = [] sent_starts = [ sent . start for sent in doc . sents ] termination_starts = [ t . start for t in terminations ] starts = sent_starts + termination_starts + [ len ( doc )] # Remove duplicates starts = list ( set ( starts )) # Sort starts starts . sort () boundaries = [( start , end ) for start , end in zip ( starts [: - 1 ], starts [ 1 :])] return boundaries","title":"BaseComponent"},{"location":"reference/pipelines/base/#edsnlp.pipelines.base.BaseComponent.set_extensions","text":"Set Doc , Span and Token extensions. Source code in edsnlp/pipelines/base.py 22 23 24 25 26 27 @staticmethod def set_extensions () -> None : \"\"\" Set `Doc`, `Span` and `Token` extensions. \"\"\" pass","title":"set_extensions()"},{"location":"reference/pipelines/base/#edsnlp.pipelines.base.BaseComponent._boundaries","text":"Create sub sentences based sentences and terminations found in text. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc terminations List of tuples with (match_id, start, end) TYPE: Optional [ List [ Span ]] DEFAULT: None RETURNS DESCRIPTION boundaries List of tuples with (start, end) of spans Source code in edsnlp/pipelines/base.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def _boundaries ( self , doc : Doc , terminations : Optional [ List [ Span ]] = None ) -> List [ Tuple [ int , int ]]: \"\"\" Create sub sentences based sentences and terminations found in text. Parameters ---------- doc: spaCy Doc object terminations: List of tuples with (match_id, start, end) Returns ------- boundaries: List of tuples with (start, end) of spans \"\"\" if terminations is None : terminations = [] sent_starts = [ sent . start for sent in doc . sents ] termination_starts = [ t . start for t in terminations ] starts = sent_starts + termination_starts + [ len ( doc )] # Remove duplicates starts = list ( set ( starts )) # Sort starts starts . sort () boundaries = [( start , end ) for start , end in zip ( starts [: - 1 ], starts [ 1 :])] return boundaries","title":"_boundaries()"},{"location":"reference/pipelines/factories/","text":"edsnlp.pipelines.factories","title":"factories"},{"location":"reference/pipelines/factories/#edsnlppipelinesfactories","text":"","title":"edsnlp.pipelines.factories"},{"location":"reference/pipelines/terminations/","text":"edsnlp.pipelines.terminations","title":"terminations"},{"location":"reference/pipelines/terminations/#edsnlppipelinesterminations","text":"","title":"edsnlp.pipelines.terminations"},{"location":"reference/pipelines/core/","text":"edsnlp.pipelines.core","title":"`edsnlp.pipelines.core`"},{"location":"reference/pipelines/core/#edsnlppipelinescore","text":"","title":"edsnlp.pipelines.core"},{"location":"reference/pipelines/core/advanced/","text":"edsnlp.pipelines.core.advanced","title":"`edsnlp.pipelines.core.advanced`"},{"location":"reference/pipelines/core/advanced/#edsnlppipelinescoreadvanced","text":"","title":"edsnlp.pipelines.core.advanced"},{"location":"reference/pipelines/core/advanced/advanced/","text":"edsnlp.pipelines.core.advanced.advanced AdvancedRegex Bases: GenericMatcher Allows additional matching in the surrounding context of the main match group, for qualification/filtering. PARAMETER DESCRIPTION nlp spaCy Language object. TYPE: Language regex_config Configuration for the main expression. TYPE: Dict[str, Any] window Number of tokens to consider before and after the main expression. TYPE: int attr Attribute to match on, eg TEXT , NORM , etc. TYPE: str verbose Verbosity level, useful for debugging. TYPE: int ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/core/advanced/advanced.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class AdvancedRegex ( GenericMatcher ): \"\"\" Allows additional matching in the surrounding context of the main match group, for qualification/filtering. Parameters ---------- nlp : Language spaCy `Language` object. regex_config : Dict[str, Any] Configuration for the main expression. window : int Number of tokens to consider before and after the main expression. attr : str Attribute to match on, eg `TEXT`, `NORM`, etc. verbose : int Verbosity level, useful for debugging. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , regex_config : Dict [ str , Any ], window : int , attr : str , verbose : int , ignore_excluded : bool , ): self . regex_config = _check_regex_config ( regex_config ) self . window = window regex = regex_config self . verbose = verbose super () . __init__ ( nlp = nlp , terms = dict (), regex = regex , attr = attr , ignore_excluded = ignore_excluded , ) self . ignore_excluded = ignore_excluded self . set_extensions () @staticmethod def set_extensions () -> None : if not Doc . has_extension ( \"my_ents\" ): Doc . set_extension ( \"my_ents\" , default = []) if not Span . has_extension ( \"matcher_name\" ): Span . set_extension ( \"matcher_name\" , default = None ) if not Span . has_extension ( \"before_extract\" ): Span . set_extension ( \"before_extract\" , default = None ) if not Span . has_extension ( \"after_extract\" ): Span . set_extension ( \"after_extract\" , default = None ) if not Span . has_extension ( \"window\" ): Span . set_extension ( \"window\" , default = None ) if not Span . has_extension ( \"before_snippet\" ): Span . set_extension ( \"before_snippet\" , default = None ) if not Span . has_extension ( \"after_snippet\" ): Span . set_extension ( \"after_snippet\" , default = None ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Process the document, looking for named entities. Parameters ---------- doc : Doc spaCy Doc object Returns ------- List[Span] List of detected spans. \"\"\" ents = super () . process ( doc ) ents = self . _postprocessing_pipeline ( ents ) return ents def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = self . process ( doc ) ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc def _postprocessing_pipeline ( self , ents : List [ Span ]): # add a window within the sentence around entities ents = [ self . _add_window ( ent ) for ent in ents ] # Remove entities based on the snippet located just before and after the entity ents = filter ( self . _exclude_filter , ents ) # Extract informations from the entity's context via regex ents = [ self . _snippet_extraction ( ent ) for ent in ents ] return ents def _add_window ( self , ent : Span ) -> Span : ent . _ . window = ent . doc [ max ( ent . start - self . window , ent . sent . start ) : min ( ent . end + self . window , ent . sent . end ) ] # include the entity in the snippets so that we can extract # the number when it is attached to the word, e.g. \"3PA\" ent . _ . before_snippet = ent . doc [ max ( ent . start - self . window , ent . sent . start ) : ent . end ] ent . _ . after_snippet = ent . doc [ ent . start : min ( ent . end + self . window , ent . sent . end ) ] return ent def get_text ( self , span : Span , label ) -> str : attr = self . regex_config [ label ] . get ( \"attr\" , None ) return get_text ( doclike = span , attr = attr , ignore_excluded = self . ignore_excluded , ) def _exclude_filter ( self , ent : Span ) -> Span : label = ent . label_ before_exclude = self . regex_config [ label ] . get ( \"before_exclude\" , None ) after_exclude = self . regex_config [ label ] . get ( \"after_exclude\" , None ) if before_exclude is not None : t = ent . _ . before_snippet t = self . get_text ( t , label ) if re . compile ( before_exclude ) . search ( t ) is not None : if self . verbose : logger . info ( f \"excluded (before) string: { t } - pattern { before_exclude } \" ) return False if after_exclude is not None : t = ent . _ . after_snippet t = self . get_text ( t , label ) if re . compile ( after_exclude ) . search ( t ) is not None : if self . verbose : logger . info ( f \"excluded (after) string: { t } - pattern { after_exclude } \" ) return False return True def _snippet_extraction ( self , ent : Span ) -> Span : label = ent . label_ before_extract = self . regex_config [ label ] . get ( \"before_extract\" , []) after_extract = self . regex_config [ label ] . get ( \"after_extract\" , []) if type ( before_extract ) == str : before_extract = [ before_extract ] if type ( after_extract ) == str : after_extract = [ after_extract ] t = ent . _ . before_snippet t = self . get_text ( t , label ) ent . _ . before_extract = [] for pattern in before_extract : pattern = re . compile ( pattern ) match = pattern . search ( t ) ent . _ . before_extract . append ( match . groups ()[ 0 ] if match else None ) t = ent . _ . after_snippet t = self . get_text ( t , label ) ent . _ . after_extract = [] for pattern in after_extract : pattern = re . compile ( pattern ) match = pattern . search ( t ) ent . _ . after_extract . append ( match . groups ()[ 0 ] if match else None ) return ent process ( doc ) Process the document, looking for named entities. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION List[Span] List of detected spans. Source code in edsnlp/pipelines/core/advanced/advanced.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Process the document, looking for named entities. Parameters ---------- doc : Doc spaCy Doc object Returns ------- List[Span] List of detected spans. \"\"\" ents = super () . process ( doc ) ents = self . _postprocessing_pipeline ( ents ) return ents __call__ ( doc ) Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/advanced/advanced.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = self . process ( doc ) ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"advanced"},{"location":"reference/pipelines/core/advanced/advanced/#edsnlppipelinescoreadvancedadvanced","text":"","title":"edsnlp.pipelines.core.advanced.advanced"},{"location":"reference/pipelines/core/advanced/advanced/#edsnlp.pipelines.core.advanced.advanced.AdvancedRegex","text":"Bases: GenericMatcher Allows additional matching in the surrounding context of the main match group, for qualification/filtering. PARAMETER DESCRIPTION nlp spaCy Language object. TYPE: Language regex_config Configuration for the main expression. TYPE: Dict[str, Any] window Number of tokens to consider before and after the main expression. TYPE: int attr Attribute to match on, eg TEXT , NORM , etc. TYPE: str verbose Verbosity level, useful for debugging. TYPE: int ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/core/advanced/advanced.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class AdvancedRegex ( GenericMatcher ): \"\"\" Allows additional matching in the surrounding context of the main match group, for qualification/filtering. Parameters ---------- nlp : Language spaCy `Language` object. regex_config : Dict[str, Any] Configuration for the main expression. window : int Number of tokens to consider before and after the main expression. attr : str Attribute to match on, eg `TEXT`, `NORM`, etc. verbose : int Verbosity level, useful for debugging. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , regex_config : Dict [ str , Any ], window : int , attr : str , verbose : int , ignore_excluded : bool , ): self . regex_config = _check_regex_config ( regex_config ) self . window = window regex = regex_config self . verbose = verbose super () . __init__ ( nlp = nlp , terms = dict (), regex = regex , attr = attr , ignore_excluded = ignore_excluded , ) self . ignore_excluded = ignore_excluded self . set_extensions () @staticmethod def set_extensions () -> None : if not Doc . has_extension ( \"my_ents\" ): Doc . set_extension ( \"my_ents\" , default = []) if not Span . has_extension ( \"matcher_name\" ): Span . set_extension ( \"matcher_name\" , default = None ) if not Span . has_extension ( \"before_extract\" ): Span . set_extension ( \"before_extract\" , default = None ) if not Span . has_extension ( \"after_extract\" ): Span . set_extension ( \"after_extract\" , default = None ) if not Span . has_extension ( \"window\" ): Span . set_extension ( \"window\" , default = None ) if not Span . has_extension ( \"before_snippet\" ): Span . set_extension ( \"before_snippet\" , default = None ) if not Span . has_extension ( \"after_snippet\" ): Span . set_extension ( \"after_snippet\" , default = None ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Process the document, looking for named entities. Parameters ---------- doc : Doc spaCy Doc object Returns ------- List[Span] List of detected spans. \"\"\" ents = super () . process ( doc ) ents = self . _postprocessing_pipeline ( ents ) return ents def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = self . process ( doc ) ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc def _postprocessing_pipeline ( self , ents : List [ Span ]): # add a window within the sentence around entities ents = [ self . _add_window ( ent ) for ent in ents ] # Remove entities based on the snippet located just before and after the entity ents = filter ( self . _exclude_filter , ents ) # Extract informations from the entity's context via regex ents = [ self . _snippet_extraction ( ent ) for ent in ents ] return ents def _add_window ( self , ent : Span ) -> Span : ent . _ . window = ent . doc [ max ( ent . start - self . window , ent . sent . start ) : min ( ent . end + self . window , ent . sent . end ) ] # include the entity in the snippets so that we can extract # the number when it is attached to the word, e.g. \"3PA\" ent . _ . before_snippet = ent . doc [ max ( ent . start - self . window , ent . sent . start ) : ent . end ] ent . _ . after_snippet = ent . doc [ ent . start : min ( ent . end + self . window , ent . sent . end ) ] return ent def get_text ( self , span : Span , label ) -> str : attr = self . regex_config [ label ] . get ( \"attr\" , None ) return get_text ( doclike = span , attr = attr , ignore_excluded = self . ignore_excluded , ) def _exclude_filter ( self , ent : Span ) -> Span : label = ent . label_ before_exclude = self . regex_config [ label ] . get ( \"before_exclude\" , None ) after_exclude = self . regex_config [ label ] . get ( \"after_exclude\" , None ) if before_exclude is not None : t = ent . _ . before_snippet t = self . get_text ( t , label ) if re . compile ( before_exclude ) . search ( t ) is not None : if self . verbose : logger . info ( f \"excluded (before) string: { t } - pattern { before_exclude } \" ) return False if after_exclude is not None : t = ent . _ . after_snippet t = self . get_text ( t , label ) if re . compile ( after_exclude ) . search ( t ) is not None : if self . verbose : logger . info ( f \"excluded (after) string: { t } - pattern { after_exclude } \" ) return False return True def _snippet_extraction ( self , ent : Span ) -> Span : label = ent . label_ before_extract = self . regex_config [ label ] . get ( \"before_extract\" , []) after_extract = self . regex_config [ label ] . get ( \"after_extract\" , []) if type ( before_extract ) == str : before_extract = [ before_extract ] if type ( after_extract ) == str : after_extract = [ after_extract ] t = ent . _ . before_snippet t = self . get_text ( t , label ) ent . _ . before_extract = [] for pattern in before_extract : pattern = re . compile ( pattern ) match = pattern . search ( t ) ent . _ . before_extract . append ( match . groups ()[ 0 ] if match else None ) t = ent . _ . after_snippet t = self . get_text ( t , label ) ent . _ . after_extract = [] for pattern in after_extract : pattern = re . compile ( pattern ) match = pattern . search ( t ) ent . _ . after_extract . append ( match . groups ()[ 0 ] if match else None ) return ent","title":"AdvancedRegex"},{"location":"reference/pipelines/core/advanced/advanced/#edsnlp.pipelines.core.advanced.advanced.AdvancedRegex.process","text":"Process the document, looking for named entities. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION List[Span] List of detected spans. Source code in edsnlp/pipelines/core/advanced/advanced.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Process the document, looking for named entities. Parameters ---------- doc : Doc spaCy Doc object Returns ------- List[Span] List of detected spans. \"\"\" ents = super () . process ( doc ) ents = self . _postprocessing_pipeline ( ents ) return ents","title":"process()"},{"location":"reference/pipelines/core/advanced/advanced/#edsnlp.pipelines.core.advanced.advanced.AdvancedRegex.__call__","text":"Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/advanced/advanced.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = self . process ( doc ) ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/advanced/factory/","text":"edsnlp.pipelines.core.advanced.factory","title":"factory"},{"location":"reference/pipelines/core/advanced/factory/#edsnlppipelinescoreadvancedfactory","text":"","title":"edsnlp.pipelines.core.advanced.factory"},{"location":"reference/pipelines/core/endlines/","text":"edsnlp.pipelines.core.endlines","title":"`edsnlp.pipelines.core.endlines`"},{"location":"reference/pipelines/core/endlines/#edsnlppipelinescoreendlines","text":"","title":"edsnlp.pipelines.core.endlines"},{"location":"reference/pipelines/core/endlines/endlines/","text":"edsnlp.pipelines.core.endlines.endlines EndLines Bases: GenericMatcher spaCy Pipeline to detect whether a newline character should be considered a space (ie introduced by the PDF). The pipeline will add the extension end_line to spans and tokens. The end_line attribute is a boolean or None , set to True if the pipeline predicts that the new line is an end line character. Otherwise, it is set to False if the new line is classified as a space. If no classification has been done over that token, it will remain None . PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language end_lines_model : Optional[Union[str, EndLinesModel]], by default None path to trained model. If None, it will use a default model Source code in edsnlp/pipelines/core/endlines/endlines.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class EndLines ( GenericMatcher ): \"\"\" spaCy Pipeline to detect whether a newline character should be considered a space (ie introduced by the PDF). The pipeline will add the extension `end_line` to spans and tokens. The `end_line` attribute is a boolean or `None`, set to `True` if the pipeline predicts that the new line is an end line character. Otherwise, it is set to `False` if the new line is classified as a space. If no classification has been done over that token, it will remain `None`. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. end_lines_model : Optional[Union[str, EndLinesModel]], by default None path to trained model. If None, it will use a default model \"\"\" def __init__ ( self , nlp : Language , end_lines_model : Optional [ Union [ str , EndLinesModel ]], ** kwargs , ): super () . __init__ ( nlp , terms = None , attr = \"TEXT\" , regex = dict ( new_line = r \"\\n+\" , ), ignore_excluded = False , ** kwargs , ) if not Token . has_extension ( \"end_line\" ): Token . set_extension ( \"end_line\" , default = None ) if not Span . has_extension ( \"end_line\" ): Span . set_extension ( \"end_line\" , default = None ) self . _read_model ( end_lines_model ) def _read_model ( self , end_lines_model : Optional [ Union [ str , EndLinesModel ]]): \"\"\" Parameters ---------- end_lines_model : Optional[Union[str, EndLinesModel]] Raises ------ TypeError \"\"\" if end_lines_model is None : path = build_path ( __file__ , \"base_model.pkl\" ) with open ( path , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == str : with open ( end_lines_model , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == EndLinesModel : self . model = end_lines_model else : raise TypeError ( \"type(`end_lines_model`) should be one of {None, str, EndLinesModel}\" ) @staticmethod def _spacy_compute_a3a4 ( token : Token ) -> str : \"\"\"Function to compute A3 and A4 Parameters ---------- token : Token Returns ------- str \"\"\" if token . is_upper : return \"UPPER\" elif token . shape_ . startswith ( \"Xx\" ): return \"S_UPPER\" elif token . shape_ . startswith ( \"x\" ): return \"LOWER\" elif ( token . is_digit ) & ( ( token . doc [ max ( token . i - 1 , 0 )] . is_punct ) | ( token . doc [ min ( token . i + 1 , len ( token . doc ) - 1 )] . is_punct ) ): return \"ENUMERATION\" elif token . is_digit : return \"DIGIT\" elif ( token . is_punct ) & ( token . text in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"STRONG_PUNCT\" elif ( token . is_punct ) & ( token . text not in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"SOFT_PUNCT\" else : return \"OTHER\" @staticmethod def _compute_length ( doc : Doc , start : int , end : int ) -> int : \"\"\"Compute length without spaces Parameters ---------- doc : Doc start : int end : int Returns ------- int \"\"\" length = 0 for t in doc [ start : end ]: length += len ( t . text ) return length def _get_df ( self , doc : Doc , new_lines : List [ Span ]) -> pd . DataFrame : \"\"\"Get a pandas DataFrame to call the classifier Parameters ---------- doc : Doc new_lines : List[Span] Returns ------- pd.DataFrame \"\"\" data = [] for i , span in enumerate ( new_lines ): start = span . start end = span . end max_index = len ( doc ) - 1 a1_token = doc [ max ( start - 1 , 0 )] a2_token = doc [ min ( start + 1 , max_index )] a1 = a1_token . orth a2 = a2_token . orth a3 = self . _spacy_compute_a3a4 ( a1_token ) a4 = self . _spacy_compute_a3a4 ( a2_token ) blank_line = \" \\n\\n \" in span . text if i > 0 : start_previous = new_lines [ i - 1 ] . start + 1 else : start_previous = 0 length = self . _compute_length ( doc , start = start_previous , end = start ) # It's ok cause i count the total length from the previous up to this one data_dict = dict ( span_start = start , span_end = end , A1 = a1 , A2 = a2 , A3 = a3 , A4 = a4 , BLANK_LINE = blank_line , length = length , ) data . append ( data_dict ) df = pd . DataFrame ( data ) mu = df [ \"length\" ] . mean () sigma = df [ \"length\" ] . std () if np . isnan ( sigma ): sigma = 1 cv = sigma / mu df [ \"B1\" ] = ( df [ \"length\" ] - mu ) / sigma df [ \"B2\" ] = cv return df def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Predict for each new line if it's an end of line or a space. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, with each new line annotated \"\"\" matches = self . process ( doc ) new_lines = get_spans ( matches , \"new_line\" ) if len ( new_lines ) > 0 : df = self . _get_df ( doc = doc , new_lines = new_lines ) df = self . model . predict ( df ) spans = [] for span , prediction in zip ( new_lines , df . PREDICTED_END_LINE ): span . label_ = _get_label ( prediction ) span . _ . end_line = prediction spans . append ( span ) for t in span : t . _ . end_line = prediction if not prediction : t . _ . excluded = True doc . spans [ \"new_lines\" ] = spans return doc _read_model ( end_lines_model ) PARAMETER DESCRIPTION end_lines_model TYPE: Optional[Union[str, EndLinesModel]] RAISES DESCRIPTION TypeError Source code in edsnlp/pipelines/core/endlines/endlines.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def _read_model ( self , end_lines_model : Optional [ Union [ str , EndLinesModel ]]): \"\"\" Parameters ---------- end_lines_model : Optional[Union[str, EndLinesModel]] Raises ------ TypeError \"\"\" if end_lines_model is None : path = build_path ( __file__ , \"base_model.pkl\" ) with open ( path , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == str : with open ( end_lines_model , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == EndLinesModel : self . model = end_lines_model else : raise TypeError ( \"type(`end_lines_model`) should be one of {None, str, EndLinesModel}\" ) _spacy_compute_a3a4 ( token ) Function to compute A3 and A4 PARAMETER DESCRIPTION token TYPE: Token RETURNS DESCRIPTION str Source code in edsnlp/pipelines/core/endlines/endlines.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @staticmethod def _spacy_compute_a3a4 ( token : Token ) -> str : \"\"\"Function to compute A3 and A4 Parameters ---------- token : Token Returns ------- str \"\"\" if token . is_upper : return \"UPPER\" elif token . shape_ . startswith ( \"Xx\" ): return \"S_UPPER\" elif token . shape_ . startswith ( \"x\" ): return \"LOWER\" elif ( token . is_digit ) & ( ( token . doc [ max ( token . i - 1 , 0 )] . is_punct ) | ( token . doc [ min ( token . i + 1 , len ( token . doc ) - 1 )] . is_punct ) ): return \"ENUMERATION\" elif token . is_digit : return \"DIGIT\" elif ( token . is_punct ) & ( token . text in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"STRONG_PUNCT\" elif ( token . is_punct ) & ( token . text not in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"SOFT_PUNCT\" else : return \"OTHER\" _compute_length ( doc , start , end ) Compute length without spaces PARAMETER DESCRIPTION doc TYPE: Doc start TYPE: int end TYPE: int RETURNS DESCRIPTION int Source code in edsnlp/pipelines/core/endlines/endlines.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 @staticmethod def _compute_length ( doc : Doc , start : int , end : int ) -> int : \"\"\"Compute length without spaces Parameters ---------- doc : Doc start : int end : int Returns ------- int \"\"\" length = 0 for t in doc [ start : end ]: length += len ( t . text ) return length _get_df ( doc , new_lines ) Get a pandas DataFrame to call the classifier PARAMETER DESCRIPTION doc TYPE: Doc new_lines TYPE: List[Span] RETURNS DESCRIPTION pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlines.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def _get_df ( self , doc : Doc , new_lines : List [ Span ]) -> pd . DataFrame : \"\"\"Get a pandas DataFrame to call the classifier Parameters ---------- doc : Doc new_lines : List[Span] Returns ------- pd.DataFrame \"\"\" data = [] for i , span in enumerate ( new_lines ): start = span . start end = span . end max_index = len ( doc ) - 1 a1_token = doc [ max ( start - 1 , 0 )] a2_token = doc [ min ( start + 1 , max_index )] a1 = a1_token . orth a2 = a2_token . orth a3 = self . _spacy_compute_a3a4 ( a1_token ) a4 = self . _spacy_compute_a3a4 ( a2_token ) blank_line = \" \\n\\n \" in span . text if i > 0 : start_previous = new_lines [ i - 1 ] . start + 1 else : start_previous = 0 length = self . _compute_length ( doc , start = start_previous , end = start ) # It's ok cause i count the total length from the previous up to this one data_dict = dict ( span_start = start , span_end = end , A1 = a1 , A2 = a2 , A3 = a3 , A4 = a4 , BLANK_LINE = blank_line , length = length , ) data . append ( data_dict ) df = pd . DataFrame ( data ) mu = df [ \"length\" ] . mean () sigma = df [ \"length\" ] . std () if np . isnan ( sigma ): sigma = 1 cv = sigma / mu df [ \"B1\" ] = ( df [ \"length\" ] - mu ) / sigma df [ \"B2\" ] = cv return df __call__ ( doc ) Predict for each new line if it's an end of line or a space. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/core/endlines/endlines.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Predict for each new line if it's an end of line or a space. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, with each new line annotated \"\"\" matches = self . process ( doc ) new_lines = get_spans ( matches , \"new_line\" ) if len ( new_lines ) > 0 : df = self . _get_df ( doc = doc , new_lines = new_lines ) df = self . model . predict ( df ) spans = [] for span , prediction in zip ( new_lines , df . PREDICTED_END_LINE ): span . label_ = _get_label ( prediction ) span . _ . end_line = prediction spans . append ( span ) for t in span : t . _ . end_line = prediction if not prediction : t . _ . excluded = True doc . spans [ \"new_lines\" ] = spans return doc","title":"endlines"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlppipelinescoreendlinesendlines","text":"","title":"edsnlp.pipelines.core.endlines.endlines"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlp.pipelines.core.endlines.endlines.EndLines","text":"Bases: GenericMatcher spaCy Pipeline to detect whether a newline character should be considered a space (ie introduced by the PDF). The pipeline will add the extension end_line to spans and tokens. The end_line attribute is a boolean or None , set to True if the pipeline predicts that the new line is an end line character. Otherwise, it is set to False if the new line is classified as a space. If no classification has been done over that token, it will remain None . PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language end_lines_model : Optional[Union[str, EndLinesModel]], by default None path to trained model. If None, it will use a default model Source code in edsnlp/pipelines/core/endlines/endlines.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class EndLines ( GenericMatcher ): \"\"\" spaCy Pipeline to detect whether a newline character should be considered a space (ie introduced by the PDF). The pipeline will add the extension `end_line` to spans and tokens. The `end_line` attribute is a boolean or `None`, set to `True` if the pipeline predicts that the new line is an end line character. Otherwise, it is set to `False` if the new line is classified as a space. If no classification has been done over that token, it will remain `None`. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. end_lines_model : Optional[Union[str, EndLinesModel]], by default None path to trained model. If None, it will use a default model \"\"\" def __init__ ( self , nlp : Language , end_lines_model : Optional [ Union [ str , EndLinesModel ]], ** kwargs , ): super () . __init__ ( nlp , terms = None , attr = \"TEXT\" , regex = dict ( new_line = r \"\\n+\" , ), ignore_excluded = False , ** kwargs , ) if not Token . has_extension ( \"end_line\" ): Token . set_extension ( \"end_line\" , default = None ) if not Span . has_extension ( \"end_line\" ): Span . set_extension ( \"end_line\" , default = None ) self . _read_model ( end_lines_model ) def _read_model ( self , end_lines_model : Optional [ Union [ str , EndLinesModel ]]): \"\"\" Parameters ---------- end_lines_model : Optional[Union[str, EndLinesModel]] Raises ------ TypeError \"\"\" if end_lines_model is None : path = build_path ( __file__ , \"base_model.pkl\" ) with open ( path , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == str : with open ( end_lines_model , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == EndLinesModel : self . model = end_lines_model else : raise TypeError ( \"type(`end_lines_model`) should be one of {None, str, EndLinesModel}\" ) @staticmethod def _spacy_compute_a3a4 ( token : Token ) -> str : \"\"\"Function to compute A3 and A4 Parameters ---------- token : Token Returns ------- str \"\"\" if token . is_upper : return \"UPPER\" elif token . shape_ . startswith ( \"Xx\" ): return \"S_UPPER\" elif token . shape_ . startswith ( \"x\" ): return \"LOWER\" elif ( token . is_digit ) & ( ( token . doc [ max ( token . i - 1 , 0 )] . is_punct ) | ( token . doc [ min ( token . i + 1 , len ( token . doc ) - 1 )] . is_punct ) ): return \"ENUMERATION\" elif token . is_digit : return \"DIGIT\" elif ( token . is_punct ) & ( token . text in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"STRONG_PUNCT\" elif ( token . is_punct ) & ( token . text not in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"SOFT_PUNCT\" else : return \"OTHER\" @staticmethod def _compute_length ( doc : Doc , start : int , end : int ) -> int : \"\"\"Compute length without spaces Parameters ---------- doc : Doc start : int end : int Returns ------- int \"\"\" length = 0 for t in doc [ start : end ]: length += len ( t . text ) return length def _get_df ( self , doc : Doc , new_lines : List [ Span ]) -> pd . DataFrame : \"\"\"Get a pandas DataFrame to call the classifier Parameters ---------- doc : Doc new_lines : List[Span] Returns ------- pd.DataFrame \"\"\" data = [] for i , span in enumerate ( new_lines ): start = span . start end = span . end max_index = len ( doc ) - 1 a1_token = doc [ max ( start - 1 , 0 )] a2_token = doc [ min ( start + 1 , max_index )] a1 = a1_token . orth a2 = a2_token . orth a3 = self . _spacy_compute_a3a4 ( a1_token ) a4 = self . _spacy_compute_a3a4 ( a2_token ) blank_line = \" \\n\\n \" in span . text if i > 0 : start_previous = new_lines [ i - 1 ] . start + 1 else : start_previous = 0 length = self . _compute_length ( doc , start = start_previous , end = start ) # It's ok cause i count the total length from the previous up to this one data_dict = dict ( span_start = start , span_end = end , A1 = a1 , A2 = a2 , A3 = a3 , A4 = a4 , BLANK_LINE = blank_line , length = length , ) data . append ( data_dict ) df = pd . DataFrame ( data ) mu = df [ \"length\" ] . mean () sigma = df [ \"length\" ] . std () if np . isnan ( sigma ): sigma = 1 cv = sigma / mu df [ \"B1\" ] = ( df [ \"length\" ] - mu ) / sigma df [ \"B2\" ] = cv return df def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Predict for each new line if it's an end of line or a space. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, with each new line annotated \"\"\" matches = self . process ( doc ) new_lines = get_spans ( matches , \"new_line\" ) if len ( new_lines ) > 0 : df = self . _get_df ( doc = doc , new_lines = new_lines ) df = self . model . predict ( df ) spans = [] for span , prediction in zip ( new_lines , df . PREDICTED_END_LINE ): span . label_ = _get_label ( prediction ) span . _ . end_line = prediction spans . append ( span ) for t in span : t . _ . end_line = prediction if not prediction : t . _ . excluded = True doc . spans [ \"new_lines\" ] = spans return doc","title":"EndLines"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlp.pipelines.core.endlines.endlines.EndLines._read_model","text":"PARAMETER DESCRIPTION end_lines_model TYPE: Optional[Union[str, EndLinesModel]] RAISES DESCRIPTION TypeError Source code in edsnlp/pipelines/core/endlines/endlines.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def _read_model ( self , end_lines_model : Optional [ Union [ str , EndLinesModel ]]): \"\"\" Parameters ---------- end_lines_model : Optional[Union[str, EndLinesModel]] Raises ------ TypeError \"\"\" if end_lines_model is None : path = build_path ( __file__ , \"base_model.pkl\" ) with open ( path , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == str : with open ( end_lines_model , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == EndLinesModel : self . model = end_lines_model else : raise TypeError ( \"type(`end_lines_model`) should be one of {None, str, EndLinesModel}\" )","title":"_read_model()"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlp.pipelines.core.endlines.endlines.EndLines._spacy_compute_a3a4","text":"Function to compute A3 and A4 PARAMETER DESCRIPTION token TYPE: Token RETURNS DESCRIPTION str Source code in edsnlp/pipelines/core/endlines/endlines.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @staticmethod def _spacy_compute_a3a4 ( token : Token ) -> str : \"\"\"Function to compute A3 and A4 Parameters ---------- token : Token Returns ------- str \"\"\" if token . is_upper : return \"UPPER\" elif token . shape_ . startswith ( \"Xx\" ): return \"S_UPPER\" elif token . shape_ . startswith ( \"x\" ): return \"LOWER\" elif ( token . is_digit ) & ( ( token . doc [ max ( token . i - 1 , 0 )] . is_punct ) | ( token . doc [ min ( token . i + 1 , len ( token . doc ) - 1 )] . is_punct ) ): return \"ENUMERATION\" elif token . is_digit : return \"DIGIT\" elif ( token . is_punct ) & ( token . text in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"STRONG_PUNCT\" elif ( token . is_punct ) & ( token . text not in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"SOFT_PUNCT\" else : return \"OTHER\"","title":"_spacy_compute_a3a4()"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlp.pipelines.core.endlines.endlines.EndLines._compute_length","text":"Compute length without spaces PARAMETER DESCRIPTION doc TYPE: Doc start TYPE: int end TYPE: int RETURNS DESCRIPTION int Source code in edsnlp/pipelines/core/endlines/endlines.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 @staticmethod def _compute_length ( doc : Doc , start : int , end : int ) -> int : \"\"\"Compute length without spaces Parameters ---------- doc : Doc start : int end : int Returns ------- int \"\"\" length = 0 for t in doc [ start : end ]: length += len ( t . text ) return length","title":"_compute_length()"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlp.pipelines.core.endlines.endlines.EndLines._get_df","text":"Get a pandas DataFrame to call the classifier PARAMETER DESCRIPTION doc TYPE: Doc new_lines TYPE: List[Span] RETURNS DESCRIPTION pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlines.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def _get_df ( self , doc : Doc , new_lines : List [ Span ]) -> pd . DataFrame : \"\"\"Get a pandas DataFrame to call the classifier Parameters ---------- doc : Doc new_lines : List[Span] Returns ------- pd.DataFrame \"\"\" data = [] for i , span in enumerate ( new_lines ): start = span . start end = span . end max_index = len ( doc ) - 1 a1_token = doc [ max ( start - 1 , 0 )] a2_token = doc [ min ( start + 1 , max_index )] a1 = a1_token . orth a2 = a2_token . orth a3 = self . _spacy_compute_a3a4 ( a1_token ) a4 = self . _spacy_compute_a3a4 ( a2_token ) blank_line = \" \\n\\n \" in span . text if i > 0 : start_previous = new_lines [ i - 1 ] . start + 1 else : start_previous = 0 length = self . _compute_length ( doc , start = start_previous , end = start ) # It's ok cause i count the total length from the previous up to this one data_dict = dict ( span_start = start , span_end = end , A1 = a1 , A2 = a2 , A3 = a3 , A4 = a4 , BLANK_LINE = blank_line , length = length , ) data . append ( data_dict ) df = pd . DataFrame ( data ) mu = df [ \"length\" ] . mean () sigma = df [ \"length\" ] . std () if np . isnan ( sigma ): sigma = 1 cv = sigma / mu df [ \"B1\" ] = ( df [ \"length\" ] - mu ) / sigma df [ \"B2\" ] = cv return df","title":"_get_df()"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlp.pipelines.core.endlines.endlines.EndLines.__call__","text":"Predict for each new line if it's an end of line or a space. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/core/endlines/endlines.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Predict for each new line if it's an end of line or a space. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, with each new line annotated \"\"\" matches = self . process ( doc ) new_lines = get_spans ( matches , \"new_line\" ) if len ( new_lines ) > 0 : df = self . _get_df ( doc = doc , new_lines = new_lines ) df = self . model . predict ( df ) spans = [] for span , prediction in zip ( new_lines , df . PREDICTED_END_LINE ): span . label_ = _get_label ( prediction ) span . _ . end_line = prediction spans . append ( span ) for t in span : t . _ . end_line = prediction if not prediction : t . _ . excluded = True doc . spans [ \"new_lines\" ] = spans return doc","title":"__call__()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/","text":"edsnlp.pipelines.core.endlines.endlinesmodel EndLinesModel Model to classify if an end line is a real one or it should be a space. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 class EndLinesModel : \"\"\"Model to classify if an end line is a real one or it should be a space. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. \"\"\" def __init__ ( self , nlp : Language ): self . nlp = nlp def _preprocess_data ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\" Parameters ---------- corpus : Iterable[Doc] Corpus of documents Returns ------- pd.DataFrame Preprocessed data \"\"\" # Extract the vocabulary string_store = self . nlp . vocab . strings # Iterate in the corpus and construct a dataframe train_data_list = [] for i , doc in enumerate ( corpus ): train_data_list . append ( self . _get_attributes ( doc , i )) df = pd . concat ( train_data_list ) df . reset_index ( inplace = True , drop = False ) df . rename ( columns = { \"ORTH\" : \"A1\" , \"index\" : \"original_token_index\" }, inplace = True ) # Retrieve string representation of token_id and shape df [ \"TEXT\" ] = df . A1 . apply ( self . _get_string , string_store = string_store ) df [ \"SHAPE_\" ] = df . SHAPE . apply ( self . _get_string , string_store = string_store ) # Convert new lines as an attribute instead of a row df = self . _convert_line_to_attribute ( df , expr = \" \\n \" , col = \"END_LINE\" ) df = self . _convert_line_to_attribute ( df , expr = \" \\n\\n \" , col = \"BLANK_LINE\" ) df = df . loc [ ~ ( df . END_LINE | df . BLANK_LINE )] df = df . drop ( columns = \"END_LINE\" ) df = df . drop ( columns = \"BLANK_LINE\" ) df . rename ( columns = { \"TEMP_END_LINE\" : \"END_LINE\" , \"TEMP_BLANK_LINE\" : \"BLANK_LINE\" }, inplace = True , ) # Construct A2 by shifting df = self . _shift_col ( df , \"A1\" , \"A2\" , direction = \"backward\" ) # Compute A3 and A4 df = self . _compute_a3 ( df ) df = self . _shift_col ( df , \"A3\" , \"A4\" , direction = \"backward\" ) # SPACE is the class to predict. Set 1 if not an END_LINE df [ \"SPACE\" ] = np . logical_not ( df [ \"END_LINE\" ]) . astype ( \"int\" ) df [[ \"END_LINE\" , \"BLANK_LINE\" ]] = df [[ \"END_LINE\" , \"BLANK_LINE\" ]] . fillna ( True , inplace = False ) # Assign a sentence id to each token df = df . groupby ( \"DOC_ID\" ) . apply ( self . _retrieve_lines ) df [ \"SENTENCE_ID\" ] = df [ \"SENTENCE_ID\" ] . astype ( \"int\" ) # Compute B1 and B2 df = self . _compute_B ( df ) # Drop Tokens without info (last token of doc) df . dropna ( subset = [ \"A1\" , \"A2\" , \"A3\" , \"A4\" ], inplace = True ) # Export the vocabularies to be able to use the model with another corpus voc_a3a4 = self . _create_vocabulary ( df . A3_ . cat . categories ) voc_B2 = self . _create_vocabulary ( df . cv_bin . cat . categories ) voc_B1 = self . _create_vocabulary ( df . l_norm_bin . cat . categories ) vocabulary = { \"A3A4\" : voc_a3a4 , \"B1\" : voc_B1 , \"B2\" : voc_B2 } self . vocabulary = vocabulary return df def fit_and_predict ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\"Fit the model and predict for the training data Parameters ---------- corpus : Iterable[Doc] An iterable of Documents Returns ------- pd.DataFrame one line by end_line prediction \"\"\" # Preprocess data to have a pd DF df = self . _preprocess_data ( corpus ) # Train and predict M1 self . _fit_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , df . SPACE ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] # Force Blank lines to 0 df . loc [ df . BLANK_LINE , \"M1\" ] = 0 # Train and predict M2 df_endlines = df . loc [ df . END_LINE ] self . _fit_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 , label = df_endlines . M1 ) outputs_M2 = self . _predict_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 ) df . loc [ df . END_LINE , \"M2\" ] = outputs_M2 [ \"predictions\" ] df . loc [ df . END_LINE , \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df = df . loc [ df . END_LINE ] df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M2\" , \"M1M2\" ]] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df def predict ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Use the model for inference The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Parameters ---------- df : pd.DataFrame The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Returns ------- pd.DataFrame The result is added to the column `PREDICTED_END_LINE` \"\"\" df = self . _convert_raw_data_to_codes ( df ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . _A3 , df . _A4 ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] outputs_M2 = self . _predict_M2 ( B1 = df . _B1 , B2 = df . _B2 ) df [ \"M2\" ] = outputs_M2 [ \"predictions\" ] df [ \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M1M2\" , ], ] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df def save ( self , path = \"base_model.pkl\" ): \"\"\"Save a pickle of the model. It could be read by the pipeline later. Parameters ---------- path : str, optional path to file .pkl, by default `base_model.pkl` \"\"\" with open ( path , \"wb\" ) as outp : del self . nlp pickle . dump ( self , outp , pickle . HIGHEST_PROTOCOL ) def _convert_A ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame col : str column to translate Returns ------- pd.DataFrame \"\"\" cat_type_A = CategoricalDtype ( categories = self . vocabulary [ \"A3A4\" ] . keys (), ordered = True ) new_col = \"_\" + col df [ new_col ] = df [ col ] . astype ( cat_type_A ) df [ new_col ] = df [ new_col ] . cat . codes # Ensure that not known values are coded as OTHER df . loc [ ~ df [ col ] . isin ( self . vocabulary [ \"A3A4\" ] . keys ()), new_col ] = self . vocabulary [ \"A3A4\" ][ \"OTHER\" ] return df def _convert_B ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame [description] col : str column to translate Returns ------- pd.DataFrame [description] \"\"\" # Translate B1 index_B = pd . IntervalIndex ( list ( self . vocabulary [ col ] . keys ())) new_col = \"_\" + col df [ new_col ] = pd . cut ( df [ col ], index_B ) df [ new_col ] = df [ new_col ] . cat . codes df . loc [ df [ col ] >= index_B . right . max (), new_col ] = max ( self . vocabulary [ col ] . values () ) df . loc [ df [ col ] <= index_B . left . min (), new_col ] = min ( self . vocabulary [ col ] . values () ) return df def _convert_raw_data_to_codes ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Function to translate data as extracted from spacy to the model codes. `A1` and `A2` are not translated cause are supposed to be already in good encoding. Parameters ---------- df : pd.DataFrame It should have columns `['A3','A4','B1','B2']` Returns ------- pd.DataFrame \"\"\" df = self . _convert_A ( df , \"A3\" ) df = self . _convert_A ( df , \"A4\" ) df = self . _convert_B ( df , \"B1\" ) df = self . _convert_B ( df , \"B2\" ) return df def _convert_line_to_attribute ( self , df : pd . DataFrame , expr : str , col : str ) -> pd . DataFrame : \"\"\" Function to convert a line into an attribute (column) of the previous row. Particularly we use it to identify \"\\\\n\" and \"\\\\n\\\\n\" that are considered tokens, express this information as an attribute of the previous token. Parameters ---------- df : pd.DataFrame expr : str pattern to search in the text. Ex.: \"\\\\n\" col : str name of the new column Returns ------- pd.DataFrame \"\"\" idx = df . TEXT . str . contains ( expr ) df . loc [ idx , col ] = True df [ col ] = df [ col ] . fillna ( False ) df = self . _shift_col ( df , col , \"TEMP_\" + col , direction = \"backward\" ) return df def _compute_a3 ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" A3 (A4 respectively): typographic form of left word (or right) : - All in capital letter - It starts with a capital letter - Starts by lowercase - It's a number - Strong punctuation - Soft punctuation - A number followed or preced by a punctuation (it's the case of enumerations) Parameters ---------- df: pd.DataFrame Returns ------- df: pd.DataFrame with the columns `A3` and `A3_` \"\"\" df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_+1\" , direction = \"backward\" , fill = False ) df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_-1\" , direction = \"forward\" , fill = False ) CONDITION1 = df . IS_UPPER CONDITION2 = df . SHAPE_ . str . startswith ( \"Xx\" , na = False ) CONDITION3 = df . SHAPE_ . str . startswith ( \"x\" , na = False ) CONDITION4 = df . IS_DIGIT STRONG_PUNCT = [ \".\" , \";\" , \"..\" , \"...\" ] CONDITION5 = ( df . IS_PUNCT ) & ( df . TEXT . isin ( STRONG_PUNCT )) CONDITION6 = ( df . IS_PUNCT ) & ( ~ df . TEXT . isin ( STRONG_PUNCT )) CONDITION7 = ( df . IS_DIGIT ) & ( df [ \"IS_PUNCT_+1\" ] | df [ \"IS_PUNCT_-1\" ]) # discuss df [ \"A3_\" ] = None df . loc [ CONDITION1 , \"A3_\" ] = \"UPPER\" df . loc [ CONDITION2 , \"A3_\" ] = \"S_UPPER\" df . loc [ CONDITION3 , \"A3_\" ] = \"LOWER\" df . loc [ CONDITION4 , \"A3_\" ] = \"DIGIT\" df . loc [ CONDITION5 , \"A3_\" ] = \"STRONG_PUNCT\" df . loc [ CONDITION6 , \"A3_\" ] = \"SOFT_PUNCT\" df . loc [ CONDITION7 , \"A3_\" ] = \"ENUMERATION\" df = df . drop ( columns = [ \"IS_PUNCT_+1\" , \"IS_PUNCT_-1\" ]) df [ \"A3_\" ] = df [ \"A3_\" ] . astype ( \"category\" ) df [ \"A3_\" ] = df [ \"A3_\" ] . cat . add_categories ( \"OTHER\" ) df [ \"A3_\" ] . fillna ( \"OTHER\" , inplace = True ) df [ \"A3\" ] = df [ \"A3_\" ] . cat . codes return df def _fit_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series , label : pd . Series , ): \"\"\"Function to train M1 classifier (Naive Bayes) Parameters ---------- A1 : pd.Series [description] A2 : pd.Series [description] A3 : pd.Series [description] A4 : pd.Series [description] label : pd.Series [description] \"\"\" # Encode classes to OneHotEncoder representation encoder_A1_A2 = self . _fit_encoder_2S ( A1 , A2 ) self . encoder_A1_A2 = encoder_A1_A2 encoder_A3_A4 = self . _fit_encoder_2S ( A3 , A4 ) self . encoder_A3_A4 = encoder_A3_A4 # M1 m1 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) m1 . fit ( X , label ) self . m1 = m1 def _fit_M2 ( self , B1 : pd . Series , B2 : pd . Series , label : pd . Series ): \"\"\"Function to train M2 classifier (Naive Bayes) Parameters ---------- B1 : pd.Series B2 : pd.Series label : pd.Series \"\"\" # Encode classes to OneHotEncoder representation encoder_B1 = self . _fit_encoder_1S ( B1 ) self . encoder_B1 = encoder_B1 encoder_B2 = self . _fit_encoder_1S ( B2 ) self . encoder_B2 = encoder_B2 # Multinomial Naive Bayes m2 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M2 ( B1 , B2 ) m2 . fit ( X , label ) self . m2 = m2 def _get_X_for_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- np.ndarray \"\"\" A1_enc = self . _encode_series ( self . encoder_A1_A2 , A1 ) A2_enc = self . _encode_series ( self . encoder_A1_A2 , A2 ) A3_enc = self . _encode_series ( self . encoder_A3_A4 , A3 ) A4_enc = self . _encode_series ( self . encoder_A3_A4 , A4 ) X = hstack ([ A1_enc , A2_enc , A3_enc , A4_enc ]) return X def _get_X_for_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- np.ndarray \"\"\" B1_enc = self . _encode_series ( self . encoder_B1 , B1 ) B2_enc = self . _encode_series ( self . encoder_B2 , B2 ) X = hstack ([ B1_enc , B2_enc ]) return X def _predict_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M1 for prediction Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) predictions = self . m1 . predict ( X ) predictions_proba = self . m1 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs def _predict_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M2 for prediction Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M2 ( B1 , B2 ) predictions = self . m2 . predict ( X ) predictions_proba = self . m2 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs def _fit_encoder_2S ( self , S1 : pd . Series , S2 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 2 Series. It concatenates the series and after it fits. Parameters ---------- S1 : pd.Series S2 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) _S2 = _convert_series_to_array ( S2 ) S = np . concatenate ([ _S1 , _S2 ]) encoder = self . _fit_one_hot_encoder ( S ) return encoder def _fit_encoder_1S ( self , S1 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 1 Series. Parameters ---------- S1 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) encoder = self . _fit_one_hot_encoder ( _S1 ) return encoder def _encode_series ( self , encoder : OneHotEncoder , S : pd . Series ) -> np . ndarray : \"\"\"Use the one hot encoder to transform a series. Parameters ---------- encoder : OneHotEncoder S : pd.Series a series to encode (transform) Returns ------- np.ndarray \"\"\" _S = _convert_series_to_array ( S ) S_enc = encoder . transform ( _S ) return S_enc def set_spans ( self , corpus : Iterable [ Doc ], df : pd . DataFrame ): \"\"\" Function to set the results of the algorithm (pd.DataFrame) as spans of the spaCy document. Parameters ---------- corpus : Iterable[Doc] Iterable of spaCy Documents df : pd.DataFrame It should have the columns: [\"DOC_ID\",\"original_token_index\",\"PREDICTED_END_LINE\"] \"\"\" for doc_id , doc in enumerate ( corpus ): spans = [] for token_i , pred in df . loc [ df . DOC_ID == doc_id , [ \"original_token_index\" , \"PREDICTED_END_LINE\" ] ] . values : s = Span ( doc , start = token_i , end = token_i + 1 , label = _get_label ( pred )) spans . append ( s ) doc . spans [ \"new_lines\" ] = spans @staticmethod def _retrieve_lines ( dfg : DataFrameGroupBy ) -> DataFrameGroupBy : \"\"\"Function to give a sentence_id to each token. Parameters ---------- dfg : DataFrameGroupBy Returns ------- DataFrameGroupBy Same DataFrameGroupBy with the column `SENTENCE_ID` \"\"\" sentences_ids = np . arange ( dfg . END_LINE . sum ()) dfg . loc [ dfg . END_LINE , \"SENTENCE_ID\" ] = sentences_ids dfg [ \"SENTENCE_ID\" ] = dfg [ \"SENTENCE_ID\" ] . fillna ( method = \"bfill\" ) return dfg @staticmethod def _create_vocabulary ( x : iterable ) -> dict : \"\"\"Function to create a vocabulary for attributes in the training set. Parameters ---------- x : iterable Returns ------- dict \"\"\" v = {} for i , key in enumerate ( x ): v [ key ] = i return v @staticmethod def _compute_B ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Function to compute B1 and B2 Parameters ---------- df : pd.DataFrame Returns ------- pd.DataFrame \"\"\" data = df . groupby ([ \"DOC_ID\" , \"SENTENCE_ID\" ]) . agg ( l = ( \"LENGTH\" , \"sum\" )) df_t = df . loc [ df . END_LINE , [ \"DOC_ID\" , \"SENTENCE_ID\" ]] . merge ( data , left_on = [ \"DOC_ID\" , \"SENTENCE_ID\" ], right_index = True , how = \"left\" ) stats_doc = df_t . groupby ( \"DOC_ID\" ) . agg ( mu = ( \"l\" , \"mean\" ), sigma = ( \"l\" , \"std\" )) stats_doc [ \"sigma\" ] . replace ( 0.0 , 1.0 , inplace = True ) # Replace the 0 std by unit std, otherwise it breaks the code. stats_doc [ \"cv\" ] = stats_doc [ \"sigma\" ] / stats_doc [ \"mu\" ] df_t = df_t . drop ( columns = [ \"DOC_ID\" , \"SENTENCE_ID\" ]) df2 = df . merge ( df_t , left_index = True , right_index = True , how = \"left\" ) df2 = df2 . merge ( stats_doc , on = [ \"DOC_ID\" ], how = \"left\" ) df2 [ \"l_norm\" ] = ( df2 [ \"l\" ] - df2 [ \"mu\" ]) / df2 [ \"sigma\" ] df2 [ \"cv_bin\" ] = pd . cut ( df2 [ \"cv\" ], bins = 10 ) df2 [ \"B2\" ] = df2 [ \"cv_bin\" ] . cat . codes df2 [ \"l_norm_bin\" ] = pd . cut ( df2 [ \"l_norm\" ], bins = 10 ) df2 [ \"B1\" ] = df2 [ \"l_norm_bin\" ] . cat . codes return df2 @staticmethod def _shift_col ( df : pd . DataFrame , col : str , new_col : str , direction = \"backward\" , fill = None ) -> pd . DataFrame : \"\"\"Shifts a column one position into backward / forward direction. Parameters ---------- df : pd.DataFrame col : str column to shift new_col : str column name to save the results direction : str, optional one of {\"backward\", \"forward\"}, by default \"backward\" fill : [type], optional , by default None Returns ------- pd.DataFrame same df with `new_col` added. \"\"\" df [ new_col ] = fill if direction == \"backward\" : df . loc [ df . index [: - 1 ], new_col ] = df [ col ] . values [ 1 :] different_doc_id = df [ \"DOC_ID\" ] . values [: - 1 ] != df [ \"DOC_ID\" ] . values [ 1 :] different_doc_id = np . append ( different_doc_id , True ) if direction == \"forward\" : df . loc [ df . index [ 1 :], new_col ] = df [ col ] . values [: - 1 ] different_doc_id = df [ \"DOC_ID\" ] . values [ 1 :] != df [ \"DOC_ID\" ] . values [: - 1 ] different_doc_id = np . append ( True , different_doc_id ) df . loc [ different_doc_id , new_col ] = fill return df @staticmethod def _get_attributes ( doc : Doc , i = 0 ): \"\"\"Function to get the attributes of tokens of a spacy doc in a pd.DataFrame format. Parameters ---------- doc : Doc spacy Doc i : int, optional document id, by default 0 Returns ------- pd.DataFrame Returns a dataframe with one line per token. It has the following columns : `[ \"ORTH\", \"LOWER\", \"SHAPE\", \"IS_DIGIT\", \"IS_SPACE\", \"IS_UPPER\", \"IS_PUNCT\", \"LENGTH\", ]` \"\"\" attributes = [ \"ORTH\" , \"LOWER\" , \"SHAPE\" , \"IS_DIGIT\" , \"IS_SPACE\" , \"IS_UPPER\" , \"IS_PUNCT\" , \"LENGTH\" , ] attributes_array = doc . to_array ( attributes ) attributes_df = pd . DataFrame ( attributes_array , columns = attributes ) attributes_df [ \"DOC_ID\" ] = i boolean_attr = [] for a in attributes : if a [: 3 ] == \"IS_\" : boolean_attr . append ( a ) attributes_df [ boolean_attr ] = attributes_df [ boolean_attr ] . astype ( \"boolean\" ) return attributes_df @staticmethod def _get_string ( _id : int , string_store : StringStore ) -> str : \"\"\"Returns the string corresponding to the token_id Parameters ---------- _id : int token id string_store : StringStore spaCy Language String Store Returns ------- str string representation of the token. \"\"\" return string_store [ _id ] @staticmethod def _fit_one_hot_encoder ( X : np . ndarray ) -> OneHotEncoder : \"\"\"Fit a one hot encoder. Parameters ---------- X : np.ndarray of shape (n,1) Returns ------- OneHotEncoder \"\"\" encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) encoder . fit ( X ) return encoder _preprocess_data ( corpus ) PARAMETER DESCRIPTION corpus Corpus of documents TYPE: Iterable[Doc] RETURNS DESCRIPTION pd.DataFrame Preprocessed data Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def _preprocess_data ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\" Parameters ---------- corpus : Iterable[Doc] Corpus of documents Returns ------- pd.DataFrame Preprocessed data \"\"\" # Extract the vocabulary string_store = self . nlp . vocab . strings # Iterate in the corpus and construct a dataframe train_data_list = [] for i , doc in enumerate ( corpus ): train_data_list . append ( self . _get_attributes ( doc , i )) df = pd . concat ( train_data_list ) df . reset_index ( inplace = True , drop = False ) df . rename ( columns = { \"ORTH\" : \"A1\" , \"index\" : \"original_token_index\" }, inplace = True ) # Retrieve string representation of token_id and shape df [ \"TEXT\" ] = df . A1 . apply ( self . _get_string , string_store = string_store ) df [ \"SHAPE_\" ] = df . SHAPE . apply ( self . _get_string , string_store = string_store ) # Convert new lines as an attribute instead of a row df = self . _convert_line_to_attribute ( df , expr = \" \\n \" , col = \"END_LINE\" ) df = self . _convert_line_to_attribute ( df , expr = \" \\n\\n \" , col = \"BLANK_LINE\" ) df = df . loc [ ~ ( df . END_LINE | df . BLANK_LINE )] df = df . drop ( columns = \"END_LINE\" ) df = df . drop ( columns = \"BLANK_LINE\" ) df . rename ( columns = { \"TEMP_END_LINE\" : \"END_LINE\" , \"TEMP_BLANK_LINE\" : \"BLANK_LINE\" }, inplace = True , ) # Construct A2 by shifting df = self . _shift_col ( df , \"A1\" , \"A2\" , direction = \"backward\" ) # Compute A3 and A4 df = self . _compute_a3 ( df ) df = self . _shift_col ( df , \"A3\" , \"A4\" , direction = \"backward\" ) # SPACE is the class to predict. Set 1 if not an END_LINE df [ \"SPACE\" ] = np . logical_not ( df [ \"END_LINE\" ]) . astype ( \"int\" ) df [[ \"END_LINE\" , \"BLANK_LINE\" ]] = df [[ \"END_LINE\" , \"BLANK_LINE\" ]] . fillna ( True , inplace = False ) # Assign a sentence id to each token df = df . groupby ( \"DOC_ID\" ) . apply ( self . _retrieve_lines ) df [ \"SENTENCE_ID\" ] = df [ \"SENTENCE_ID\" ] . astype ( \"int\" ) # Compute B1 and B2 df = self . _compute_B ( df ) # Drop Tokens without info (last token of doc) df . dropna ( subset = [ \"A1\" , \"A2\" , \"A3\" , \"A4\" ], inplace = True ) # Export the vocabularies to be able to use the model with another corpus voc_a3a4 = self . _create_vocabulary ( df . A3_ . cat . categories ) voc_B2 = self . _create_vocabulary ( df . cv_bin . cat . categories ) voc_B1 = self . _create_vocabulary ( df . l_norm_bin . cat . categories ) vocabulary = { \"A3A4\" : voc_a3a4 , \"B1\" : voc_B1 , \"B2\" : voc_B2 } self . vocabulary = vocabulary return df fit_and_predict ( corpus ) Fit the model and predict for the training data PARAMETER DESCRIPTION corpus An iterable of Documents TYPE: Iterable[Doc] RETURNS DESCRIPTION pd.DataFrame one line by end_line prediction Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def fit_and_predict ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\"Fit the model and predict for the training data Parameters ---------- corpus : Iterable[Doc] An iterable of Documents Returns ------- pd.DataFrame one line by end_line prediction \"\"\" # Preprocess data to have a pd DF df = self . _preprocess_data ( corpus ) # Train and predict M1 self . _fit_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , df . SPACE ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] # Force Blank lines to 0 df . loc [ df . BLANK_LINE , \"M1\" ] = 0 # Train and predict M2 df_endlines = df . loc [ df . END_LINE ] self . _fit_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 , label = df_endlines . M1 ) outputs_M2 = self . _predict_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 ) df . loc [ df . END_LINE , \"M2\" ] = outputs_M2 [ \"predictions\" ] df . loc [ df . END_LINE , \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df = df . loc [ df . END_LINE ] df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M2\" , \"M1M2\" ]] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df predict ( df ) Use the model for inference The df should have the following columns: [\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"] PARAMETER DESCRIPTION df The df should have the following columns: [\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"] TYPE: pd.DataFrame RETURNS DESCRIPTION pd.DataFrame The result is added to the column PREDICTED_END_LINE Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def predict ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Use the model for inference The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Parameters ---------- df : pd.DataFrame The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Returns ------- pd.DataFrame The result is added to the column `PREDICTED_END_LINE` \"\"\" df = self . _convert_raw_data_to_codes ( df ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . _A3 , df . _A4 ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] outputs_M2 = self . _predict_M2 ( B1 = df . _B1 , B2 = df . _B2 ) df [ \"M2\" ] = outputs_M2 [ \"predictions\" ] df [ \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M1M2\" , ], ] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df save ( path = 'base_model.pkl' ) Save a pickle of the model. It could be read by the pipeline later. PARAMETER DESCRIPTION path path to file .pkl, by default base_model.pkl TYPE: str, optional DEFAULT: 'base_model.pkl' Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 213 214 215 216 217 218 219 220 221 222 223 def save ( self , path = \"base_model.pkl\" ): \"\"\"Save a pickle of the model. It could be read by the pipeline later. Parameters ---------- path : str, optional path to file .pkl, by default `base_model.pkl` \"\"\" with open ( path , \"wb\" ) as outp : del self . nlp pickle . dump ( self , outp , pickle . HIGHEST_PROTOCOL ) _convert_A ( df , col ) PARAMETER DESCRIPTION df TYPE: pd.DataFrame col column to translate TYPE: str RETURNS DESCRIPTION pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def _convert_A ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame col : str column to translate Returns ------- pd.DataFrame \"\"\" cat_type_A = CategoricalDtype ( categories = self . vocabulary [ \"A3A4\" ] . keys (), ordered = True ) new_col = \"_\" + col df [ new_col ] = df [ col ] . astype ( cat_type_A ) df [ new_col ] = df [ new_col ] . cat . codes # Ensure that not known values are coded as OTHER df . loc [ ~ df [ col ] . isin ( self . vocabulary [ \"A3A4\" ] . keys ()), new_col ] = self . vocabulary [ \"A3A4\" ][ \"OTHER\" ] return df _convert_B ( df , col ) PARAMETER DESCRIPTION df [description] TYPE: pd.DataFrame col column to translate TYPE: str RETURNS DESCRIPTION pd.DataFrame [description] Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def _convert_B ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame [description] col : str column to translate Returns ------- pd.DataFrame [description] \"\"\" # Translate B1 index_B = pd . IntervalIndex ( list ( self . vocabulary [ col ] . keys ())) new_col = \"_\" + col df [ new_col ] = pd . cut ( df [ col ], index_B ) df [ new_col ] = df [ new_col ] . cat . codes df . loc [ df [ col ] >= index_B . right . max (), new_col ] = max ( self . vocabulary [ col ] . values () ) df . loc [ df [ col ] <= index_B . left . min (), new_col ] = min ( self . vocabulary [ col ] . values () ) return df _convert_raw_data_to_codes ( df ) Function to translate data as extracted from spacy to the model codes. A1 and A2 are not translated cause are supposed to be already in good encoding. PARAMETER DESCRIPTION df It should have columns ['A3','A4','B1','B2'] TYPE: pd.DataFrame RETURNS DESCRIPTION pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def _convert_raw_data_to_codes ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Function to translate data as extracted from spacy to the model codes. `A1` and `A2` are not translated cause are supposed to be already in good encoding. Parameters ---------- df : pd.DataFrame It should have columns `['A3','A4','B1','B2']` Returns ------- pd.DataFrame \"\"\" df = self . _convert_A ( df , \"A3\" ) df = self . _convert_A ( df , \"A4\" ) df = self . _convert_B ( df , \"B1\" ) df = self . _convert_B ( df , \"B2\" ) return df _convert_line_to_attribute ( df , expr , col ) Function to convert a line into an attribute (column) of the previous row. Particularly we use it to identify \"\\n\" and \"\\n\\n\" that are considered tokens, express this information as an attribute of the previous token. PARAMETER DESCRIPTION df TYPE: pd.DataFrame expr pattern to search in the text. Ex.: \"\\n\" TYPE: str col name of the new column TYPE: str RETURNS DESCRIPTION pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 def _convert_line_to_attribute ( self , df : pd . DataFrame , expr : str , col : str ) -> pd . DataFrame : \"\"\" Function to convert a line into an attribute (column) of the previous row. Particularly we use it to identify \"\\\\n\" and \"\\\\n\\\\n\" that are considered tokens, express this information as an attribute of the previous token. Parameters ---------- df : pd.DataFrame expr : str pattern to search in the text. Ex.: \"\\\\n\" col : str name of the new column Returns ------- pd.DataFrame \"\"\" idx = df . TEXT . str . contains ( expr ) df . loc [ idx , col ] = True df [ col ] = df [ col ] . fillna ( False ) df = self . _shift_col ( df , col , \"TEMP_\" + col , direction = \"backward\" ) return df _compute_a3 ( df ) A3 (A4 respectively): typographic form of left word (or right) : All in capital letter It starts with a capital letter Starts by lowercase It's a number Strong punctuation Soft punctuation A number followed or preced by a punctuation (it's the case of enumerations) PARAMETER DESCRIPTION df TYPE: pd . DataFrame RETURNS DESCRIPTION df Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def _compute_a3 ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" A3 (A4 respectively): typographic form of left word (or right) : - All in capital letter - It starts with a capital letter - Starts by lowercase - It's a number - Strong punctuation - Soft punctuation - A number followed or preced by a punctuation (it's the case of enumerations) Parameters ---------- df: pd.DataFrame Returns ------- df: pd.DataFrame with the columns `A3` and `A3_` \"\"\" df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_+1\" , direction = \"backward\" , fill = False ) df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_-1\" , direction = \"forward\" , fill = False ) CONDITION1 = df . IS_UPPER CONDITION2 = df . SHAPE_ . str . startswith ( \"Xx\" , na = False ) CONDITION3 = df . SHAPE_ . str . startswith ( \"x\" , na = False ) CONDITION4 = df . IS_DIGIT STRONG_PUNCT = [ \".\" , \";\" , \"..\" , \"...\" ] CONDITION5 = ( df . IS_PUNCT ) & ( df . TEXT . isin ( STRONG_PUNCT )) CONDITION6 = ( df . IS_PUNCT ) & ( ~ df . TEXT . isin ( STRONG_PUNCT )) CONDITION7 = ( df . IS_DIGIT ) & ( df [ \"IS_PUNCT_+1\" ] | df [ \"IS_PUNCT_-1\" ]) # discuss df [ \"A3_\" ] = None df . loc [ CONDITION1 , \"A3_\" ] = \"UPPER\" df . loc [ CONDITION2 , \"A3_\" ] = \"S_UPPER\" df . loc [ CONDITION3 , \"A3_\" ] = \"LOWER\" df . loc [ CONDITION4 , \"A3_\" ] = \"DIGIT\" df . loc [ CONDITION5 , \"A3_\" ] = \"STRONG_PUNCT\" df . loc [ CONDITION6 , \"A3_\" ] = \"SOFT_PUNCT\" df . loc [ CONDITION7 , \"A3_\" ] = \"ENUMERATION\" df = df . drop ( columns = [ \"IS_PUNCT_+1\" , \"IS_PUNCT_-1\" ]) df [ \"A3_\" ] = df [ \"A3_\" ] . astype ( \"category\" ) df [ \"A3_\" ] = df [ \"A3_\" ] . cat . add_categories ( \"OTHER\" ) df [ \"A3_\" ] . fillna ( \"OTHER\" , inplace = True ) df [ \"A3\" ] = df [ \"A3_\" ] . cat . codes return df _fit_M1 ( A1 , A2 , A3 , A4 , label ) Function to train M1 classifier (Naive Bayes) PARAMETER DESCRIPTION A1 [description] TYPE: pd.Series A2 [description] TYPE: pd.Series A3 [description] TYPE: pd.Series A4 [description] TYPE: pd.Series label [description] TYPE: pd.Series Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 def _fit_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series , label : pd . Series , ): \"\"\"Function to train M1 classifier (Naive Bayes) Parameters ---------- A1 : pd.Series [description] A2 : pd.Series [description] A3 : pd.Series [description] A4 : pd.Series [description] label : pd.Series [description] \"\"\" # Encode classes to OneHotEncoder representation encoder_A1_A2 = self . _fit_encoder_2S ( A1 , A2 ) self . encoder_A1_A2 = encoder_A1_A2 encoder_A3_A4 = self . _fit_encoder_2S ( A3 , A4 ) self . encoder_A3_A4 = encoder_A3_A4 # M1 m1 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) m1 . fit ( X , label ) self . m1 = m1 _fit_M2 ( B1 , B2 , label ) Function to train M2 classifier (Naive Bayes) PARAMETER DESCRIPTION B1 TYPE: pd.Series B2 TYPE: pd.Series label TYPE: pd.Series Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 def _fit_M2 ( self , B1 : pd . Series , B2 : pd . Series , label : pd . Series ): \"\"\"Function to train M2 classifier (Naive Bayes) Parameters ---------- B1 : pd.Series B2 : pd.Series label : pd.Series \"\"\" # Encode classes to OneHotEncoder representation encoder_B1 = self . _fit_encoder_1S ( B1 ) self . encoder_B1 = encoder_B1 encoder_B2 = self . _fit_encoder_1S ( B2 ) self . encoder_B2 = encoder_B2 # Multinomial Naive Bayes m2 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M2 ( B1 , B2 ) m2 . fit ( X , label ) self . m2 = m2 _get_X_for_M1 ( A1 , A2 , A3 , A4 ) Get X matrix for classifier PARAMETER DESCRIPTION A1 TYPE: pd.Series A2 TYPE: pd.Series A3 TYPE: pd.Series A4 TYPE: pd.Series RETURNS DESCRIPTION np.ndarray Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def _get_X_for_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- np.ndarray \"\"\" A1_enc = self . _encode_series ( self . encoder_A1_A2 , A1 ) A2_enc = self . _encode_series ( self . encoder_A1_A2 , A2 ) A3_enc = self . _encode_series ( self . encoder_A3_A4 , A3 ) A4_enc = self . _encode_series ( self . encoder_A3_A4 , A4 ) X = hstack ([ A1_enc , A2_enc , A3_enc , A4_enc ]) return X _get_X_for_M2 ( B1 , B2 ) Get X matrix for classifier PARAMETER DESCRIPTION B1 TYPE: pd.Series B2 TYPE: pd.Series RETURNS DESCRIPTION np.ndarray Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def _get_X_for_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- np.ndarray \"\"\" B1_enc = self . _encode_series ( self . encoder_B1 , B1 ) B2_enc = self . _encode_series ( self . encoder_B2 , B2 ) X = hstack ([ B1_enc , B2_enc ]) return X _predict_M1 ( A1 , A2 , A3 , A4 ) Use M1 for prediction PARAMETER DESCRIPTION A1 TYPE: pd.Series A2 TYPE: pd.Series A3 TYPE: pd.Series A4 TYPE: pd.Series RETURNS DESCRIPTION Dict[str, Any] Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 def _predict_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M1 for prediction Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) predictions = self . m1 . predict ( X ) predictions_proba = self . m1 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs _predict_M2 ( B1 , B2 ) Use M2 for prediction PARAMETER DESCRIPTION B1 TYPE: pd.Series B2 TYPE: pd.Series RETURNS DESCRIPTION Dict[str, Any] Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def _predict_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M2 for prediction Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M2 ( B1 , B2 ) predictions = self . m2 . predict ( X ) predictions_proba = self . m2 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs _fit_encoder_2S ( S1 , S2 ) Fit a one hot encoder with 2 Series. It concatenates the series and after it fits. PARAMETER DESCRIPTION S1 TYPE: pd.Series S2 TYPE: pd.Series RETURNS DESCRIPTION OneHotEncoder Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 def _fit_encoder_2S ( self , S1 : pd . Series , S2 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 2 Series. It concatenates the series and after it fits. Parameters ---------- S1 : pd.Series S2 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) _S2 = _convert_series_to_array ( S2 ) S = np . concatenate ([ _S1 , _S2 ]) encoder = self . _fit_one_hot_encoder ( S ) return encoder _fit_encoder_1S ( S1 ) Fit a one hot encoder with 1 Series. PARAMETER DESCRIPTION S1 TYPE: pd.Series RETURNS DESCRIPTION OneHotEncoder Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 540 541 542 543 544 545 546 547 548 549 550 551 552 553 def _fit_encoder_1S ( self , S1 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 1 Series. Parameters ---------- S1 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) encoder = self . _fit_one_hot_encoder ( _S1 ) return encoder _encode_series ( encoder , S ) Use the one hot encoder to transform a series. PARAMETER DESCRIPTION encoder TYPE: OneHotEncoder S a series to encode (transform) TYPE: pd.Series RETURNS DESCRIPTION np.ndarray Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 def _encode_series ( self , encoder : OneHotEncoder , S : pd . Series ) -> np . ndarray : \"\"\"Use the one hot encoder to transform a series. Parameters ---------- encoder : OneHotEncoder S : pd.Series a series to encode (transform) Returns ------- np.ndarray \"\"\" _S = _convert_series_to_array ( S ) S_enc = encoder . transform ( _S ) return S_enc set_spans ( corpus , df ) Function to set the results of the algorithm (pd.DataFrame) as spans of the spaCy document. PARAMETER DESCRIPTION corpus Iterable of spaCy Documents TYPE: Iterable[Doc] df It should have the columns: [\"DOC_ID\",\"original_token_index\",\"PREDICTED_END_LINE\"] TYPE: pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def set_spans ( self , corpus : Iterable [ Doc ], df : pd . DataFrame ): \"\"\" Function to set the results of the algorithm (pd.DataFrame) as spans of the spaCy document. Parameters ---------- corpus : Iterable[Doc] Iterable of spaCy Documents df : pd.DataFrame It should have the columns: [\"DOC_ID\",\"original_token_index\",\"PREDICTED_END_LINE\"] \"\"\" for doc_id , doc in enumerate ( corpus ): spans = [] for token_i , pred in df . loc [ df . DOC_ID == doc_id , [ \"original_token_index\" , \"PREDICTED_END_LINE\" ] ] . values : s = Span ( doc , start = token_i , end = token_i + 1 , label = _get_label ( pred )) spans . append ( s ) doc . spans [ \"new_lines\" ] = spans _retrieve_lines ( dfg ) Function to give a sentence_id to each token. PARAMETER DESCRIPTION dfg TYPE: DataFrameGroupBy RETURNS DESCRIPTION DataFrameGroupBy Same DataFrameGroupBy with the column SENTENCE_ID Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 @staticmethod def _retrieve_lines ( dfg : DataFrameGroupBy ) -> DataFrameGroupBy : \"\"\"Function to give a sentence_id to each token. Parameters ---------- dfg : DataFrameGroupBy Returns ------- DataFrameGroupBy Same DataFrameGroupBy with the column `SENTENCE_ID` \"\"\" sentences_ids = np . arange ( dfg . END_LINE . sum ()) dfg . loc [ dfg . END_LINE , \"SENTENCE_ID\" ] = sentences_ids dfg [ \"SENTENCE_ID\" ] = dfg [ \"SENTENCE_ID\" ] . fillna ( method = \"bfill\" ) return dfg _create_vocabulary ( x ) Function to create a vocabulary for attributes in the training set. PARAMETER DESCRIPTION x TYPE: iterable RETURNS DESCRIPTION dict Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 @staticmethod def _create_vocabulary ( x : iterable ) -> dict : \"\"\"Function to create a vocabulary for attributes in the training set. Parameters ---------- x : iterable Returns ------- dict \"\"\" v = {} for i , key in enumerate ( x ): v [ key ] = i return v _compute_B ( df ) Function to compute B1 and B2 PARAMETER DESCRIPTION df TYPE: pd.DataFrame RETURNS DESCRIPTION pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 @staticmethod def _compute_B ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Function to compute B1 and B2 Parameters ---------- df : pd.DataFrame Returns ------- pd.DataFrame \"\"\" data = df . groupby ([ \"DOC_ID\" , \"SENTENCE_ID\" ]) . agg ( l = ( \"LENGTH\" , \"sum\" )) df_t = df . loc [ df . END_LINE , [ \"DOC_ID\" , \"SENTENCE_ID\" ]] . merge ( data , left_on = [ \"DOC_ID\" , \"SENTENCE_ID\" ], right_index = True , how = \"left\" ) stats_doc = df_t . groupby ( \"DOC_ID\" ) . agg ( mu = ( \"l\" , \"mean\" ), sigma = ( \"l\" , \"std\" )) stats_doc [ \"sigma\" ] . replace ( 0.0 , 1.0 , inplace = True ) # Replace the 0 std by unit std, otherwise it breaks the code. stats_doc [ \"cv\" ] = stats_doc [ \"sigma\" ] / stats_doc [ \"mu\" ] df_t = df_t . drop ( columns = [ \"DOC_ID\" , \"SENTENCE_ID\" ]) df2 = df . merge ( df_t , left_index = True , right_index = True , how = \"left\" ) df2 = df2 . merge ( stats_doc , on = [ \"DOC_ID\" ], how = \"left\" ) df2 [ \"l_norm\" ] = ( df2 [ \"l\" ] - df2 [ \"mu\" ]) / df2 [ \"sigma\" ] df2 [ \"cv_bin\" ] = pd . cut ( df2 [ \"cv\" ], bins = 10 ) df2 [ \"B2\" ] = df2 [ \"cv_bin\" ] . cat . codes df2 [ \"l_norm_bin\" ] = pd . cut ( df2 [ \"l_norm\" ], bins = 10 ) df2 [ \"B1\" ] = df2 [ \"l_norm_bin\" ] . cat . codes return df2 _shift_col ( df , col , new_col , direction = 'backward' , fill = None ) Shifts a column one position into backward / forward direction. PARAMETER DESCRIPTION df TYPE: pd.DataFrame col column to shift TYPE: str new_col column name to save the results TYPE: str direction one of {\"backward\", \"forward\"}, by default \"backward\" TYPE: str, optional DEFAULT: 'backward' fill , by default None TYPE: [type], optional DEFAULT: None RETURNS DESCRIPTION pd.DataFrame same df with new_col added. Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 @staticmethod def _shift_col ( df : pd . DataFrame , col : str , new_col : str , direction = \"backward\" , fill = None ) -> pd . DataFrame : \"\"\"Shifts a column one position into backward / forward direction. Parameters ---------- df : pd.DataFrame col : str column to shift new_col : str column name to save the results direction : str, optional one of {\"backward\", \"forward\"}, by default \"backward\" fill : [type], optional , by default None Returns ------- pd.DataFrame same df with `new_col` added. \"\"\" df [ new_col ] = fill if direction == \"backward\" : df . loc [ df . index [: - 1 ], new_col ] = df [ col ] . values [ 1 :] different_doc_id = df [ \"DOC_ID\" ] . values [: - 1 ] != df [ \"DOC_ID\" ] . values [ 1 :] different_doc_id = np . append ( different_doc_id , True ) if direction == \"forward\" : df . loc [ df . index [ 1 :], new_col ] = df [ col ] . values [: - 1 ] different_doc_id = df [ \"DOC_ID\" ] . values [ 1 :] != df [ \"DOC_ID\" ] . values [: - 1 ] different_doc_id = np . append ( True , different_doc_id ) df . loc [ different_doc_id , new_col ] = fill return df _get_attributes ( doc , i = 0 ) Function to get the attributes of tokens of a spacy doc in a pd.DataFrame format. PARAMETER DESCRIPTION doc spacy Doc TYPE: Doc i document id, by default 0 TYPE: int, optional DEFAULT: 0 RETURNS DESCRIPTION pd.DataFrame Returns a dataframe with one line per token. It has the following columns : [ \"ORTH\", \"LOWER\", \"SHAPE\", \"IS_DIGIT\", \"IS_SPACE\", \"IS_UPPER\", \"IS_PUNCT\", \"LENGTH\", ] Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 @staticmethod def _get_attributes ( doc : Doc , i = 0 ): \"\"\"Function to get the attributes of tokens of a spacy doc in a pd.DataFrame format. Parameters ---------- doc : Doc spacy Doc i : int, optional document id, by default 0 Returns ------- pd.DataFrame Returns a dataframe with one line per token. It has the following columns : `[ \"ORTH\", \"LOWER\", \"SHAPE\", \"IS_DIGIT\", \"IS_SPACE\", \"IS_UPPER\", \"IS_PUNCT\", \"LENGTH\", ]` \"\"\" attributes = [ \"ORTH\" , \"LOWER\" , \"SHAPE\" , \"IS_DIGIT\" , \"IS_SPACE\" , \"IS_UPPER\" , \"IS_PUNCT\" , \"LENGTH\" , ] attributes_array = doc . to_array ( attributes ) attributes_df = pd . DataFrame ( attributes_array , columns = attributes ) attributes_df [ \"DOC_ID\" ] = i boolean_attr = [] for a in attributes : if a [: 3 ] == \"IS_\" : boolean_attr . append ( a ) attributes_df [ boolean_attr ] = attributes_df [ boolean_attr ] . astype ( \"boolean\" ) return attributes_df _get_string ( _id , string_store ) Returns the string corresponding to the token_id PARAMETER DESCRIPTION _id token id TYPE: int string_store spaCy Language String Store TYPE: StringStore RETURNS DESCRIPTION str string representation of the token. Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 @staticmethod def _get_string ( _id : int , string_store : StringStore ) -> str : \"\"\"Returns the string corresponding to the token_id Parameters ---------- _id : int token id string_store : StringStore spaCy Language String Store Returns ------- str string representation of the token. \"\"\" return string_store [ _id ] _fit_one_hot_encoder ( X ) Fit a one hot encoder. PARAMETER DESCRIPTION X of shape (n,1) TYPE: np.ndarray RETURNS DESCRIPTION OneHotEncoder Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 @staticmethod def _fit_one_hot_encoder ( X : np . ndarray ) -> OneHotEncoder : \"\"\"Fit a one hot encoder. Parameters ---------- X : np.ndarray of shape (n,1) Returns ------- OneHotEncoder \"\"\" encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) encoder . fit ( X ) return encoder","title":"endlinesmodel"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlppipelinescoreendlinesendlinesmodel","text":"","title":"edsnlp.pipelines.core.endlines.endlinesmodel"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel","text":"Model to classify if an end line is a real one or it should be a space. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 class EndLinesModel : \"\"\"Model to classify if an end line is a real one or it should be a space. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. \"\"\" def __init__ ( self , nlp : Language ): self . nlp = nlp def _preprocess_data ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\" Parameters ---------- corpus : Iterable[Doc] Corpus of documents Returns ------- pd.DataFrame Preprocessed data \"\"\" # Extract the vocabulary string_store = self . nlp . vocab . strings # Iterate in the corpus and construct a dataframe train_data_list = [] for i , doc in enumerate ( corpus ): train_data_list . append ( self . _get_attributes ( doc , i )) df = pd . concat ( train_data_list ) df . reset_index ( inplace = True , drop = False ) df . rename ( columns = { \"ORTH\" : \"A1\" , \"index\" : \"original_token_index\" }, inplace = True ) # Retrieve string representation of token_id and shape df [ \"TEXT\" ] = df . A1 . apply ( self . _get_string , string_store = string_store ) df [ \"SHAPE_\" ] = df . SHAPE . apply ( self . _get_string , string_store = string_store ) # Convert new lines as an attribute instead of a row df = self . _convert_line_to_attribute ( df , expr = \" \\n \" , col = \"END_LINE\" ) df = self . _convert_line_to_attribute ( df , expr = \" \\n\\n \" , col = \"BLANK_LINE\" ) df = df . loc [ ~ ( df . END_LINE | df . BLANK_LINE )] df = df . drop ( columns = \"END_LINE\" ) df = df . drop ( columns = \"BLANK_LINE\" ) df . rename ( columns = { \"TEMP_END_LINE\" : \"END_LINE\" , \"TEMP_BLANK_LINE\" : \"BLANK_LINE\" }, inplace = True , ) # Construct A2 by shifting df = self . _shift_col ( df , \"A1\" , \"A2\" , direction = \"backward\" ) # Compute A3 and A4 df = self . _compute_a3 ( df ) df = self . _shift_col ( df , \"A3\" , \"A4\" , direction = \"backward\" ) # SPACE is the class to predict. Set 1 if not an END_LINE df [ \"SPACE\" ] = np . logical_not ( df [ \"END_LINE\" ]) . astype ( \"int\" ) df [[ \"END_LINE\" , \"BLANK_LINE\" ]] = df [[ \"END_LINE\" , \"BLANK_LINE\" ]] . fillna ( True , inplace = False ) # Assign a sentence id to each token df = df . groupby ( \"DOC_ID\" ) . apply ( self . _retrieve_lines ) df [ \"SENTENCE_ID\" ] = df [ \"SENTENCE_ID\" ] . astype ( \"int\" ) # Compute B1 and B2 df = self . _compute_B ( df ) # Drop Tokens without info (last token of doc) df . dropna ( subset = [ \"A1\" , \"A2\" , \"A3\" , \"A4\" ], inplace = True ) # Export the vocabularies to be able to use the model with another corpus voc_a3a4 = self . _create_vocabulary ( df . A3_ . cat . categories ) voc_B2 = self . _create_vocabulary ( df . cv_bin . cat . categories ) voc_B1 = self . _create_vocabulary ( df . l_norm_bin . cat . categories ) vocabulary = { \"A3A4\" : voc_a3a4 , \"B1\" : voc_B1 , \"B2\" : voc_B2 } self . vocabulary = vocabulary return df def fit_and_predict ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\"Fit the model and predict for the training data Parameters ---------- corpus : Iterable[Doc] An iterable of Documents Returns ------- pd.DataFrame one line by end_line prediction \"\"\" # Preprocess data to have a pd DF df = self . _preprocess_data ( corpus ) # Train and predict M1 self . _fit_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , df . SPACE ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] # Force Blank lines to 0 df . loc [ df . BLANK_LINE , \"M1\" ] = 0 # Train and predict M2 df_endlines = df . loc [ df . END_LINE ] self . _fit_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 , label = df_endlines . M1 ) outputs_M2 = self . _predict_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 ) df . loc [ df . END_LINE , \"M2\" ] = outputs_M2 [ \"predictions\" ] df . loc [ df . END_LINE , \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df = df . loc [ df . END_LINE ] df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M2\" , \"M1M2\" ]] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df def predict ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Use the model for inference The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Parameters ---------- df : pd.DataFrame The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Returns ------- pd.DataFrame The result is added to the column `PREDICTED_END_LINE` \"\"\" df = self . _convert_raw_data_to_codes ( df ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . _A3 , df . _A4 ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] outputs_M2 = self . _predict_M2 ( B1 = df . _B1 , B2 = df . _B2 ) df [ \"M2\" ] = outputs_M2 [ \"predictions\" ] df [ \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M1M2\" , ], ] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df def save ( self , path = \"base_model.pkl\" ): \"\"\"Save a pickle of the model. It could be read by the pipeline later. Parameters ---------- path : str, optional path to file .pkl, by default `base_model.pkl` \"\"\" with open ( path , \"wb\" ) as outp : del self . nlp pickle . dump ( self , outp , pickle . HIGHEST_PROTOCOL ) def _convert_A ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame col : str column to translate Returns ------- pd.DataFrame \"\"\" cat_type_A = CategoricalDtype ( categories = self . vocabulary [ \"A3A4\" ] . keys (), ordered = True ) new_col = \"_\" + col df [ new_col ] = df [ col ] . astype ( cat_type_A ) df [ new_col ] = df [ new_col ] . cat . codes # Ensure that not known values are coded as OTHER df . loc [ ~ df [ col ] . isin ( self . vocabulary [ \"A3A4\" ] . keys ()), new_col ] = self . vocabulary [ \"A3A4\" ][ \"OTHER\" ] return df def _convert_B ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame [description] col : str column to translate Returns ------- pd.DataFrame [description] \"\"\" # Translate B1 index_B = pd . IntervalIndex ( list ( self . vocabulary [ col ] . keys ())) new_col = \"_\" + col df [ new_col ] = pd . cut ( df [ col ], index_B ) df [ new_col ] = df [ new_col ] . cat . codes df . loc [ df [ col ] >= index_B . right . max (), new_col ] = max ( self . vocabulary [ col ] . values () ) df . loc [ df [ col ] <= index_B . left . min (), new_col ] = min ( self . vocabulary [ col ] . values () ) return df def _convert_raw_data_to_codes ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Function to translate data as extracted from spacy to the model codes. `A1` and `A2` are not translated cause are supposed to be already in good encoding. Parameters ---------- df : pd.DataFrame It should have columns `['A3','A4','B1','B2']` Returns ------- pd.DataFrame \"\"\" df = self . _convert_A ( df , \"A3\" ) df = self . _convert_A ( df , \"A4\" ) df = self . _convert_B ( df , \"B1\" ) df = self . _convert_B ( df , \"B2\" ) return df def _convert_line_to_attribute ( self , df : pd . DataFrame , expr : str , col : str ) -> pd . DataFrame : \"\"\" Function to convert a line into an attribute (column) of the previous row. Particularly we use it to identify \"\\\\n\" and \"\\\\n\\\\n\" that are considered tokens, express this information as an attribute of the previous token. Parameters ---------- df : pd.DataFrame expr : str pattern to search in the text. Ex.: \"\\\\n\" col : str name of the new column Returns ------- pd.DataFrame \"\"\" idx = df . TEXT . str . contains ( expr ) df . loc [ idx , col ] = True df [ col ] = df [ col ] . fillna ( False ) df = self . _shift_col ( df , col , \"TEMP_\" + col , direction = \"backward\" ) return df def _compute_a3 ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" A3 (A4 respectively): typographic form of left word (or right) : - All in capital letter - It starts with a capital letter - Starts by lowercase - It's a number - Strong punctuation - Soft punctuation - A number followed or preced by a punctuation (it's the case of enumerations) Parameters ---------- df: pd.DataFrame Returns ------- df: pd.DataFrame with the columns `A3` and `A3_` \"\"\" df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_+1\" , direction = \"backward\" , fill = False ) df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_-1\" , direction = \"forward\" , fill = False ) CONDITION1 = df . IS_UPPER CONDITION2 = df . SHAPE_ . str . startswith ( \"Xx\" , na = False ) CONDITION3 = df . SHAPE_ . str . startswith ( \"x\" , na = False ) CONDITION4 = df . IS_DIGIT STRONG_PUNCT = [ \".\" , \";\" , \"..\" , \"...\" ] CONDITION5 = ( df . IS_PUNCT ) & ( df . TEXT . isin ( STRONG_PUNCT )) CONDITION6 = ( df . IS_PUNCT ) & ( ~ df . TEXT . isin ( STRONG_PUNCT )) CONDITION7 = ( df . IS_DIGIT ) & ( df [ \"IS_PUNCT_+1\" ] | df [ \"IS_PUNCT_-1\" ]) # discuss df [ \"A3_\" ] = None df . loc [ CONDITION1 , \"A3_\" ] = \"UPPER\" df . loc [ CONDITION2 , \"A3_\" ] = \"S_UPPER\" df . loc [ CONDITION3 , \"A3_\" ] = \"LOWER\" df . loc [ CONDITION4 , \"A3_\" ] = \"DIGIT\" df . loc [ CONDITION5 , \"A3_\" ] = \"STRONG_PUNCT\" df . loc [ CONDITION6 , \"A3_\" ] = \"SOFT_PUNCT\" df . loc [ CONDITION7 , \"A3_\" ] = \"ENUMERATION\" df = df . drop ( columns = [ \"IS_PUNCT_+1\" , \"IS_PUNCT_-1\" ]) df [ \"A3_\" ] = df [ \"A3_\" ] . astype ( \"category\" ) df [ \"A3_\" ] = df [ \"A3_\" ] . cat . add_categories ( \"OTHER\" ) df [ \"A3_\" ] . fillna ( \"OTHER\" , inplace = True ) df [ \"A3\" ] = df [ \"A3_\" ] . cat . codes return df def _fit_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series , label : pd . Series , ): \"\"\"Function to train M1 classifier (Naive Bayes) Parameters ---------- A1 : pd.Series [description] A2 : pd.Series [description] A3 : pd.Series [description] A4 : pd.Series [description] label : pd.Series [description] \"\"\" # Encode classes to OneHotEncoder representation encoder_A1_A2 = self . _fit_encoder_2S ( A1 , A2 ) self . encoder_A1_A2 = encoder_A1_A2 encoder_A3_A4 = self . _fit_encoder_2S ( A3 , A4 ) self . encoder_A3_A4 = encoder_A3_A4 # M1 m1 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) m1 . fit ( X , label ) self . m1 = m1 def _fit_M2 ( self , B1 : pd . Series , B2 : pd . Series , label : pd . Series ): \"\"\"Function to train M2 classifier (Naive Bayes) Parameters ---------- B1 : pd.Series B2 : pd.Series label : pd.Series \"\"\" # Encode classes to OneHotEncoder representation encoder_B1 = self . _fit_encoder_1S ( B1 ) self . encoder_B1 = encoder_B1 encoder_B2 = self . _fit_encoder_1S ( B2 ) self . encoder_B2 = encoder_B2 # Multinomial Naive Bayes m2 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M2 ( B1 , B2 ) m2 . fit ( X , label ) self . m2 = m2 def _get_X_for_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- np.ndarray \"\"\" A1_enc = self . _encode_series ( self . encoder_A1_A2 , A1 ) A2_enc = self . _encode_series ( self . encoder_A1_A2 , A2 ) A3_enc = self . _encode_series ( self . encoder_A3_A4 , A3 ) A4_enc = self . _encode_series ( self . encoder_A3_A4 , A4 ) X = hstack ([ A1_enc , A2_enc , A3_enc , A4_enc ]) return X def _get_X_for_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- np.ndarray \"\"\" B1_enc = self . _encode_series ( self . encoder_B1 , B1 ) B2_enc = self . _encode_series ( self . encoder_B2 , B2 ) X = hstack ([ B1_enc , B2_enc ]) return X def _predict_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M1 for prediction Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) predictions = self . m1 . predict ( X ) predictions_proba = self . m1 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs def _predict_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M2 for prediction Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M2 ( B1 , B2 ) predictions = self . m2 . predict ( X ) predictions_proba = self . m2 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs def _fit_encoder_2S ( self , S1 : pd . Series , S2 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 2 Series. It concatenates the series and after it fits. Parameters ---------- S1 : pd.Series S2 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) _S2 = _convert_series_to_array ( S2 ) S = np . concatenate ([ _S1 , _S2 ]) encoder = self . _fit_one_hot_encoder ( S ) return encoder def _fit_encoder_1S ( self , S1 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 1 Series. Parameters ---------- S1 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) encoder = self . _fit_one_hot_encoder ( _S1 ) return encoder def _encode_series ( self , encoder : OneHotEncoder , S : pd . Series ) -> np . ndarray : \"\"\"Use the one hot encoder to transform a series. Parameters ---------- encoder : OneHotEncoder S : pd.Series a series to encode (transform) Returns ------- np.ndarray \"\"\" _S = _convert_series_to_array ( S ) S_enc = encoder . transform ( _S ) return S_enc def set_spans ( self , corpus : Iterable [ Doc ], df : pd . DataFrame ): \"\"\" Function to set the results of the algorithm (pd.DataFrame) as spans of the spaCy document. Parameters ---------- corpus : Iterable[Doc] Iterable of spaCy Documents df : pd.DataFrame It should have the columns: [\"DOC_ID\",\"original_token_index\",\"PREDICTED_END_LINE\"] \"\"\" for doc_id , doc in enumerate ( corpus ): spans = [] for token_i , pred in df . loc [ df . DOC_ID == doc_id , [ \"original_token_index\" , \"PREDICTED_END_LINE\" ] ] . values : s = Span ( doc , start = token_i , end = token_i + 1 , label = _get_label ( pred )) spans . append ( s ) doc . spans [ \"new_lines\" ] = spans @staticmethod def _retrieve_lines ( dfg : DataFrameGroupBy ) -> DataFrameGroupBy : \"\"\"Function to give a sentence_id to each token. Parameters ---------- dfg : DataFrameGroupBy Returns ------- DataFrameGroupBy Same DataFrameGroupBy with the column `SENTENCE_ID` \"\"\" sentences_ids = np . arange ( dfg . END_LINE . sum ()) dfg . loc [ dfg . END_LINE , \"SENTENCE_ID\" ] = sentences_ids dfg [ \"SENTENCE_ID\" ] = dfg [ \"SENTENCE_ID\" ] . fillna ( method = \"bfill\" ) return dfg @staticmethod def _create_vocabulary ( x : iterable ) -> dict : \"\"\"Function to create a vocabulary for attributes in the training set. Parameters ---------- x : iterable Returns ------- dict \"\"\" v = {} for i , key in enumerate ( x ): v [ key ] = i return v @staticmethod def _compute_B ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Function to compute B1 and B2 Parameters ---------- df : pd.DataFrame Returns ------- pd.DataFrame \"\"\" data = df . groupby ([ \"DOC_ID\" , \"SENTENCE_ID\" ]) . agg ( l = ( \"LENGTH\" , \"sum\" )) df_t = df . loc [ df . END_LINE , [ \"DOC_ID\" , \"SENTENCE_ID\" ]] . merge ( data , left_on = [ \"DOC_ID\" , \"SENTENCE_ID\" ], right_index = True , how = \"left\" ) stats_doc = df_t . groupby ( \"DOC_ID\" ) . agg ( mu = ( \"l\" , \"mean\" ), sigma = ( \"l\" , \"std\" )) stats_doc [ \"sigma\" ] . replace ( 0.0 , 1.0 , inplace = True ) # Replace the 0 std by unit std, otherwise it breaks the code. stats_doc [ \"cv\" ] = stats_doc [ \"sigma\" ] / stats_doc [ \"mu\" ] df_t = df_t . drop ( columns = [ \"DOC_ID\" , \"SENTENCE_ID\" ]) df2 = df . merge ( df_t , left_index = True , right_index = True , how = \"left\" ) df2 = df2 . merge ( stats_doc , on = [ \"DOC_ID\" ], how = \"left\" ) df2 [ \"l_norm\" ] = ( df2 [ \"l\" ] - df2 [ \"mu\" ]) / df2 [ \"sigma\" ] df2 [ \"cv_bin\" ] = pd . cut ( df2 [ \"cv\" ], bins = 10 ) df2 [ \"B2\" ] = df2 [ \"cv_bin\" ] . cat . codes df2 [ \"l_norm_bin\" ] = pd . cut ( df2 [ \"l_norm\" ], bins = 10 ) df2 [ \"B1\" ] = df2 [ \"l_norm_bin\" ] . cat . codes return df2 @staticmethod def _shift_col ( df : pd . DataFrame , col : str , new_col : str , direction = \"backward\" , fill = None ) -> pd . DataFrame : \"\"\"Shifts a column one position into backward / forward direction. Parameters ---------- df : pd.DataFrame col : str column to shift new_col : str column name to save the results direction : str, optional one of {\"backward\", \"forward\"}, by default \"backward\" fill : [type], optional , by default None Returns ------- pd.DataFrame same df with `new_col` added. \"\"\" df [ new_col ] = fill if direction == \"backward\" : df . loc [ df . index [: - 1 ], new_col ] = df [ col ] . values [ 1 :] different_doc_id = df [ \"DOC_ID\" ] . values [: - 1 ] != df [ \"DOC_ID\" ] . values [ 1 :] different_doc_id = np . append ( different_doc_id , True ) if direction == \"forward\" : df . loc [ df . index [ 1 :], new_col ] = df [ col ] . values [: - 1 ] different_doc_id = df [ \"DOC_ID\" ] . values [ 1 :] != df [ \"DOC_ID\" ] . values [: - 1 ] different_doc_id = np . append ( True , different_doc_id ) df . loc [ different_doc_id , new_col ] = fill return df @staticmethod def _get_attributes ( doc : Doc , i = 0 ): \"\"\"Function to get the attributes of tokens of a spacy doc in a pd.DataFrame format. Parameters ---------- doc : Doc spacy Doc i : int, optional document id, by default 0 Returns ------- pd.DataFrame Returns a dataframe with one line per token. It has the following columns : `[ \"ORTH\", \"LOWER\", \"SHAPE\", \"IS_DIGIT\", \"IS_SPACE\", \"IS_UPPER\", \"IS_PUNCT\", \"LENGTH\", ]` \"\"\" attributes = [ \"ORTH\" , \"LOWER\" , \"SHAPE\" , \"IS_DIGIT\" , \"IS_SPACE\" , \"IS_UPPER\" , \"IS_PUNCT\" , \"LENGTH\" , ] attributes_array = doc . to_array ( attributes ) attributes_df = pd . DataFrame ( attributes_array , columns = attributes ) attributes_df [ \"DOC_ID\" ] = i boolean_attr = [] for a in attributes : if a [: 3 ] == \"IS_\" : boolean_attr . append ( a ) attributes_df [ boolean_attr ] = attributes_df [ boolean_attr ] . astype ( \"boolean\" ) return attributes_df @staticmethod def _get_string ( _id : int , string_store : StringStore ) -> str : \"\"\"Returns the string corresponding to the token_id Parameters ---------- _id : int token id string_store : StringStore spaCy Language String Store Returns ------- str string representation of the token. \"\"\" return string_store [ _id ] @staticmethod def _fit_one_hot_encoder ( X : np . ndarray ) -> OneHotEncoder : \"\"\"Fit a one hot encoder. Parameters ---------- X : np.ndarray of shape (n,1) Returns ------- OneHotEncoder \"\"\" encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) encoder . fit ( X ) return encoder","title":"EndLinesModel"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._preprocess_data","text":"PARAMETER DESCRIPTION corpus Corpus of documents TYPE: Iterable[Doc] RETURNS DESCRIPTION pd.DataFrame Preprocessed data Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def _preprocess_data ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\" Parameters ---------- corpus : Iterable[Doc] Corpus of documents Returns ------- pd.DataFrame Preprocessed data \"\"\" # Extract the vocabulary string_store = self . nlp . vocab . strings # Iterate in the corpus and construct a dataframe train_data_list = [] for i , doc in enumerate ( corpus ): train_data_list . append ( self . _get_attributes ( doc , i )) df = pd . concat ( train_data_list ) df . reset_index ( inplace = True , drop = False ) df . rename ( columns = { \"ORTH\" : \"A1\" , \"index\" : \"original_token_index\" }, inplace = True ) # Retrieve string representation of token_id and shape df [ \"TEXT\" ] = df . A1 . apply ( self . _get_string , string_store = string_store ) df [ \"SHAPE_\" ] = df . SHAPE . apply ( self . _get_string , string_store = string_store ) # Convert new lines as an attribute instead of a row df = self . _convert_line_to_attribute ( df , expr = \" \\n \" , col = \"END_LINE\" ) df = self . _convert_line_to_attribute ( df , expr = \" \\n\\n \" , col = \"BLANK_LINE\" ) df = df . loc [ ~ ( df . END_LINE | df . BLANK_LINE )] df = df . drop ( columns = \"END_LINE\" ) df = df . drop ( columns = \"BLANK_LINE\" ) df . rename ( columns = { \"TEMP_END_LINE\" : \"END_LINE\" , \"TEMP_BLANK_LINE\" : \"BLANK_LINE\" }, inplace = True , ) # Construct A2 by shifting df = self . _shift_col ( df , \"A1\" , \"A2\" , direction = \"backward\" ) # Compute A3 and A4 df = self . _compute_a3 ( df ) df = self . _shift_col ( df , \"A3\" , \"A4\" , direction = \"backward\" ) # SPACE is the class to predict. Set 1 if not an END_LINE df [ \"SPACE\" ] = np . logical_not ( df [ \"END_LINE\" ]) . astype ( \"int\" ) df [[ \"END_LINE\" , \"BLANK_LINE\" ]] = df [[ \"END_LINE\" , \"BLANK_LINE\" ]] . fillna ( True , inplace = False ) # Assign a sentence id to each token df = df . groupby ( \"DOC_ID\" ) . apply ( self . _retrieve_lines ) df [ \"SENTENCE_ID\" ] = df [ \"SENTENCE_ID\" ] . astype ( \"int\" ) # Compute B1 and B2 df = self . _compute_B ( df ) # Drop Tokens without info (last token of doc) df . dropna ( subset = [ \"A1\" , \"A2\" , \"A3\" , \"A4\" ], inplace = True ) # Export the vocabularies to be able to use the model with another corpus voc_a3a4 = self . _create_vocabulary ( df . A3_ . cat . categories ) voc_B2 = self . _create_vocabulary ( df . cv_bin . cat . categories ) voc_B1 = self . _create_vocabulary ( df . l_norm_bin . cat . categories ) vocabulary = { \"A3A4\" : voc_a3a4 , \"B1\" : voc_B1 , \"B2\" : voc_B2 } self . vocabulary = vocabulary return df","title":"_preprocess_data()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.fit_and_predict","text":"Fit the model and predict for the training data PARAMETER DESCRIPTION corpus An iterable of Documents TYPE: Iterable[Doc] RETURNS DESCRIPTION pd.DataFrame one line by end_line prediction Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def fit_and_predict ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\"Fit the model and predict for the training data Parameters ---------- corpus : Iterable[Doc] An iterable of Documents Returns ------- pd.DataFrame one line by end_line prediction \"\"\" # Preprocess data to have a pd DF df = self . _preprocess_data ( corpus ) # Train and predict M1 self . _fit_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , df . SPACE ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] # Force Blank lines to 0 df . loc [ df . BLANK_LINE , \"M1\" ] = 0 # Train and predict M2 df_endlines = df . loc [ df . END_LINE ] self . _fit_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 , label = df_endlines . M1 ) outputs_M2 = self . _predict_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 ) df . loc [ df . END_LINE , \"M2\" ] = outputs_M2 [ \"predictions\" ] df . loc [ df . END_LINE , \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df = df . loc [ df . END_LINE ] df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M2\" , \"M1M2\" ]] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df","title":"fit_and_predict()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.predict","text":"Use the model for inference The df should have the following columns: [\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"] PARAMETER DESCRIPTION df The df should have the following columns: [\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"] TYPE: pd.DataFrame RETURNS DESCRIPTION pd.DataFrame The result is added to the column PREDICTED_END_LINE Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def predict ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Use the model for inference The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Parameters ---------- df : pd.DataFrame The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Returns ------- pd.DataFrame The result is added to the column `PREDICTED_END_LINE` \"\"\" df = self . _convert_raw_data_to_codes ( df ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . _A3 , df . _A4 ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] outputs_M2 = self . _predict_M2 ( B1 = df . _B1 , B2 = df . _B2 ) df [ \"M2\" ] = outputs_M2 [ \"predictions\" ] df [ \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M1M2\" , ], ] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df","title":"predict()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.save","text":"Save a pickle of the model. It could be read by the pipeline later. PARAMETER DESCRIPTION path path to file .pkl, by default base_model.pkl TYPE: str, optional DEFAULT: 'base_model.pkl' Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 213 214 215 216 217 218 219 220 221 222 223 def save ( self , path = \"base_model.pkl\" ): \"\"\"Save a pickle of the model. It could be read by the pipeline later. Parameters ---------- path : str, optional path to file .pkl, by default `base_model.pkl` \"\"\" with open ( path , \"wb\" ) as outp : del self . nlp pickle . dump ( self , outp , pickle . HIGHEST_PROTOCOL )","title":"save()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._convert_A","text":"PARAMETER DESCRIPTION df TYPE: pd.DataFrame col column to translate TYPE: str RETURNS DESCRIPTION pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def _convert_A ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame col : str column to translate Returns ------- pd.DataFrame \"\"\" cat_type_A = CategoricalDtype ( categories = self . vocabulary [ \"A3A4\" ] . keys (), ordered = True ) new_col = \"_\" + col df [ new_col ] = df [ col ] . astype ( cat_type_A ) df [ new_col ] = df [ new_col ] . cat . codes # Ensure that not known values are coded as OTHER df . loc [ ~ df [ col ] . isin ( self . vocabulary [ \"A3A4\" ] . keys ()), new_col ] = self . vocabulary [ \"A3A4\" ][ \"OTHER\" ] return df","title":"_convert_A()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._convert_B","text":"PARAMETER DESCRIPTION df [description] TYPE: pd.DataFrame col column to translate TYPE: str RETURNS DESCRIPTION pd.DataFrame [description] Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def _convert_B ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame [description] col : str column to translate Returns ------- pd.DataFrame [description] \"\"\" # Translate B1 index_B = pd . IntervalIndex ( list ( self . vocabulary [ col ] . keys ())) new_col = \"_\" + col df [ new_col ] = pd . cut ( df [ col ], index_B ) df [ new_col ] = df [ new_col ] . cat . codes df . loc [ df [ col ] >= index_B . right . max (), new_col ] = max ( self . vocabulary [ col ] . values () ) df . loc [ df [ col ] <= index_B . left . min (), new_col ] = min ( self . vocabulary [ col ] . values () ) return df","title":"_convert_B()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._convert_raw_data_to_codes","text":"Function to translate data as extracted from spacy to the model codes. A1 and A2 are not translated cause are supposed to be already in good encoding. PARAMETER DESCRIPTION df It should have columns ['A3','A4','B1','B2'] TYPE: pd.DataFrame RETURNS DESCRIPTION pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def _convert_raw_data_to_codes ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Function to translate data as extracted from spacy to the model codes. `A1` and `A2` are not translated cause are supposed to be already in good encoding. Parameters ---------- df : pd.DataFrame It should have columns `['A3','A4','B1','B2']` Returns ------- pd.DataFrame \"\"\" df = self . _convert_A ( df , \"A3\" ) df = self . _convert_A ( df , \"A4\" ) df = self . _convert_B ( df , \"B1\" ) df = self . _convert_B ( df , \"B2\" ) return df","title":"_convert_raw_data_to_codes()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._convert_line_to_attribute","text":"Function to convert a line into an attribute (column) of the previous row. Particularly we use it to identify \"\\n\" and \"\\n\\n\" that are considered tokens, express this information as an attribute of the previous token. PARAMETER DESCRIPTION df TYPE: pd.DataFrame expr pattern to search in the text. Ex.: \"\\n\" TYPE: str col name of the new column TYPE: str RETURNS DESCRIPTION pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 def _convert_line_to_attribute ( self , df : pd . DataFrame , expr : str , col : str ) -> pd . DataFrame : \"\"\" Function to convert a line into an attribute (column) of the previous row. Particularly we use it to identify \"\\\\n\" and \"\\\\n\\\\n\" that are considered tokens, express this information as an attribute of the previous token. Parameters ---------- df : pd.DataFrame expr : str pattern to search in the text. Ex.: \"\\\\n\" col : str name of the new column Returns ------- pd.DataFrame \"\"\" idx = df . TEXT . str . contains ( expr ) df . loc [ idx , col ] = True df [ col ] = df [ col ] . fillna ( False ) df = self . _shift_col ( df , col , \"TEMP_\" + col , direction = \"backward\" ) return df","title":"_convert_line_to_attribute()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._compute_a3","text":"A3 (A4 respectively): typographic form of left word (or right) : All in capital letter It starts with a capital letter Starts by lowercase It's a number Strong punctuation Soft punctuation A number followed or preced by a punctuation (it's the case of enumerations) PARAMETER DESCRIPTION df TYPE: pd . DataFrame RETURNS DESCRIPTION df Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def _compute_a3 ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" A3 (A4 respectively): typographic form of left word (or right) : - All in capital letter - It starts with a capital letter - Starts by lowercase - It's a number - Strong punctuation - Soft punctuation - A number followed or preced by a punctuation (it's the case of enumerations) Parameters ---------- df: pd.DataFrame Returns ------- df: pd.DataFrame with the columns `A3` and `A3_` \"\"\" df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_+1\" , direction = \"backward\" , fill = False ) df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_-1\" , direction = \"forward\" , fill = False ) CONDITION1 = df . IS_UPPER CONDITION2 = df . SHAPE_ . str . startswith ( \"Xx\" , na = False ) CONDITION3 = df . SHAPE_ . str . startswith ( \"x\" , na = False ) CONDITION4 = df . IS_DIGIT STRONG_PUNCT = [ \".\" , \";\" , \"..\" , \"...\" ] CONDITION5 = ( df . IS_PUNCT ) & ( df . TEXT . isin ( STRONG_PUNCT )) CONDITION6 = ( df . IS_PUNCT ) & ( ~ df . TEXT . isin ( STRONG_PUNCT )) CONDITION7 = ( df . IS_DIGIT ) & ( df [ \"IS_PUNCT_+1\" ] | df [ \"IS_PUNCT_-1\" ]) # discuss df [ \"A3_\" ] = None df . loc [ CONDITION1 , \"A3_\" ] = \"UPPER\" df . loc [ CONDITION2 , \"A3_\" ] = \"S_UPPER\" df . loc [ CONDITION3 , \"A3_\" ] = \"LOWER\" df . loc [ CONDITION4 , \"A3_\" ] = \"DIGIT\" df . loc [ CONDITION5 , \"A3_\" ] = \"STRONG_PUNCT\" df . loc [ CONDITION6 , \"A3_\" ] = \"SOFT_PUNCT\" df . loc [ CONDITION7 , \"A3_\" ] = \"ENUMERATION\" df = df . drop ( columns = [ \"IS_PUNCT_+1\" , \"IS_PUNCT_-1\" ]) df [ \"A3_\" ] = df [ \"A3_\" ] . astype ( \"category\" ) df [ \"A3_\" ] = df [ \"A3_\" ] . cat . add_categories ( \"OTHER\" ) df [ \"A3_\" ] . fillna ( \"OTHER\" , inplace = True ) df [ \"A3\" ] = df [ \"A3_\" ] . cat . codes return df","title":"_compute_a3()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._fit_M1","text":"Function to train M1 classifier (Naive Bayes) PARAMETER DESCRIPTION A1 [description] TYPE: pd.Series A2 [description] TYPE: pd.Series A3 [description] TYPE: pd.Series A4 [description] TYPE: pd.Series label [description] TYPE: pd.Series Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 def _fit_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series , label : pd . Series , ): \"\"\"Function to train M1 classifier (Naive Bayes) Parameters ---------- A1 : pd.Series [description] A2 : pd.Series [description] A3 : pd.Series [description] A4 : pd.Series [description] label : pd.Series [description] \"\"\" # Encode classes to OneHotEncoder representation encoder_A1_A2 = self . _fit_encoder_2S ( A1 , A2 ) self . encoder_A1_A2 = encoder_A1_A2 encoder_A3_A4 = self . _fit_encoder_2S ( A3 , A4 ) self . encoder_A3_A4 = encoder_A3_A4 # M1 m1 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) m1 . fit ( X , label ) self . m1 = m1","title":"_fit_M1()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._fit_M2","text":"Function to train M2 classifier (Naive Bayes) PARAMETER DESCRIPTION B1 TYPE: pd.Series B2 TYPE: pd.Series label TYPE: pd.Series Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 def _fit_M2 ( self , B1 : pd . Series , B2 : pd . Series , label : pd . Series ): \"\"\"Function to train M2 classifier (Naive Bayes) Parameters ---------- B1 : pd.Series B2 : pd.Series label : pd.Series \"\"\" # Encode classes to OneHotEncoder representation encoder_B1 = self . _fit_encoder_1S ( B1 ) self . encoder_B1 = encoder_B1 encoder_B2 = self . _fit_encoder_1S ( B2 ) self . encoder_B2 = encoder_B2 # Multinomial Naive Bayes m2 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M2 ( B1 , B2 ) m2 . fit ( X , label ) self . m2 = m2","title":"_fit_M2()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._get_X_for_M1","text":"Get X matrix for classifier PARAMETER DESCRIPTION A1 TYPE: pd.Series A2 TYPE: pd.Series A3 TYPE: pd.Series A4 TYPE: pd.Series RETURNS DESCRIPTION np.ndarray Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def _get_X_for_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- np.ndarray \"\"\" A1_enc = self . _encode_series ( self . encoder_A1_A2 , A1 ) A2_enc = self . _encode_series ( self . encoder_A1_A2 , A2 ) A3_enc = self . _encode_series ( self . encoder_A3_A4 , A3 ) A4_enc = self . _encode_series ( self . encoder_A3_A4 , A4 ) X = hstack ([ A1_enc , A2_enc , A3_enc , A4_enc ]) return X","title":"_get_X_for_M1()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._get_X_for_M2","text":"Get X matrix for classifier PARAMETER DESCRIPTION B1 TYPE: pd.Series B2 TYPE: pd.Series RETURNS DESCRIPTION np.ndarray Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def _get_X_for_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- np.ndarray \"\"\" B1_enc = self . _encode_series ( self . encoder_B1 , B1 ) B2_enc = self . _encode_series ( self . encoder_B2 , B2 ) X = hstack ([ B1_enc , B2_enc ]) return X","title":"_get_X_for_M2()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._predict_M1","text":"Use M1 for prediction PARAMETER DESCRIPTION A1 TYPE: pd.Series A2 TYPE: pd.Series A3 TYPE: pd.Series A4 TYPE: pd.Series RETURNS DESCRIPTION Dict[str, Any] Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 def _predict_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M1 for prediction Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) predictions = self . m1 . predict ( X ) predictions_proba = self . m1 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs","title":"_predict_M1()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._predict_M2","text":"Use M2 for prediction PARAMETER DESCRIPTION B1 TYPE: pd.Series B2 TYPE: pd.Series RETURNS DESCRIPTION Dict[str, Any] Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def _predict_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M2 for prediction Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M2 ( B1 , B2 ) predictions = self . m2 . predict ( X ) predictions_proba = self . m2 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs","title":"_predict_M2()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._fit_encoder_2S","text":"Fit a one hot encoder with 2 Series. It concatenates the series and after it fits. PARAMETER DESCRIPTION S1 TYPE: pd.Series S2 TYPE: pd.Series RETURNS DESCRIPTION OneHotEncoder Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 def _fit_encoder_2S ( self , S1 : pd . Series , S2 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 2 Series. It concatenates the series and after it fits. Parameters ---------- S1 : pd.Series S2 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) _S2 = _convert_series_to_array ( S2 ) S = np . concatenate ([ _S1 , _S2 ]) encoder = self . _fit_one_hot_encoder ( S ) return encoder","title":"_fit_encoder_2S()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._fit_encoder_1S","text":"Fit a one hot encoder with 1 Series. PARAMETER DESCRIPTION S1 TYPE: pd.Series RETURNS DESCRIPTION OneHotEncoder Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 540 541 542 543 544 545 546 547 548 549 550 551 552 553 def _fit_encoder_1S ( self , S1 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 1 Series. Parameters ---------- S1 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) encoder = self . _fit_one_hot_encoder ( _S1 ) return encoder","title":"_fit_encoder_1S()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._encode_series","text":"Use the one hot encoder to transform a series. PARAMETER DESCRIPTION encoder TYPE: OneHotEncoder S a series to encode (transform) TYPE: pd.Series RETURNS DESCRIPTION np.ndarray Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 def _encode_series ( self , encoder : OneHotEncoder , S : pd . Series ) -> np . ndarray : \"\"\"Use the one hot encoder to transform a series. Parameters ---------- encoder : OneHotEncoder S : pd.Series a series to encode (transform) Returns ------- np.ndarray \"\"\" _S = _convert_series_to_array ( S ) S_enc = encoder . transform ( _S ) return S_enc","title":"_encode_series()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.set_spans","text":"Function to set the results of the algorithm (pd.DataFrame) as spans of the spaCy document. PARAMETER DESCRIPTION corpus Iterable of spaCy Documents TYPE: Iterable[Doc] df It should have the columns: [\"DOC_ID\",\"original_token_index\",\"PREDICTED_END_LINE\"] TYPE: pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 def set_spans ( self , corpus : Iterable [ Doc ], df : pd . DataFrame ): \"\"\" Function to set the results of the algorithm (pd.DataFrame) as spans of the spaCy document. Parameters ---------- corpus : Iterable[Doc] Iterable of spaCy Documents df : pd.DataFrame It should have the columns: [\"DOC_ID\",\"original_token_index\",\"PREDICTED_END_LINE\"] \"\"\" for doc_id , doc in enumerate ( corpus ): spans = [] for token_i , pred in df . loc [ df . DOC_ID == doc_id , [ \"original_token_index\" , \"PREDICTED_END_LINE\" ] ] . values : s = Span ( doc , start = token_i , end = token_i + 1 , label = _get_label ( pred )) spans . append ( s ) doc . spans [ \"new_lines\" ] = spans","title":"set_spans()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._retrieve_lines","text":"Function to give a sentence_id to each token. PARAMETER DESCRIPTION dfg TYPE: DataFrameGroupBy RETURNS DESCRIPTION DataFrameGroupBy Same DataFrameGroupBy with the column SENTENCE_ID Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 @staticmethod def _retrieve_lines ( dfg : DataFrameGroupBy ) -> DataFrameGroupBy : \"\"\"Function to give a sentence_id to each token. Parameters ---------- dfg : DataFrameGroupBy Returns ------- DataFrameGroupBy Same DataFrameGroupBy with the column `SENTENCE_ID` \"\"\" sentences_ids = np . arange ( dfg . END_LINE . sum ()) dfg . loc [ dfg . END_LINE , \"SENTENCE_ID\" ] = sentences_ids dfg [ \"SENTENCE_ID\" ] = dfg [ \"SENTENCE_ID\" ] . fillna ( method = \"bfill\" ) return dfg","title":"_retrieve_lines()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._create_vocabulary","text":"Function to create a vocabulary for attributes in the training set. PARAMETER DESCRIPTION x TYPE: iterable RETURNS DESCRIPTION dict Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 @staticmethod def _create_vocabulary ( x : iterable ) -> dict : \"\"\"Function to create a vocabulary for attributes in the training set. Parameters ---------- x : iterable Returns ------- dict \"\"\" v = {} for i , key in enumerate ( x ): v [ key ] = i return v","title":"_create_vocabulary()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._compute_B","text":"Function to compute B1 and B2 PARAMETER DESCRIPTION df TYPE: pd.DataFrame RETURNS DESCRIPTION pd.DataFrame Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 @staticmethod def _compute_B ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Function to compute B1 and B2 Parameters ---------- df : pd.DataFrame Returns ------- pd.DataFrame \"\"\" data = df . groupby ([ \"DOC_ID\" , \"SENTENCE_ID\" ]) . agg ( l = ( \"LENGTH\" , \"sum\" )) df_t = df . loc [ df . END_LINE , [ \"DOC_ID\" , \"SENTENCE_ID\" ]] . merge ( data , left_on = [ \"DOC_ID\" , \"SENTENCE_ID\" ], right_index = True , how = \"left\" ) stats_doc = df_t . groupby ( \"DOC_ID\" ) . agg ( mu = ( \"l\" , \"mean\" ), sigma = ( \"l\" , \"std\" )) stats_doc [ \"sigma\" ] . replace ( 0.0 , 1.0 , inplace = True ) # Replace the 0 std by unit std, otherwise it breaks the code. stats_doc [ \"cv\" ] = stats_doc [ \"sigma\" ] / stats_doc [ \"mu\" ] df_t = df_t . drop ( columns = [ \"DOC_ID\" , \"SENTENCE_ID\" ]) df2 = df . merge ( df_t , left_index = True , right_index = True , how = \"left\" ) df2 = df2 . merge ( stats_doc , on = [ \"DOC_ID\" ], how = \"left\" ) df2 [ \"l_norm\" ] = ( df2 [ \"l\" ] - df2 [ \"mu\" ]) / df2 [ \"sigma\" ] df2 [ \"cv_bin\" ] = pd . cut ( df2 [ \"cv\" ], bins = 10 ) df2 [ \"B2\" ] = df2 [ \"cv_bin\" ] . cat . codes df2 [ \"l_norm_bin\" ] = pd . cut ( df2 [ \"l_norm\" ], bins = 10 ) df2 [ \"B1\" ] = df2 [ \"l_norm_bin\" ] . cat . codes return df2","title":"_compute_B()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._shift_col","text":"Shifts a column one position into backward / forward direction. PARAMETER DESCRIPTION df TYPE: pd.DataFrame col column to shift TYPE: str new_col column name to save the results TYPE: str direction one of {\"backward\", \"forward\"}, by default \"backward\" TYPE: str, optional DEFAULT: 'backward' fill , by default None TYPE: [type], optional DEFAULT: None RETURNS DESCRIPTION pd.DataFrame same df with new_col added. Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 @staticmethod def _shift_col ( df : pd . DataFrame , col : str , new_col : str , direction = \"backward\" , fill = None ) -> pd . DataFrame : \"\"\"Shifts a column one position into backward / forward direction. Parameters ---------- df : pd.DataFrame col : str column to shift new_col : str column name to save the results direction : str, optional one of {\"backward\", \"forward\"}, by default \"backward\" fill : [type], optional , by default None Returns ------- pd.DataFrame same df with `new_col` added. \"\"\" df [ new_col ] = fill if direction == \"backward\" : df . loc [ df . index [: - 1 ], new_col ] = df [ col ] . values [ 1 :] different_doc_id = df [ \"DOC_ID\" ] . values [: - 1 ] != df [ \"DOC_ID\" ] . values [ 1 :] different_doc_id = np . append ( different_doc_id , True ) if direction == \"forward\" : df . loc [ df . index [ 1 :], new_col ] = df [ col ] . values [: - 1 ] different_doc_id = df [ \"DOC_ID\" ] . values [ 1 :] != df [ \"DOC_ID\" ] . values [: - 1 ] different_doc_id = np . append ( True , different_doc_id ) df . loc [ different_doc_id , new_col ] = fill return df","title":"_shift_col()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._get_attributes","text":"Function to get the attributes of tokens of a spacy doc in a pd.DataFrame format. PARAMETER DESCRIPTION doc spacy Doc TYPE: Doc i document id, by default 0 TYPE: int, optional DEFAULT: 0 RETURNS DESCRIPTION pd.DataFrame Returns a dataframe with one line per token. It has the following columns : [ \"ORTH\", \"LOWER\", \"SHAPE\", \"IS_DIGIT\", \"IS_SPACE\", \"IS_UPPER\", \"IS_PUNCT\", \"LENGTH\", ] Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 @staticmethod def _get_attributes ( doc : Doc , i = 0 ): \"\"\"Function to get the attributes of tokens of a spacy doc in a pd.DataFrame format. Parameters ---------- doc : Doc spacy Doc i : int, optional document id, by default 0 Returns ------- pd.DataFrame Returns a dataframe with one line per token. It has the following columns : `[ \"ORTH\", \"LOWER\", \"SHAPE\", \"IS_DIGIT\", \"IS_SPACE\", \"IS_UPPER\", \"IS_PUNCT\", \"LENGTH\", ]` \"\"\" attributes = [ \"ORTH\" , \"LOWER\" , \"SHAPE\" , \"IS_DIGIT\" , \"IS_SPACE\" , \"IS_UPPER\" , \"IS_PUNCT\" , \"LENGTH\" , ] attributes_array = doc . to_array ( attributes ) attributes_df = pd . DataFrame ( attributes_array , columns = attributes ) attributes_df [ \"DOC_ID\" ] = i boolean_attr = [] for a in attributes : if a [: 3 ] == \"IS_\" : boolean_attr . append ( a ) attributes_df [ boolean_attr ] = attributes_df [ boolean_attr ] . astype ( \"boolean\" ) return attributes_df","title":"_get_attributes()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._get_string","text":"Returns the string corresponding to the token_id PARAMETER DESCRIPTION _id token id TYPE: int string_store spaCy Language String Store TYPE: StringStore RETURNS DESCRIPTION str string representation of the token. Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 @staticmethod def _get_string ( _id : int , string_store : StringStore ) -> str : \"\"\"Returns the string corresponding to the token_id Parameters ---------- _id : int token id string_store : StringStore spaCy Language String Store Returns ------- str string representation of the token. \"\"\" return string_store [ _id ]","title":"_get_string()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel._fit_one_hot_encoder","text":"Fit a one hot encoder. PARAMETER DESCRIPTION X of shape (n,1) TYPE: np.ndarray RETURNS DESCRIPTION OneHotEncoder Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 @staticmethod def _fit_one_hot_encoder ( X : np . ndarray ) -> OneHotEncoder : \"\"\"Fit a one hot encoder. Parameters ---------- X : np.ndarray of shape (n,1) Returns ------- OneHotEncoder \"\"\" encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) encoder . fit ( X ) return encoder","title":"_fit_one_hot_encoder()"},{"location":"reference/pipelines/core/endlines/factory/","text":"edsnlp.pipelines.core.endlines.factory","title":"factory"},{"location":"reference/pipelines/core/endlines/factory/#edsnlppipelinescoreendlinesfactory","text":"","title":"edsnlp.pipelines.core.endlines.factory"},{"location":"reference/pipelines/core/endlines/functional/","text":"edsnlp.pipelines.core.endlines.functional _get_label ( prediction ) Returns the label for the prediction PREDICTED_END_LINE PARAMETER DESCRIPTION prediction value of PREDICTED_END_LINE TYPE: bool RETURNS DESCRIPTION str Label for PREDICTED_END_LINE Source code in edsnlp/pipelines/core/endlines/functional.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def _get_label ( prediction : bool ) -> str : \"\"\"Returns the label for the prediction `PREDICTED_END_LINE` Parameters ---------- prediction : bool value of `PREDICTED_END_LINE` Returns ------- str Label for `PREDICTED_END_LINE` \"\"\" if prediction : return \"end_line\" else : return \"space\" build_path ( file , relative_path ) Function to build an absolut path. PARAMETER DESCRIPTION file relative_path relative path from the main file to the desired output RETURNS DESCRIPTION path Source code in edsnlp/pipelines/core/endlines/functional.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def build_path ( file , relative_path ): \"\"\" Function to build an absolut path. Parameters ---------- file: main file from where we are calling. It could be __file__ relative_path: str, relative path from the main file to the desired output Returns ------- path: absolute path \"\"\" dir_path = get_dir_path ( file ) path = os . path . abspath ( os . path . join ( dir_path , relative_path )) return path _convert_series_to_array ( s ) Converts pandas series of n elements to an array of shape (n,1). PARAMETER DESCRIPTION s TYPE: pd.Series RETURNS DESCRIPTION np.ndarray Source code in edsnlp/pipelines/core/endlines/functional.py 50 51 52 53 54 55 56 57 58 59 60 61 62 def _convert_series_to_array ( s : pd . Series ) -> np . ndarray : \"\"\"Converts pandas series of n elements to an array of shape (n,1). Parameters ---------- s : pd.Series Returns ------- np.ndarray \"\"\" X = s . to_numpy () . reshape ( - 1 , 1 ) . astype ( \"O\" ) # .astype(np.int64) return X","title":"functional"},{"location":"reference/pipelines/core/endlines/functional/#edsnlppipelinescoreendlinesfunctional","text":"","title":"edsnlp.pipelines.core.endlines.functional"},{"location":"reference/pipelines/core/endlines/functional/#edsnlp.pipelines.core.endlines.functional._get_label","text":"Returns the label for the prediction PREDICTED_END_LINE PARAMETER DESCRIPTION prediction value of PREDICTED_END_LINE TYPE: bool RETURNS DESCRIPTION str Label for PREDICTED_END_LINE Source code in edsnlp/pipelines/core/endlines/functional.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def _get_label ( prediction : bool ) -> str : \"\"\"Returns the label for the prediction `PREDICTED_END_LINE` Parameters ---------- prediction : bool value of `PREDICTED_END_LINE` Returns ------- str Label for `PREDICTED_END_LINE` \"\"\" if prediction : return \"end_line\" else : return \"space\"","title":"_get_label()"},{"location":"reference/pipelines/core/endlines/functional/#edsnlp.pipelines.core.endlines.functional.build_path","text":"Function to build an absolut path. PARAMETER DESCRIPTION file relative_path relative path from the main file to the desired output RETURNS DESCRIPTION path Source code in edsnlp/pipelines/core/endlines/functional.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def build_path ( file , relative_path ): \"\"\" Function to build an absolut path. Parameters ---------- file: main file from where we are calling. It could be __file__ relative_path: str, relative path from the main file to the desired output Returns ------- path: absolute path \"\"\" dir_path = get_dir_path ( file ) path = os . path . abspath ( os . path . join ( dir_path , relative_path )) return path","title":"build_path()"},{"location":"reference/pipelines/core/endlines/functional/#edsnlp.pipelines.core.endlines.functional._convert_series_to_array","text":"Converts pandas series of n elements to an array of shape (n,1). PARAMETER DESCRIPTION s TYPE: pd.Series RETURNS DESCRIPTION np.ndarray Source code in edsnlp/pipelines/core/endlines/functional.py 50 51 52 53 54 55 56 57 58 59 60 61 62 def _convert_series_to_array ( s : pd . Series ) -> np . ndarray : \"\"\"Converts pandas series of n elements to an array of shape (n,1). Parameters ---------- s : pd.Series Returns ------- np.ndarray \"\"\" X = s . to_numpy () . reshape ( - 1 , 1 ) . astype ( \"O\" ) # .astype(np.int64) return X","title":"_convert_series_to_array()"},{"location":"reference/pipelines/core/matcher/","text":"edsnlp.pipelines.core.matcher matcher GenericMatcher Bases: BaseComponent Provides a generic matcher component. PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language terms A dictionary of terms. TYPE: Optional[Patterns] regex A dictionary of regular expressions. TYPE: Optional[Patterns] attr The default attribute to use for matching. Can be overiden using the terms and regex configurations. TYPE: str filter_matches Whether to filter out matches. TYPE: bool on_ents_only Whether to to look for matches around pre-extracted entities only. TYPE: bool ignore_excluded Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). TYPE: bool Source code in edsnlp/pipelines/core/matcher/matcher.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class GenericMatcher ( BaseComponent ): \"\"\" Provides a generic matcher component. Parameters ---------- nlp : Language The spaCy object. terms : Optional[Patterns] A dictionary of terms. regex : Optional[Patterns] A dictionary of regular expressions. attr : str The default attribute to use for matching. Can be overiden using the `terms` and `regex` configurations. filter_matches : bool Whether to filter out matches. on_ents_only : bool Whether to to look for matches around pre-extracted entities only. ignore_excluded : bool Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). \"\"\" def __init__ ( self , nlp : Language , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , ): self . nlp = nlp self . attr = attr self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions () def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc process ( doc ) Find matching spans in doc. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION spans List of Spans returned by the matchers. Source code in edsnlp/pipelines/core/matcher/matcher.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans __call__ ( doc ) Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/matcher/matcher.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"`edsnlp.pipelines.core.matcher`"},{"location":"reference/pipelines/core/matcher/#edsnlppipelinescorematcher","text":"","title":"edsnlp.pipelines.core.matcher"},{"location":"reference/pipelines/core/matcher/#edsnlp.pipelines.core.matcher.matcher","text":"","title":"matcher"},{"location":"reference/pipelines/core/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher","text":"Bases: BaseComponent Provides a generic matcher component. PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language terms A dictionary of terms. TYPE: Optional[Patterns] regex A dictionary of regular expressions. TYPE: Optional[Patterns] attr The default attribute to use for matching. Can be overiden using the terms and regex configurations. TYPE: str filter_matches Whether to filter out matches. TYPE: bool on_ents_only Whether to to look for matches around pre-extracted entities only. TYPE: bool ignore_excluded Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). TYPE: bool Source code in edsnlp/pipelines/core/matcher/matcher.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class GenericMatcher ( BaseComponent ): \"\"\" Provides a generic matcher component. Parameters ---------- nlp : Language The spaCy object. terms : Optional[Patterns] A dictionary of terms. regex : Optional[Patterns] A dictionary of regular expressions. attr : str The default attribute to use for matching. Can be overiden using the `terms` and `regex` configurations. filter_matches : bool Whether to filter out matches. on_ents_only : bool Whether to to look for matches around pre-extracted entities only. ignore_excluded : bool Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). \"\"\" def __init__ ( self , nlp : Language , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , ): self . nlp = nlp self . attr = attr self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions () def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"GenericMatcher"},{"location":"reference/pipelines/core/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.process","text":"Find matching spans in doc. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION spans List of Spans returned by the matchers. Source code in edsnlp/pipelines/core/matcher/matcher.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans","title":"process()"},{"location":"reference/pipelines/core/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.__call__","text":"Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/matcher/matcher.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/matcher/factory/","text":"edsnlp.pipelines.core.matcher.factory","title":"factory"},{"location":"reference/pipelines/core/matcher/factory/#edsnlppipelinescorematcherfactory","text":"","title":"edsnlp.pipelines.core.matcher.factory"},{"location":"reference/pipelines/core/matcher/matcher/","text":"edsnlp.pipelines.core.matcher.matcher GenericMatcher Bases: BaseComponent Provides a generic matcher component. PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language terms A dictionary of terms. TYPE: Optional[Patterns] regex A dictionary of regular expressions. TYPE: Optional[Patterns] attr The default attribute to use for matching. Can be overiden using the terms and regex configurations. TYPE: str filter_matches Whether to filter out matches. TYPE: bool on_ents_only Whether to to look for matches around pre-extracted entities only. TYPE: bool ignore_excluded Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). TYPE: bool Source code in edsnlp/pipelines/core/matcher/matcher.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class GenericMatcher ( BaseComponent ): \"\"\" Provides a generic matcher component. Parameters ---------- nlp : Language The spaCy object. terms : Optional[Patterns] A dictionary of terms. regex : Optional[Patterns] A dictionary of regular expressions. attr : str The default attribute to use for matching. Can be overiden using the `terms` and `regex` configurations. filter_matches : bool Whether to filter out matches. on_ents_only : bool Whether to to look for matches around pre-extracted entities only. ignore_excluded : bool Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). \"\"\" def __init__ ( self , nlp : Language , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , ): self . nlp = nlp self . attr = attr self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions () def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc process ( doc ) Find matching spans in doc. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION spans List of Spans returned by the matchers. Source code in edsnlp/pipelines/core/matcher/matcher.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans __call__ ( doc ) Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/matcher/matcher.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"matcher"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlppipelinescorematchermatcher","text":"","title":"edsnlp.pipelines.core.matcher.matcher"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher","text":"Bases: BaseComponent Provides a generic matcher component. PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language terms A dictionary of terms. TYPE: Optional[Patterns] regex A dictionary of regular expressions. TYPE: Optional[Patterns] attr The default attribute to use for matching. Can be overiden using the terms and regex configurations. TYPE: str filter_matches Whether to filter out matches. TYPE: bool on_ents_only Whether to to look for matches around pre-extracted entities only. TYPE: bool ignore_excluded Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). TYPE: bool Source code in edsnlp/pipelines/core/matcher/matcher.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class GenericMatcher ( BaseComponent ): \"\"\" Provides a generic matcher component. Parameters ---------- nlp : Language The spaCy object. terms : Optional[Patterns] A dictionary of terms. regex : Optional[Patterns] A dictionary of regular expressions. attr : str The default attribute to use for matching. Can be overiden using the `terms` and `regex` configurations. filter_matches : bool Whether to filter out matches. on_ents_only : bool Whether to to look for matches around pre-extracted entities only. ignore_excluded : bool Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). \"\"\" def __init__ ( self , nlp : Language , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , ): self . nlp = nlp self . attr = attr self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions () def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"GenericMatcher"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.process","text":"Find matching spans in doc. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION spans List of Spans returned by the matchers. Source code in edsnlp/pipelines/core/matcher/matcher.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans","title":"process()"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.__call__","text":"Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/matcher/matcher.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/normalizer/","text":"edsnlp.pipelines.core.normalizer","title":"`edsnlp.pipelines.core.normalizer`"},{"location":"reference/pipelines/core/normalizer/#edsnlppipelinescorenormalizer","text":"","title":"edsnlp.pipelines.core.normalizer"},{"location":"reference/pipelines/core/normalizer/factory/","text":"edsnlp.pipelines.core.normalizer.factory","title":"factory"},{"location":"reference/pipelines/core/normalizer/factory/#edsnlppipelinescorenormalizerfactory","text":"","title":"edsnlp.pipelines.core.normalizer.factory"},{"location":"reference/pipelines/core/normalizer/normalizer/","text":"edsnlp.pipelines.core.normalizer.normalizer Normalizer Bases: object Normalisation pipeline. Modifies the NORM attribute, acting on four dimensions : lowercase : using the default NORM accents : deterministic and fixed-length normalisation of accents. quotes : deterministic and fixed-length normalisation of quotation marks. pollution : removal of pollutions. PARAMETER DESCRIPTION lowercase Whether to remove case. TYPE: bool accents Optional Accents object. TYPE: Optional[Accents] quotes Optional Quotes object. TYPE: Optional[Quotes] pollution Optional Pollution object. TYPE: Optional[Pollution] Source code in edsnlp/pipelines/core/normalizer/normalizer.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class Normalizer ( object ): \"\"\" Normalisation pipeline. Modifies the `NORM` attribute, acting on four dimensions : - `lowercase`: using the default `NORM` - `accents`: deterministic and fixed-length normalisation of accents. - `quotes`: deterministic and fixed-length normalisation of quotation marks. - `pollution`: removal of pollutions. Parameters ---------- lowercase : bool Whether to remove case. accents : Optional[Accents] Optional `Accents` object. quotes : Optional[Quotes] Optional `Quotes` object. pollution : Optional[Pollution] Optional `Pollution` object. \"\"\" def __init__ ( self , lowercase : bool , accents : Optional [ Accents ], quotes : Optional [ Quotes ], pollution : Optional [ Pollution ], ): self . lowercase = lowercase self . accents = accents self . quotes = quotes self . pollution = pollution def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Apply the normalisation pipeline, one component at a time. Parameters ---------- doc : Doc spaCy `Doc` object Returns ------- Doc Doc object with `NORM` attribute modified \"\"\" if not self . lowercase : remove_lowercase ( doc ) if self . accents is not None : self . accents ( doc ) if self . quotes is not None : self . quotes ( doc ) if self . pollution is not None : self . pollution ( doc ) return doc __call__ ( doc ) Apply the normalisation pipeline, one component at a time. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION Doc Doc object with NORM attribute modified Source code in edsnlp/pipelines/core/normalizer/normalizer.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Apply the normalisation pipeline, one component at a time. Parameters ---------- doc : Doc spaCy `Doc` object Returns ------- Doc Doc object with `NORM` attribute modified \"\"\" if not self . lowercase : remove_lowercase ( doc ) if self . accents is not None : self . accents ( doc ) if self . quotes is not None : self . quotes ( doc ) if self . pollution is not None : self . pollution ( doc ) return doc","title":"normalizer"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlppipelinescorenormalizernormalizer","text":"","title":"edsnlp.pipelines.core.normalizer.normalizer"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlp.pipelines.core.normalizer.normalizer.Normalizer","text":"Bases: object Normalisation pipeline. Modifies the NORM attribute, acting on four dimensions : lowercase : using the default NORM accents : deterministic and fixed-length normalisation of accents. quotes : deterministic and fixed-length normalisation of quotation marks. pollution : removal of pollutions. PARAMETER DESCRIPTION lowercase Whether to remove case. TYPE: bool accents Optional Accents object. TYPE: Optional[Accents] quotes Optional Quotes object. TYPE: Optional[Quotes] pollution Optional Pollution object. TYPE: Optional[Pollution] Source code in edsnlp/pipelines/core/normalizer/normalizer.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class Normalizer ( object ): \"\"\" Normalisation pipeline. Modifies the `NORM` attribute, acting on four dimensions : - `lowercase`: using the default `NORM` - `accents`: deterministic and fixed-length normalisation of accents. - `quotes`: deterministic and fixed-length normalisation of quotation marks. - `pollution`: removal of pollutions. Parameters ---------- lowercase : bool Whether to remove case. accents : Optional[Accents] Optional `Accents` object. quotes : Optional[Quotes] Optional `Quotes` object. pollution : Optional[Pollution] Optional `Pollution` object. \"\"\" def __init__ ( self , lowercase : bool , accents : Optional [ Accents ], quotes : Optional [ Quotes ], pollution : Optional [ Pollution ], ): self . lowercase = lowercase self . accents = accents self . quotes = quotes self . pollution = pollution def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Apply the normalisation pipeline, one component at a time. Parameters ---------- doc : Doc spaCy `Doc` object Returns ------- Doc Doc object with `NORM` attribute modified \"\"\" if not self . lowercase : remove_lowercase ( doc ) if self . accents is not None : self . accents ( doc ) if self . quotes is not None : self . quotes ( doc ) if self . pollution is not None : self . pollution ( doc ) return doc","title":"Normalizer"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlp.pipelines.core.normalizer.normalizer.Normalizer.__call__","text":"Apply the normalisation pipeline, one component at a time. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION Doc Doc object with NORM attribute modified Source code in edsnlp/pipelines/core/normalizer/normalizer.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Apply the normalisation pipeline, one component at a time. Parameters ---------- doc : Doc spaCy `Doc` object Returns ------- Doc Doc object with `NORM` attribute modified \"\"\" if not self . lowercase : remove_lowercase ( doc ) if self . accents is not None : self . accents ( doc ) if self . quotes is not None : self . quotes ( doc ) if self . pollution is not None : self . pollution ( doc ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/normalizer/utils/","text":"edsnlp.pipelines.core.normalizer.utils replace ( text , rep ) Replaces a list of characters in a given text. PARAMETER DESCRIPTION text Text to modify. TYPE: str rep List of (old, new) tuples. old can list multiple characters. TYPE: List[Tuple[str, str]] RETURNS DESCRIPTION str Processed text. Source code in edsnlp/pipelines/core/normalizer/utils.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def replace ( text : str , rep : List [ Tuple [ str , str ]], ) -> str : \"\"\" Replaces a list of characters in a given text. Parameters ---------- text : str Text to modify. rep : List[Tuple[str, str]] List of `(old, new)` tuples. `old` can list multiple characters. Returns ------- str Processed text. \"\"\" for olds , new in rep : for old in olds : text = text . replace ( old , new ) return text","title":"utils"},{"location":"reference/pipelines/core/normalizer/utils/#edsnlppipelinescorenormalizerutils","text":"","title":"edsnlp.pipelines.core.normalizer.utils"},{"location":"reference/pipelines/core/normalizer/utils/#edsnlp.pipelines.core.normalizer.utils.replace","text":"Replaces a list of characters in a given text. PARAMETER DESCRIPTION text Text to modify. TYPE: str rep List of (old, new) tuples. old can list multiple characters. TYPE: List[Tuple[str, str]] RETURNS DESCRIPTION str Processed text. Source code in edsnlp/pipelines/core/normalizer/utils.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def replace ( text : str , rep : List [ Tuple [ str , str ]], ) -> str : \"\"\" Replaces a list of characters in a given text. Parameters ---------- text : str Text to modify. rep : List[Tuple[str, str]] List of `(old, new)` tuples. `old` can list multiple characters. Returns ------- str Processed text. \"\"\" for olds , new in rep : for old in olds : text = text . replace ( old , new ) return text","title":"replace()"},{"location":"reference/pipelines/core/normalizer/accents/","text":"edsnlp.pipelines.core.normalizer.accents accents Accents Bases: object Normalises accents, using a same-length strategy. PARAMETER DESCRIPTION accents List of accentuated characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class Accents ( object ): \"\"\" Normalises accents, using a same-length strategy. Parameters ---------- accents : List[Tuple[str, str]] List of accentuated characters and their transcription. \"\"\" def __init__ ( self , accents : Optional [ List [ Tuple [ str , str ]]]) -> None : if accents is None : accents = patterns . accents self . accents = accents def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . accents ) return doc __call__ ( doc ) Remove accents from spacy NORM attribute. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with accents removed in Token.norm_ . Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . accents ) return doc","title":"`edsnlp.pipelines.core.normalizer.accents`"},{"location":"reference/pipelines/core/normalizer/accents/#edsnlppipelinescorenormalizeraccents","text":"","title":"edsnlp.pipelines.core.normalizer.accents"},{"location":"reference/pipelines/core/normalizer/accents/#edsnlp.pipelines.core.normalizer.accents.accents","text":"","title":"accents"},{"location":"reference/pipelines/core/normalizer/accents/#edsnlp.pipelines.core.normalizer.accents.accents.Accents","text":"Bases: object Normalises accents, using a same-length strategy. PARAMETER DESCRIPTION accents List of accentuated characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class Accents ( object ): \"\"\" Normalises accents, using a same-length strategy. Parameters ---------- accents : List[Tuple[str, str]] List of accentuated characters and their transcription. \"\"\" def __init__ ( self , accents : Optional [ List [ Tuple [ str , str ]]]) -> None : if accents is None : accents = patterns . accents self . accents = accents def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . accents ) return doc","title":"Accents"},{"location":"reference/pipelines/core/normalizer/accents/#edsnlp.pipelines.core.normalizer.accents.accents.Accents.__call__","text":"Remove accents from spacy NORM attribute. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with accents removed in Token.norm_ . Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . accents ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/normalizer/accents/accents/","text":"edsnlp.pipelines.core.normalizer.accents.accents Accents Bases: object Normalises accents, using a same-length strategy. PARAMETER DESCRIPTION accents List of accentuated characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class Accents ( object ): \"\"\" Normalises accents, using a same-length strategy. Parameters ---------- accents : List[Tuple[str, str]] List of accentuated characters and their transcription. \"\"\" def __init__ ( self , accents : Optional [ List [ Tuple [ str , str ]]]) -> None : if accents is None : accents = patterns . accents self . accents = accents def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . accents ) return doc __call__ ( doc ) Remove accents from spacy NORM attribute. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with accents removed in Token.norm_ . Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . accents ) return doc","title":"accents"},{"location":"reference/pipelines/core/normalizer/accents/accents/#edsnlppipelinescorenormalizeraccentsaccents","text":"","title":"edsnlp.pipelines.core.normalizer.accents.accents"},{"location":"reference/pipelines/core/normalizer/accents/accents/#edsnlp.pipelines.core.normalizer.accents.accents.Accents","text":"Bases: object Normalises accents, using a same-length strategy. PARAMETER DESCRIPTION accents List of accentuated characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class Accents ( object ): \"\"\" Normalises accents, using a same-length strategy. Parameters ---------- accents : List[Tuple[str, str]] List of accentuated characters and their transcription. \"\"\" def __init__ ( self , accents : Optional [ List [ Tuple [ str , str ]]]) -> None : if accents is None : accents = patterns . accents self . accents = accents def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . accents ) return doc","title":"Accents"},{"location":"reference/pipelines/core/normalizer/accents/accents/#edsnlp.pipelines.core.normalizer.accents.accents.Accents.__call__","text":"Remove accents from spacy NORM attribute. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with accents removed in Token.norm_ . Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . accents ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/normalizer/accents/factory/","text":"edsnlp.pipelines.core.normalizer.accents.factory","title":"factory"},{"location":"reference/pipelines/core/normalizer/accents/factory/#edsnlppipelinescorenormalizeraccentsfactory","text":"","title":"edsnlp.pipelines.core.normalizer.accents.factory"},{"location":"reference/pipelines/core/normalizer/accents/patterns/","text":"edsnlp.pipelines.core.normalizer.accents.patterns","title":"patterns"},{"location":"reference/pipelines/core/normalizer/accents/patterns/#edsnlppipelinescorenormalizeraccentspatterns","text":"","title":"edsnlp.pipelines.core.normalizer.accents.patterns"},{"location":"reference/pipelines/core/normalizer/lowercase/","text":"edsnlp.pipelines.core.normalizer.lowercase factory remove_lowercase ( doc ) Add case on the NORM custom attribute. Should always be applied first. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with case put back in NORM . Source code in edsnlp/pipelines/core/normalizer/lowercase/factory.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Language . component ( \"remove-lowercase\" ) @Language . component ( \"eds.remove-lowercase\" ) def remove_lowercase ( doc : Doc ): \"\"\" Add case on the `NORM` custom attribute. Should always be applied first. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with case put back in `NORM`. \"\"\" for token in doc : token . norm_ = token . text return doc","title":"`edsnlp.pipelines.core.normalizer.lowercase`"},{"location":"reference/pipelines/core/normalizer/lowercase/#edsnlppipelinescorenormalizerlowercase","text":"","title":"edsnlp.pipelines.core.normalizer.lowercase"},{"location":"reference/pipelines/core/normalizer/lowercase/#edsnlp.pipelines.core.normalizer.lowercase.factory","text":"","title":"factory"},{"location":"reference/pipelines/core/normalizer/lowercase/#edsnlp.pipelines.core.normalizer.lowercase.factory.remove_lowercase","text":"Add case on the NORM custom attribute. Should always be applied first. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with case put back in NORM . Source code in edsnlp/pipelines/core/normalizer/lowercase/factory.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Language . component ( \"remove-lowercase\" ) @Language . component ( \"eds.remove-lowercase\" ) def remove_lowercase ( doc : Doc ): \"\"\" Add case on the `NORM` custom attribute. Should always be applied first. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with case put back in `NORM`. \"\"\" for token in doc : token . norm_ = token . text return doc","title":"remove_lowercase()"},{"location":"reference/pipelines/core/normalizer/lowercase/factory/","text":"edsnlp.pipelines.core.normalizer.lowercase.factory remove_lowercase ( doc ) Add case on the NORM custom attribute. Should always be applied first. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with case put back in NORM . Source code in edsnlp/pipelines/core/normalizer/lowercase/factory.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Language . component ( \"remove-lowercase\" ) @Language . component ( \"eds.remove-lowercase\" ) def remove_lowercase ( doc : Doc ): \"\"\" Add case on the `NORM` custom attribute. Should always be applied first. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with case put back in `NORM`. \"\"\" for token in doc : token . norm_ = token . text return doc","title":"factory"},{"location":"reference/pipelines/core/normalizer/lowercase/factory/#edsnlppipelinescorenormalizerlowercasefactory","text":"","title":"edsnlp.pipelines.core.normalizer.lowercase.factory"},{"location":"reference/pipelines/core/normalizer/lowercase/factory/#edsnlp.pipelines.core.normalizer.lowercase.factory.remove_lowercase","text":"Add case on the NORM custom attribute. Should always be applied first. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with case put back in NORM . Source code in edsnlp/pipelines/core/normalizer/lowercase/factory.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Language . component ( \"remove-lowercase\" ) @Language . component ( \"eds.remove-lowercase\" ) def remove_lowercase ( doc : Doc ): \"\"\" Add case on the `NORM` custom attribute. Should always be applied first. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with case put back in `NORM`. \"\"\" for token in doc : token . norm_ = token . text return doc","title":"remove_lowercase()"},{"location":"reference/pipelines/core/normalizer/pollution/","text":"edsnlp.pipelines.core.normalizer.pollution pollution Pollution Bases: BaseComponent Tags pollution tokens. Populates a number of spaCy extensions : Token._.pollution : indicates whether the token is a pollution Doc._.clean : lists non-pollution tokens Doc._.clean_ : original text with pollutions removed. Doc._.char_clean_span : method to create a Span using character indices extracted using the cleaned text. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: Language pollution Dictionary containing regular expressions of pollution. TYPE: Dict[str, Union[str, List[str]]] Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class Pollution ( BaseComponent ): \"\"\" Tags pollution tokens. Populates a number of spaCy extensions : - `Token._.pollution` : indicates whether the token is a pollution - `Doc._.clean` : lists non-pollution tokens - `Doc._.clean_` : original text with pollutions removed. - `Doc._.char_clean_span` : method to create a Span using character indices extracted using the cleaned text. Parameters ---------- nlp : Language Language pipeline object pollution : Dict[str, Union[str, List[str]]] Dictionary containing regular expressions of pollution. \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , pollution : Optional [ Dict [ str , Union [ str , List [ str ]]]], ): self . nlp = nlp if pollution is None : pollution = patterns . pollution self . pollution = pollution for k , v in self . pollution . items (): if isinstance ( v , str ): self . pollution [ k ] = [ v ] self . regex_matcher = RegexMatcher () self . build_patterns () def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True doc . spans [ \"pollutions\" ] = pollutions return doc build_patterns () Builds the patterns for phrase matching. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 54 55 56 57 58 59 60 61 def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v ) process ( doc ) Find pollutions in doc and clean candidate negations to remove pseudo negations PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION pollution list of pollution spans Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions __call__ ( doc ) Tags pollutions. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for pollutions. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True doc . spans [ \"pollutions\" ] = pollutions return doc","title":"`edsnlp.pipelines.core.normalizer.pollution`"},{"location":"reference/pipelines/core/normalizer/pollution/#edsnlppipelinescorenormalizerpollution","text":"","title":"edsnlp.pipelines.core.normalizer.pollution"},{"location":"reference/pipelines/core/normalizer/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution","text":"","title":"pollution"},{"location":"reference/pipelines/core/normalizer/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution","text":"Bases: BaseComponent Tags pollution tokens. Populates a number of spaCy extensions : Token._.pollution : indicates whether the token is a pollution Doc._.clean : lists non-pollution tokens Doc._.clean_ : original text with pollutions removed. Doc._.char_clean_span : method to create a Span using character indices extracted using the cleaned text. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: Language pollution Dictionary containing regular expressions of pollution. TYPE: Dict[str, Union[str, List[str]]] Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class Pollution ( BaseComponent ): \"\"\" Tags pollution tokens. Populates a number of spaCy extensions : - `Token._.pollution` : indicates whether the token is a pollution - `Doc._.clean` : lists non-pollution tokens - `Doc._.clean_` : original text with pollutions removed. - `Doc._.char_clean_span` : method to create a Span using character indices extracted using the cleaned text. Parameters ---------- nlp : Language Language pipeline object pollution : Dict[str, Union[str, List[str]]] Dictionary containing regular expressions of pollution. \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , pollution : Optional [ Dict [ str , Union [ str , List [ str ]]]], ): self . nlp = nlp if pollution is None : pollution = patterns . pollution self . pollution = pollution for k , v in self . pollution . items (): if isinstance ( v , str ): self . pollution [ k ] = [ v ] self . regex_matcher = RegexMatcher () self . build_patterns () def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True doc . spans [ \"pollutions\" ] = pollutions return doc","title":"Pollution"},{"location":"reference/pipelines/core/normalizer/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.build_patterns","text":"Builds the patterns for phrase matching. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 54 55 56 57 58 59 60 61 def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v )","title":"build_patterns()"},{"location":"reference/pipelines/core/normalizer/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.process","text":"Find pollutions in doc and clean candidate negations to remove pseudo negations PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION pollution list of pollution spans Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions","title":"process()"},{"location":"reference/pipelines/core/normalizer/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.__call__","text":"Tags pollutions. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for pollutions. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True doc . spans [ \"pollutions\" ] = pollutions return doc","title":"__call__()"},{"location":"reference/pipelines/core/normalizer/pollution/factory/","text":"edsnlp.pipelines.core.normalizer.pollution.factory","title":"factory"},{"location":"reference/pipelines/core/normalizer/pollution/factory/#edsnlppipelinescorenormalizerpollutionfactory","text":"","title":"edsnlp.pipelines.core.normalizer.pollution.factory"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/","text":"edsnlp.pipelines.core.normalizer.pollution.patterns","title":"patterns"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlppipelinescorenormalizerpollutionpatterns","text":"","title":"edsnlp.pipelines.core.normalizer.pollution.patterns"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/","text":"edsnlp.pipelines.core.normalizer.pollution.pollution Pollution Bases: BaseComponent Tags pollution tokens. Populates a number of spaCy extensions : Token._.pollution : indicates whether the token is a pollution Doc._.clean : lists non-pollution tokens Doc._.clean_ : original text with pollutions removed. Doc._.char_clean_span : method to create a Span using character indices extracted using the cleaned text. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: Language pollution Dictionary containing regular expressions of pollution. TYPE: Dict[str, Union[str, List[str]]] Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class Pollution ( BaseComponent ): \"\"\" Tags pollution tokens. Populates a number of spaCy extensions : - `Token._.pollution` : indicates whether the token is a pollution - `Doc._.clean` : lists non-pollution tokens - `Doc._.clean_` : original text with pollutions removed. - `Doc._.char_clean_span` : method to create a Span using character indices extracted using the cleaned text. Parameters ---------- nlp : Language Language pipeline object pollution : Dict[str, Union[str, List[str]]] Dictionary containing regular expressions of pollution. \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , pollution : Optional [ Dict [ str , Union [ str , List [ str ]]]], ): self . nlp = nlp if pollution is None : pollution = patterns . pollution self . pollution = pollution for k , v in self . pollution . items (): if isinstance ( v , str ): self . pollution [ k ] = [ v ] self . regex_matcher = RegexMatcher () self . build_patterns () def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True doc . spans [ \"pollutions\" ] = pollutions return doc build_patterns () Builds the patterns for phrase matching. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 54 55 56 57 58 59 60 61 def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v ) process ( doc ) Find pollutions in doc and clean candidate negations to remove pseudo negations PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION pollution list of pollution spans Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions __call__ ( doc ) Tags pollutions. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for pollutions. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True doc . spans [ \"pollutions\" ] = pollutions return doc","title":"pollution"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlppipelinescorenormalizerpollutionpollution","text":"","title":"edsnlp.pipelines.core.normalizer.pollution.pollution"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution","text":"Bases: BaseComponent Tags pollution tokens. Populates a number of spaCy extensions : Token._.pollution : indicates whether the token is a pollution Doc._.clean : lists non-pollution tokens Doc._.clean_ : original text with pollutions removed. Doc._.char_clean_span : method to create a Span using character indices extracted using the cleaned text. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: Language pollution Dictionary containing regular expressions of pollution. TYPE: Dict[str, Union[str, List[str]]] Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class Pollution ( BaseComponent ): \"\"\" Tags pollution tokens. Populates a number of spaCy extensions : - `Token._.pollution` : indicates whether the token is a pollution - `Doc._.clean` : lists non-pollution tokens - `Doc._.clean_` : original text with pollutions removed. - `Doc._.char_clean_span` : method to create a Span using character indices extracted using the cleaned text. Parameters ---------- nlp : Language Language pipeline object pollution : Dict[str, Union[str, List[str]]] Dictionary containing regular expressions of pollution. \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , pollution : Optional [ Dict [ str , Union [ str , List [ str ]]]], ): self . nlp = nlp if pollution is None : pollution = patterns . pollution self . pollution = pollution for k , v in self . pollution . items (): if isinstance ( v , str ): self . pollution [ k ] = [ v ] self . regex_matcher = RegexMatcher () self . build_patterns () def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True doc . spans [ \"pollutions\" ] = pollutions return doc","title":"Pollution"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.build_patterns","text":"Builds the patterns for phrase matching. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 54 55 56 57 58 59 60 61 def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v )","title":"build_patterns()"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.process","text":"Find pollutions in doc and clean candidate negations to remove pseudo negations PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION pollution list of pollution spans Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions","title":"process()"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.__call__","text":"Tags pollutions. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for pollutions. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True doc . spans [ \"pollutions\" ] = pollutions return doc","title":"__call__()"},{"location":"reference/pipelines/core/normalizer/quotes/","text":"edsnlp.pipelines.core.normalizer.quotes quotes Quotes Bases: object We normalise quotes, following this source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html> _. PARAMETER DESCRIPTION quotes List of quotation characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Quotes ( object ): \"\"\" We normalise quotes, following this `source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html>`_. Parameters ---------- quotes : List[Tuple[str, str]] List of quotation characters and their transcription. \"\"\" def __init__ ( self , quotes : Optional [ List [ Tuple [ str , str ]]]) -> None : if quotes is None : quotes = quotes_and_apostrophes self . quotes = quotes def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . quotes ) return doc __call__ ( doc ) Normalises quotes. PARAMETER DESCRIPTION doc Document to process. TYPE: Doc RETURNS DESCRIPTION Doc Same document, with quotes normalised. Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . quotes ) return doc","title":"`edsnlp.pipelines.core.normalizer.quotes`"},{"location":"reference/pipelines/core/normalizer/quotes/#edsnlppipelinescorenormalizerquotes","text":"","title":"edsnlp.pipelines.core.normalizer.quotes"},{"location":"reference/pipelines/core/normalizer/quotes/#edsnlp.pipelines.core.normalizer.quotes.quotes","text":"","title":"quotes"},{"location":"reference/pipelines/core/normalizer/quotes/#edsnlp.pipelines.core.normalizer.quotes.quotes.Quotes","text":"Bases: object We normalise quotes, following this source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html> _. PARAMETER DESCRIPTION quotes List of quotation characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Quotes ( object ): \"\"\" We normalise quotes, following this `source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html>`_. Parameters ---------- quotes : List[Tuple[str, str]] List of quotation characters and their transcription. \"\"\" def __init__ ( self , quotes : Optional [ List [ Tuple [ str , str ]]]) -> None : if quotes is None : quotes = quotes_and_apostrophes self . quotes = quotes def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . quotes ) return doc","title":"Quotes"},{"location":"reference/pipelines/core/normalizer/quotes/#edsnlp.pipelines.core.normalizer.quotes.quotes.Quotes.__call__","text":"Normalises quotes. PARAMETER DESCRIPTION doc Document to process. TYPE: Doc RETURNS DESCRIPTION Doc Same document, with quotes normalised. Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . quotes ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/normalizer/quotes/factory/","text":"edsnlp.pipelines.core.normalizer.quotes.factory","title":"factory"},{"location":"reference/pipelines/core/normalizer/quotes/factory/#edsnlppipelinescorenormalizerquotesfactory","text":"","title":"edsnlp.pipelines.core.normalizer.quotes.factory"},{"location":"reference/pipelines/core/normalizer/quotes/patterns/","text":"edsnlp.pipelines.core.normalizer.quotes.patterns","title":"patterns"},{"location":"reference/pipelines/core/normalizer/quotes/patterns/#edsnlppipelinescorenormalizerquotespatterns","text":"","title":"edsnlp.pipelines.core.normalizer.quotes.patterns"},{"location":"reference/pipelines/core/normalizer/quotes/quotes/","text":"edsnlp.pipelines.core.normalizer.quotes.quotes Quotes Bases: object We normalise quotes, following this source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html> _. PARAMETER DESCRIPTION quotes List of quotation characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Quotes ( object ): \"\"\" We normalise quotes, following this `source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html>`_. Parameters ---------- quotes : List[Tuple[str, str]] List of quotation characters and their transcription. \"\"\" def __init__ ( self , quotes : Optional [ List [ Tuple [ str , str ]]]) -> None : if quotes is None : quotes = quotes_and_apostrophes self . quotes = quotes def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . quotes ) return doc __call__ ( doc ) Normalises quotes. PARAMETER DESCRIPTION doc Document to process. TYPE: Doc RETURNS DESCRIPTION Doc Same document, with quotes normalised. Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . quotes ) return doc","title":"quotes"},{"location":"reference/pipelines/core/normalizer/quotes/quotes/#edsnlppipelinescorenormalizerquotesquotes","text":"","title":"edsnlp.pipelines.core.normalizer.quotes.quotes"},{"location":"reference/pipelines/core/normalizer/quotes/quotes/#edsnlp.pipelines.core.normalizer.quotes.quotes.Quotes","text":"Bases: object We normalise quotes, following this source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html> _. PARAMETER DESCRIPTION quotes List of quotation characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Quotes ( object ): \"\"\" We normalise quotes, following this `source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html>`_. Parameters ---------- quotes : List[Tuple[str, str]] List of quotation characters and their transcription. \"\"\" def __init__ ( self , quotes : Optional [ List [ Tuple [ str , str ]]]) -> None : if quotes is None : quotes = quotes_and_apostrophes self . quotes = quotes def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . quotes ) return doc","title":"Quotes"},{"location":"reference/pipelines/core/normalizer/quotes/quotes/#edsnlp.pipelines.core.normalizer.quotes.quotes.Quotes.__call__","text":"Normalises quotes. PARAMETER DESCRIPTION doc Document to process. TYPE: Doc RETURNS DESCRIPTION Doc Same document, with quotes normalised. Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = replace ( text = token . norm_ , rep = self . quotes ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/sentences/","text":"edsnlp.pipelines.core.sentences","title":"`edsnlp.pipelines.core.sentences`"},{"location":"reference/pipelines/core/sentences/#edsnlppipelinescoresentences","text":"","title":"edsnlp.pipelines.core.sentences"},{"location":"reference/pipelines/core/sentences/factory/","text":"edsnlp.pipelines.core.sentences.factory","title":"factory"},{"location":"reference/pipelines/core/sentences/factory/#edsnlppipelinescoresentencesfactory","text":"","title":"edsnlp.pipelines.core.sentences.factory"},{"location":"reference/pipelines/core/sentences/sentences/","text":"edsnlp.pipelines.core.sentences.sentences SentenceSegmenter Bases: object Segments the Doc into sentences using a rule-based strategy, specific to AP-HP documents. Applies the same rule-based pipeline as spaCy's sentencizer, and adds a simple rule on the new lines : if a new line is followed by a capitalised word, then it is also an end of sentence. DOCS: https://spacy.io/api/sentencizer Arguments punct_chars : Optional[List[str]] Punctuation characters. use_endlines : bool Whether to use endlines prediction. Source code in edsnlp/pipelines/core/sentences/sentences.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class SentenceSegmenter ( object ): \"\"\" Segments the Doc into sentences using a rule-based strategy, specific to AP-HP documents. Applies the same rule-based pipeline as spaCy's sentencizer, and adds a simple rule on the new lines : if a new line is followed by a capitalised word, then it is also an end of sentence. DOCS: https://spacy.io/api/sentencizer Arguments --------- punct_chars : Optional[List[str]] Punctuation characters. use_endlines : bool Whether to use endlines prediction. \"\"\" def __init__ ( self , punct_chars : Optional [ List [ str ]], use_endlines : bool , ): if punct_chars is None : punct_chars = punctuation self . punct_chars = set ( punct_chars ) self . use_endlines = use_endlines def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Segments the document in sentences. Arguments --------- doc: A spacy Doc object. Returns ------- doc: A spaCy Doc object, annotated for sentences. \"\"\" if not doc : return doc doc [ 0 ] . sent_start = True seen_period = False seen_newline = False for i , token in enumerate ( doc ): is_in_punct_chars = token . text in self . punct_chars is_newline = token . is_space and \" \\n \" in token . text if self . use_endlines : end_line = getattr ( token . _ , \"end_line\" , None ) is_newline = is_newline and ( end_line or end_line is None ) token . sent_start = ( i == 0 ) # To set the attributes at False by default for the other tokens if seen_period or seen_newline : if token . is_punct or is_in_punct_chars or is_newline : continue if seen_period : token . sent_start = True seen_newline = False seen_period = False else : token . sent_start = token . shape_ . startswith ( \"Xx\" ) seen_newline = False seen_period = False elif is_in_punct_chars : seen_period = True elif is_newline : seen_newline = True return doc __call__ ( doc ) Segments the document in sentences. Arguments doc: A spacy Doc object. RETURNS DESCRIPTION doc A spaCy Doc object, annotated for sentences. Source code in edsnlp/pipelines/core/sentences/sentences.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Segments the document in sentences. Arguments --------- doc: A spacy Doc object. Returns ------- doc: A spaCy Doc object, annotated for sentences. \"\"\" if not doc : return doc doc [ 0 ] . sent_start = True seen_period = False seen_newline = False for i , token in enumerate ( doc ): is_in_punct_chars = token . text in self . punct_chars is_newline = token . is_space and \" \\n \" in token . text if self . use_endlines : end_line = getattr ( token . _ , \"end_line\" , None ) is_newline = is_newline and ( end_line or end_line is None ) token . sent_start = ( i == 0 ) # To set the attributes at False by default for the other tokens if seen_period or seen_newline : if token . is_punct or is_in_punct_chars or is_newline : continue if seen_period : token . sent_start = True seen_newline = False seen_period = False else : token . sent_start = token . shape_ . startswith ( \"Xx\" ) seen_newline = False seen_period = False elif is_in_punct_chars : seen_period = True elif is_newline : seen_newline = True return doc","title":"sentences"},{"location":"reference/pipelines/core/sentences/sentences/#edsnlppipelinescoresentencessentences","text":"","title":"edsnlp.pipelines.core.sentences.sentences"},{"location":"reference/pipelines/core/sentences/sentences/#edsnlp.pipelines.core.sentences.sentences.SentenceSegmenter","text":"Bases: object Segments the Doc into sentences using a rule-based strategy, specific to AP-HP documents. Applies the same rule-based pipeline as spaCy's sentencizer, and adds a simple rule on the new lines : if a new line is followed by a capitalised word, then it is also an end of sentence. DOCS: https://spacy.io/api/sentencizer","title":"SentenceSegmenter"},{"location":"reference/pipelines/core/sentences/sentences/#edsnlp.pipelines.core.sentences.sentences.SentenceSegmenter--arguments","text":"punct_chars : Optional[List[str]] Punctuation characters. use_endlines : bool Whether to use endlines prediction. Source code in edsnlp/pipelines/core/sentences/sentences.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class SentenceSegmenter ( object ): \"\"\" Segments the Doc into sentences using a rule-based strategy, specific to AP-HP documents. Applies the same rule-based pipeline as spaCy's sentencizer, and adds a simple rule on the new lines : if a new line is followed by a capitalised word, then it is also an end of sentence. DOCS: https://spacy.io/api/sentencizer Arguments --------- punct_chars : Optional[List[str]] Punctuation characters. use_endlines : bool Whether to use endlines prediction. \"\"\" def __init__ ( self , punct_chars : Optional [ List [ str ]], use_endlines : bool , ): if punct_chars is None : punct_chars = punctuation self . punct_chars = set ( punct_chars ) self . use_endlines = use_endlines def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Segments the document in sentences. Arguments --------- doc: A spacy Doc object. Returns ------- doc: A spaCy Doc object, annotated for sentences. \"\"\" if not doc : return doc doc [ 0 ] . sent_start = True seen_period = False seen_newline = False for i , token in enumerate ( doc ): is_in_punct_chars = token . text in self . punct_chars is_newline = token . is_space and \" \\n \" in token . text if self . use_endlines : end_line = getattr ( token . _ , \"end_line\" , None ) is_newline = is_newline and ( end_line or end_line is None ) token . sent_start = ( i == 0 ) # To set the attributes at False by default for the other tokens if seen_period or seen_newline : if token . is_punct or is_in_punct_chars or is_newline : continue if seen_period : token . sent_start = True seen_newline = False seen_period = False else : token . sent_start = token . shape_ . startswith ( \"Xx\" ) seen_newline = False seen_period = False elif is_in_punct_chars : seen_period = True elif is_newline : seen_newline = True return doc","title":"Arguments"},{"location":"reference/pipelines/core/sentences/sentences/#edsnlp.pipelines.core.sentences.sentences.SentenceSegmenter.__call__","text":"Segments the document in sentences.","title":"__call__()"},{"location":"reference/pipelines/core/sentences/sentences/#edsnlp.pipelines.core.sentences.sentences.SentenceSegmenter.__call__--arguments","text":"doc: A spacy Doc object. RETURNS DESCRIPTION doc A spaCy Doc object, annotated for sentences. Source code in edsnlp/pipelines/core/sentences/sentences.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Segments the document in sentences. Arguments --------- doc: A spacy Doc object. Returns ------- doc: A spaCy Doc object, annotated for sentences. \"\"\" if not doc : return doc doc [ 0 ] . sent_start = True seen_period = False seen_newline = False for i , token in enumerate ( doc ): is_in_punct_chars = token . text in self . punct_chars is_newline = token . is_space and \" \\n \" in token . text if self . use_endlines : end_line = getattr ( token . _ , \"end_line\" , None ) is_newline = is_newline and ( end_line or end_line is None ) token . sent_start = ( i == 0 ) # To set the attributes at False by default for the other tokens if seen_period or seen_newline : if token . is_punct or is_in_punct_chars or is_newline : continue if seen_period : token . sent_start = True seen_newline = False seen_period = False else : token . sent_start = token . shape_ . startswith ( \"Xx\" ) seen_newline = False seen_period = False elif is_in_punct_chars : seen_period = True elif is_newline : seen_newline = True return doc","title":"Arguments"},{"location":"reference/pipelines/core/sentences/terms/","text":"edsnlp.pipelines.core.sentences.terms","title":"terms"},{"location":"reference/pipelines/core/sentences/terms/#edsnlppipelinescoresentencesterms","text":"","title":"edsnlp.pipelines.core.sentences.terms"},{"location":"reference/pipelines/misc/","text":"edsnlp.pipelines.misc","title":"`edsnlp.pipelines.misc`"},{"location":"reference/pipelines/misc/#edsnlppipelinesmisc","text":"","title":"edsnlp.pipelines.misc"},{"location":"reference/pipelines/misc/consultation_dates/","text":"edsnlp.pipelines.misc.consultation_dates","title":"`edsnlp.pipelines.misc.consultation_dates`"},{"location":"reference/pipelines/misc/consultation_dates/#edsnlppipelinesmiscconsultation_dates","text":"","title":"edsnlp.pipelines.misc.consultation_dates"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/","text":"edsnlp.pipelines.misc.consultation_dates.consultation_dates ConsultationDates Bases: GenericMatcher Class to extract consultation dates from \"CR-CONS\" documents. The pipeline populates the doc . spans [ 'consultation_dates' ] list. For each extraction s in this list, the corresponding date is available as s._.consultation_date . PARAMETER DESCRIPTION nlp Language pipeline object TYPE: Language consultation_mention List of RegEx for consultation mentions. If type==list : Overrides the default list If type==bool : Uses the default list of True, disable if False TYPE: Union[List[str], bool] town_mention : Union[List[str], bool] List of RegEx for all AP-HP hospitals' towns mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False document_date_mention : Union[List[str], bool] List of RegEx for document date. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class ConsultationDates ( GenericMatcher ): \"\"\" Class to extract consultation dates from \"CR-CONS\" documents. The pipeline populates the `#!python doc.spans['consultation_dates']` list. For each extraction `s` in this list, the corresponding date is available as `s._.consultation_date`. Parameters ---------- nlp : Language Language pipeline object consultation_mention : Union[List[str], bool] List of RegEx for consultation mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False town_mention : Union[List[str], bool] List of RegEx for all AP-HP hospitals' towns mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False document_date_mention : Union[List[str], bool] List of RegEx for document date. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False \"\"\" def __init__ ( self , nlp : Language , consultation_mention : Union [ List [ str ], bool ], town_mention : Union [ List [ str ], bool ], document_date_mention : Union [ List [ str ], bool ], attr : str , ** kwargs , ): logger . warning ( \"This pipeline is still in beta\" ) logger . warning ( \"This pipeline should ONLY be used on notes \" \"where `note_class_source_value == 'CR-CONS'`\" ) logger . warning ( \"\"\"This pipeline requires to use the normalizer pipeline with: lowercase=True, accents=True, quotes=True\"\"\" ) if not ( nlp . has_pipe ( \"dates\" ) and nlp . get_pipe ( \"dates\" ) . on_ents_only is False ): config = dict ( ** DEFAULT_CONFIG ) config [ \"on_ents_only\" ] = \"consultation_mentions\" self . date_matcher = Dates ( nlp , ** config ) else : self . date_matcher = None if not consultation_mention : consultation_mention = [] elif consultation_mention is True : consultation_mention = consult_regex . consultation_mention if not document_date_mention : document_date_mention = [] elif document_date_mention is True : document_date_mention = consult_regex . document_date_mention if not town_mention : town_mention = [] elif town_mention is True : town_mention = consult_regex . town_mention regex = dict ( consultation_mention = consultation_mention , town_mention = town_mention , document_date_mention = document_date_mention , ) super () . __init__ ( nlp , regex = regex , terms = dict (), attr = attr , ignore_excluded = False , ** kwargs , ) self . set_extensions () @staticmethod def set_extensions () -> None : if not Span . has_extension ( \"consultation_date\" ): Span . set_extension ( \"consultation_date\" , default = None ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Finds entities Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object with additionnal doc.spans['consultation_dates] SpanGroup \"\"\" ents = self . process ( doc ) doc . spans [ \"consultation_mentions\" ] = ents doc . spans [ \"consultation_dates\" ] = [] if self . date_matcher is not None : doc = self . date_matcher ( doc ) for mention in ents : # Looking for a date # - In the same sentence # - Not less than 10 tokens AFTER the consultation mention matching_dates = [ date for date in doc . spans [ \"dates\" ] if ( ( mention . sent == date . sent ) and ( date . start > mention . start ) and ( date . start - mention . end <= 10 ) ) ] if matching_dates : # We keep the first mention of a date kept_date = min ( matching_dates , key = lambda d : d . start ) span = doc [ mention . start : kept_date . end ] span . label_ = mention . label_ span . _ . consultation_date = kept_date . _ . parsed_date doc . spans [ \"consultation_dates\" ] . append ( span ) del doc . spans [ \"consultation_mentions\" ] return doc __call__ ( doc ) Finds entities PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Finds entities Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object with additionnal doc.spans['consultation_dates] SpanGroup \"\"\" ents = self . process ( doc ) doc . spans [ \"consultation_mentions\" ] = ents doc . spans [ \"consultation_dates\" ] = [] if self . date_matcher is not None : doc = self . date_matcher ( doc ) for mention in ents : # Looking for a date # - In the same sentence # - Not less than 10 tokens AFTER the consultation mention matching_dates = [ date for date in doc . spans [ \"dates\" ] if ( ( mention . sent == date . sent ) and ( date . start > mention . start ) and ( date . start - mention . end <= 10 ) ) ] if matching_dates : # We keep the first mention of a date kept_date = min ( matching_dates , key = lambda d : d . start ) span = doc [ mention . start : kept_date . end ] span . label_ = mention . label_ span . _ . consultation_date = kept_date . _ . parsed_date doc . spans [ \"consultation_dates\" ] . append ( span ) del doc . spans [ \"consultation_mentions\" ] return doc","title":"consultation_dates"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/#edsnlppipelinesmiscconsultation_datesconsultation_dates","text":"","title":"edsnlp.pipelines.misc.consultation_dates.consultation_dates"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/#edsnlp.pipelines.misc.consultation_dates.consultation_dates.ConsultationDates","text":"Bases: GenericMatcher Class to extract consultation dates from \"CR-CONS\" documents. The pipeline populates the doc . spans [ 'consultation_dates' ] list. For each extraction s in this list, the corresponding date is available as s._.consultation_date . PARAMETER DESCRIPTION nlp Language pipeline object TYPE: Language consultation_mention List of RegEx for consultation mentions. If type==list : Overrides the default list If type==bool : Uses the default list of True, disable if False TYPE: Union[List[str], bool] town_mention : Union[List[str], bool] List of RegEx for all AP-HP hospitals' towns mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False document_date_mention : Union[List[str], bool] List of RegEx for document date. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class ConsultationDates ( GenericMatcher ): \"\"\" Class to extract consultation dates from \"CR-CONS\" documents. The pipeline populates the `#!python doc.spans['consultation_dates']` list. For each extraction `s` in this list, the corresponding date is available as `s._.consultation_date`. Parameters ---------- nlp : Language Language pipeline object consultation_mention : Union[List[str], bool] List of RegEx for consultation mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False town_mention : Union[List[str], bool] List of RegEx for all AP-HP hospitals' towns mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False document_date_mention : Union[List[str], bool] List of RegEx for document date. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False \"\"\" def __init__ ( self , nlp : Language , consultation_mention : Union [ List [ str ], bool ], town_mention : Union [ List [ str ], bool ], document_date_mention : Union [ List [ str ], bool ], attr : str , ** kwargs , ): logger . warning ( \"This pipeline is still in beta\" ) logger . warning ( \"This pipeline should ONLY be used on notes \" \"where `note_class_source_value == 'CR-CONS'`\" ) logger . warning ( \"\"\"This pipeline requires to use the normalizer pipeline with: lowercase=True, accents=True, quotes=True\"\"\" ) if not ( nlp . has_pipe ( \"dates\" ) and nlp . get_pipe ( \"dates\" ) . on_ents_only is False ): config = dict ( ** DEFAULT_CONFIG ) config [ \"on_ents_only\" ] = \"consultation_mentions\" self . date_matcher = Dates ( nlp , ** config ) else : self . date_matcher = None if not consultation_mention : consultation_mention = [] elif consultation_mention is True : consultation_mention = consult_regex . consultation_mention if not document_date_mention : document_date_mention = [] elif document_date_mention is True : document_date_mention = consult_regex . document_date_mention if not town_mention : town_mention = [] elif town_mention is True : town_mention = consult_regex . town_mention regex = dict ( consultation_mention = consultation_mention , town_mention = town_mention , document_date_mention = document_date_mention , ) super () . __init__ ( nlp , regex = regex , terms = dict (), attr = attr , ignore_excluded = False , ** kwargs , ) self . set_extensions () @staticmethod def set_extensions () -> None : if not Span . has_extension ( \"consultation_date\" ): Span . set_extension ( \"consultation_date\" , default = None ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Finds entities Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object with additionnal doc.spans['consultation_dates] SpanGroup \"\"\" ents = self . process ( doc ) doc . spans [ \"consultation_mentions\" ] = ents doc . spans [ \"consultation_dates\" ] = [] if self . date_matcher is not None : doc = self . date_matcher ( doc ) for mention in ents : # Looking for a date # - In the same sentence # - Not less than 10 tokens AFTER the consultation mention matching_dates = [ date for date in doc . spans [ \"dates\" ] if ( ( mention . sent == date . sent ) and ( date . start > mention . start ) and ( date . start - mention . end <= 10 ) ) ] if matching_dates : # We keep the first mention of a date kept_date = min ( matching_dates , key = lambda d : d . start ) span = doc [ mention . start : kept_date . end ] span . label_ = mention . label_ span . _ . consultation_date = kept_date . _ . parsed_date doc . spans [ \"consultation_dates\" ] . append ( span ) del doc . spans [ \"consultation_mentions\" ] return doc","title":"ConsultationDates"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/#edsnlp.pipelines.misc.consultation_dates.consultation_dates.ConsultationDates.__call__","text":"Finds entities PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Finds entities Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object with additionnal doc.spans['consultation_dates] SpanGroup \"\"\" ents = self . process ( doc ) doc . spans [ \"consultation_mentions\" ] = ents doc . spans [ \"consultation_dates\" ] = [] if self . date_matcher is not None : doc = self . date_matcher ( doc ) for mention in ents : # Looking for a date # - In the same sentence # - Not less than 10 tokens AFTER the consultation mention matching_dates = [ date for date in doc . spans [ \"dates\" ] if ( ( mention . sent == date . sent ) and ( date . start > mention . start ) and ( date . start - mention . end <= 10 ) ) ] if matching_dates : # We keep the first mention of a date kept_date = min ( matching_dates , key = lambda d : d . start ) span = doc [ mention . start : kept_date . end ] span . label_ = mention . label_ span . _ . consultation_date = kept_date . _ . parsed_date doc . spans [ \"consultation_dates\" ] . append ( span ) del doc . spans [ \"consultation_mentions\" ] return doc","title":"__call__()"},{"location":"reference/pipelines/misc/consultation_dates/factory/","text":"edsnlp.pipelines.misc.consultation_dates.factory","title":"factory"},{"location":"reference/pipelines/misc/consultation_dates/factory/#edsnlppipelinesmiscconsultation_datesfactory","text":"","title":"edsnlp.pipelines.misc.consultation_dates.factory"},{"location":"reference/pipelines/misc/consultation_dates/patterns/","text":"edsnlp.pipelines.misc.consultation_dates.patterns","title":"patterns"},{"location":"reference/pipelines/misc/consultation_dates/patterns/#edsnlppipelinesmiscconsultation_datespatterns","text":"","title":"edsnlp.pipelines.misc.consultation_dates.patterns"},{"location":"reference/pipelines/misc/dates/","text":"edsnlp.pipelines.misc.dates parsing str2int ( time ) Converts a string to an integer. Returns None if the string cannot be converted. PARAMETER DESCRIPTION time String representation TYPE: str RETURNS DESCRIPTION int Integer conversion. Source code in edsnlp/pipelines/misc/dates/parsing.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def str2int ( time : str ) -> int : \"\"\" Converts a string to an integer. Returns `None` if the string cannot be converted. Parameters ---------- time : str String representation Returns ------- int Integer conversion. \"\"\" try : return int ( time ) except ValueError : return None time2int_factory ( patterns ) Factory for a time2int conversion function. PARAMETER DESCRIPTION patterns Dictionary of conversion/pattern. TYPE: Dict[str, int] RETURNS DESCRIPTION Callable[[str], int] String to integer function. Source code in edsnlp/pipelines/misc/dates/parsing.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def time2int_factory ( patterns : Dict [ str , int ]) -> Callable [[ str ], int ]: \"\"\" Factory for a `time2int` conversion function. Parameters ---------- patterns : Dict[str, int] Dictionary of conversion/pattern. Returns ------- Callable[[str], int] String to integer function. \"\"\" def time2int ( time : str ) -> int : \"\"\" Converts a string representation to the proper integer, iterating over a dictionnary of pattern/conversion. Parameters ---------- time : str String representation Returns ------- int Integer conversion \"\"\" m = str2int ( time ) if m is not None : return m for pattern , key in patterns . items (): if re . match ( f \"^ { pattern } $\" , time ): m = key break return m return time2int dates Dates Bases: BaseComponent Tags and normalizes dates, using the open-source dateparser library. The pipeline uses spaCy's filter_spans function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: spacy.language.Language absolute List of regular expressions for absolute dates. TYPE: Union[List[str], str] full List of regular expressions for full dates in YYYY-MM-DD format. TYPE: Union[List[str], str] relative List of regular expressions for relative dates (eg hier , la semaine prochaine ). TYPE: Union[List[str], str] no_year List of regular expressions for dates that do not display a year. TYPE: Union[List[str], str] no_day List of regular expressions for dates that do not display a day. TYPE: Union[List[str], str] year_only List of regular expressions for dates that only display a year. TYPE: Union[List[str], str] current List of regular expressions for dates that relate to the current month, week, year, etc. TYPE: Union[List[str], str] false_positive List of regular expressions for false positive (eg phone numbers, etc). TYPE: Union[List[str], str] on_ents_only Wether to look on dates in the whole document or in specific sentences: If True : Only look in the sentences of each entity in doc.ents If False: Look in the whole document If given a string key or list of string: Only look in the sentences of each entity in doc . spans [ key ] TYPE: Union[bool, str, List[str]] Source code in edsnlp/pipelines/misc/dates/dates.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 class Dates ( BaseComponent ): \"\"\" Tags and normalizes dates, using the open-source `dateparser` library. The pipeline uses spaCy's `filter_spans` function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. Parameters ---------- nlp : spacy.language.Language Language pipeline object absolute : Union[List[str], str] List of regular expressions for absolute dates. full : Union[List[str], str] List of regular expressions for full dates in YYYY-MM-DD format. relative : Union[List[str], str] List of regular expressions for relative dates (eg `hier`, `la semaine prochaine`). no_year : Union[List[str], str] List of regular expressions for dates that do not display a year. no_day : Union[List[str], str] List of regular expressions for dates that do not display a day. year_only : Union[List[str], str] List of regular expressions for dates that only display a year. current : Union[List[str], str] List of regular expressions for dates that relate to the current month, week, year, etc. false_positive : Union[List[str], str] List of regular expressions for false positive (eg phone numbers, etc). on_ents_only : Union[bool, str, List[str]] Wether to look on dates in the whole document or in specific sentences: - If `True`: Only look in the sentences of each entity in doc.ents - If False: Look in the whole document - If given a string `key` or list of string: Only look in the sentences of each entity in `#!python doc.spans[key]` \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , absolute : Optional [ List [ str ]], full : Optional [ List [ str ]], relative : Optional [ List [ str ]], no_year : Optional [ List [ str ]], no_day : Optional [ List [ str ]], year_only : Optional [ List [ str ]], current : Optional [ List [ str ]], false_positive : Optional [ List [ str ]], on_ents_only : bool , attr : str , ): self . nlp = nlp if no_year is None : no_year = patterns . no_year_pattern if year_only is None : year_only = patterns . full_year_pattern if no_day is None : no_day = patterns . no_day_pattern if absolute is None : absolute = patterns . absolute_date_pattern if relative is None : relative = patterns . relative_date_pattern if full is None : full = patterns . full_date_pattern if current is None : current = patterns . current_pattern if false_positive is None : false_positive = patterns . false_positive_pattern if isinstance ( absolute , str ): absolute = [ absolute ] if isinstance ( relative , str ): relative = [ relative ] if isinstance ( no_year , str ): no_year = [ no_year ] if isinstance ( no_day , str ): no_day = [ no_day ] if isinstance ( year_only , str ): year_only = [ year_only ] if isinstance ( full , str ): full = [ full ] if isinstance ( current , str ): current = [ current ] if isinstance ( false_positive , str ): false_positive = [ false_positive ] self . on_ents_only = on_ents_only self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"false_positive\" , false_positive ) self . regex_matcher . add ( \"full_date\" , full ) self . regex_matcher . add ( \"absolute\" , absolute ) self . regex_matcher . add ( \"relative\" , relative ) self . regex_matcher . add ( \"no_year\" , no_year ) self . regex_matcher . add ( \"no_day\" , no_day ) self . regex_matcher . add ( \"year_only\" , year_only ) self . regex_matcher . add ( \"current\" , current ) self . parser = date_parser self . set_extensions () @staticmethod def set_extensions () -> None : if not Doc . has_extension ( \"note_datetime\" ): Doc . set_extension ( \"note_datetime\" , default = None ) if not Span . has_extension ( \"parsed_date\" ): Span . set_extension ( \"parsed_date\" , default = None ) if not Span . has_extension ( \"parsed_delta\" ): Span . set_extension ( \"parsed_delta\" , default = None ) if not Span . has_extension ( \"date\" ): Span . set_extension ( \"date\" , getter = date_getter ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , # return_groupdict=True, ), ) else : dates = self . regex_matcher ( doc , as_spans = True , # return_groupdict=True, ) # dates = apply_groupdict(dates) dates = filter_spans ( dates ) dates = [ date for date in dates if date . label_ != \"false_positive\" ] return dates def get_date ( self , date : Span ) -> Optional [ datetime ]: \"\"\" Get normalised date using `dateparser`. Parameters ---------- date : Span Date span. Returns ------- Optional[datetime] If a date is recognised, returns a Python `datetime` object. Returns `None` otherwise. \"\"\" text_date = date . text if date . label_ == \"no_day\" : text_date = \"01/\" + re . sub ( r \"[\\.\\/\\s]\" , \"/\" , text_date ) elif date . label_ == \"full_date\" : text_date = re . sub ( r \"[\\.\\/\\s]\" , \"-\" , text_date ) try : return datetime . strptime ( text_date , \"%Y-%m- %d \" ) except ValueError : try : return datetime . strptime ( text_date , \"%Y- %d -%m\" ) except ValueError : return None # text_date = re.sub(r\"\\.\", \"-\", text_date) return self . parser ( text_date ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) for date in dates : d = self . get_date ( date ) if d is None : date . _ . parsed_date = None else : date . _ . parsed_date = d date . _ . parsed_delta = d - datetime . now () + timedelta ( seconds = 10 ) doc . spans [ \"dates\" ] = dates return doc process ( doc ) Find dates in doc. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION dates list of date spans Source code in edsnlp/pipelines/misc/dates/dates.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , # return_groupdict=True, ), ) else : dates = self . regex_matcher ( doc , as_spans = True , # return_groupdict=True, ) # dates = apply_groupdict(dates) dates = filter_spans ( dates ) dates = [ date for date in dates if date . label_ != \"false_positive\" ] return dates get_date ( date ) Get normalised date using dateparser . PARAMETER DESCRIPTION date Date span. TYPE: Span RETURNS DESCRIPTION Optional[datetime] If a date is recognised, returns a Python datetime object. Returns None otherwise. Source code in edsnlp/pipelines/misc/dates/dates.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def get_date ( self , date : Span ) -> Optional [ datetime ]: \"\"\" Get normalised date using `dateparser`. Parameters ---------- date : Span Date span. Returns ------- Optional[datetime] If a date is recognised, returns a Python `datetime` object. Returns `None` otherwise. \"\"\" text_date = date . text if date . label_ == \"no_day\" : text_date = \"01/\" + re . sub ( r \"[\\.\\/\\s]\" , \"/\" , text_date ) elif date . label_ == \"full_date\" : text_date = re . sub ( r \"[\\.\\/\\s]\" , \"-\" , text_date ) try : return datetime . strptime ( text_date , \"%Y-%m- %d \" ) except ValueError : try : return datetime . strptime ( text_date , \"%Y- %d -%m\" ) except ValueError : return None # text_date = re.sub(r\"\\.\", \"-\", text_date) return self . parser ( text_date ) __call__ ( doc ) Tags dates. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for dates Source code in edsnlp/pipelines/misc/dates/dates.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) for date in dates : d = self . get_date ( date ) if d is None : date . _ . parsed_date = None else : date . _ . parsed_date = d date . _ . parsed_delta = d - datetime . now () + timedelta ( seconds = 10 ) doc . spans [ \"dates\" ] = dates return doc td2str ( td ) Transforms a timedelta object to a string representation. PARAMETER DESCRIPTION td The timedelta object to represent. TYPE: timedelta RETURNS DESCRIPTION str Usable representation for the timedelta object. Source code in edsnlp/pipelines/misc/dates/dates.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def td2str ( td : timedelta ): \"\"\" Transforms a timedelta object to a string representation. Parameters ---------- td : timedelta The timedelta object to represent. Returns ------- str Usable representation for the timedelta object. \"\"\" seconds = td . total_seconds () days = int ( seconds / 3600 / 24 ) return f \"TD { days : +d } \" date_getter ( date ) Getter for dates. Uses the information from note_datetime . PARAMETER DESCRIPTION date Date detected by the pipeline. TYPE: Span RETURNS DESCRIPTION str Normalized date. Source code in edsnlp/pipelines/misc/dates/dates.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def date_getter ( date : Span ) -> str : \"\"\" Getter for dates. Uses the information from `note_datetime`. Parameters ---------- date : Span Date detected by the pipeline. Returns ------- str Normalized date. \"\"\" d = date . _ . parsed_date if d is None : # dateparser could not interpret the date. return \"????-??-??\" delta = date . _ . parsed_delta note_datetime = date . doc . _ . note_datetime if date . label_ in { \"absolute\" , \"full_date\" , \"no_day\" }: normalized = d . strftime ( \"%Y-%m- %d \" ) elif date . label_ == \"no_year\" : if note_datetime : year = note_datetime . strftime ( \"%Y\" ) else : year = \"????\" normalized = d . strftime ( f \" { year } -%m-%d\" ) else : if note_datetime : # We need to adjust the timedelta, since most dates are set at 00h00. # The slightest difference leads to a day difference. d = note_datetime + delta normalized = d . strftime ( \"%Y-%m- %d \" ) else : normalized = td2str ( d - datetime . now ()) return normalized date_parser ( text_date ) Function to parse dates. It try first all available parsers ('timestamp', 'custom-formats', 'absolute-time') but 'relative-time'. If no date is found, retries with 'relative-time'. When just the year is identified, it returns a datetime object with month and day equal to 1. PARAMETER DESCRIPTION text_date TYPE: str RETURNS DESCRIPTION datetime Source code in edsnlp/pipelines/misc/dates/dates.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def date_parser ( text_date : str ) -> datetime : \"\"\" Function to parse dates. It try first all available parsers ('timestamp', 'custom-formats', 'absolute-time') but 'relative-time'. If no date is found, retries with 'relative-time'. When just the year is identified, it returns a datetime object with month and day equal to 1. Parameters ---------- text_date : str Returns ------- datetime \"\"\" parsed_date = parser1 . get_date_data ( text_date ) if parsed_date . date_obj : if parsed_date . period == \"year\" : return datetime ( year = parsed_date . date_obj . year , month = 1 , day = 1 ) else : return parsed_date . date_obj else : parsed_date2 = parser2 . get_date_data ( text_date ) return parsed_date2 . date_obj parse_groupdict ( day = None , month = None , year = None , hour = None , minute = None , second = None , ** kwargs ) Parse date groupdict. PARAMETER DESCRIPTION day String representation of the day, by default None TYPE: str, optional DEFAULT: None month String representation of the month, by default None TYPE: str, optional DEFAULT: None year String representation of the year, by default None TYPE: str, optional DEFAULT: None hour String representation of the hour, by default None TYPE: str, optional DEFAULT: None minute String representation of the minute, by default None TYPE: str, optional DEFAULT: None second String representation of the minute, by default None TYPE: str, optional DEFAULT: None RETURNS DESCRIPTION Dict[str, int] Parsed groupdict. Source code in edsnlp/pipelines/misc/dates/dates.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def parse_groupdict ( day : str = None , month : str = None , year : str = None , hour : str = None , minute : str = None , second : str = None , ** kwargs : Dict [ str , str ], ) -> Dict [ str , int ]: \"\"\" Parse date groupdict. Parameters ---------- day : str, optional String representation of the day, by default None month : str, optional String representation of the month, by default None year : str, optional String representation of the year, by default None hour : str, optional String representation of the hour, by default None minute : str, optional String representation of the minute, by default None second : str, optional String representation of the minute, by default None Returns ------- Dict[str, int] Parsed groupdict. \"\"\" result = dict () if day is not None : result [ \"day\" ] = day2int ( day ) if month is not None : result [ \"month\" ] = month2int ( month ) if year is not None : result [ \"year\" ] = str2int ( year ) if hour is not None : result [ \"hour\" ] = str2int ( hour ) if minute is not None : result [ \"minute\" ] = str2int ( minute ) if second is not None : result [ \"second\" ] = str2int ( second ) result . update ( ** kwargs ) return result","title":"`edsnlp.pipelines.misc.dates`"},{"location":"reference/pipelines/misc/dates/#edsnlppipelinesmiscdates","text":"","title":"edsnlp.pipelines.misc.dates"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.parsing","text":"","title":"parsing"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.parsing.str2int","text":"Converts a string to an integer. Returns None if the string cannot be converted. PARAMETER DESCRIPTION time String representation TYPE: str RETURNS DESCRIPTION int Integer conversion. Source code in edsnlp/pipelines/misc/dates/parsing.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def str2int ( time : str ) -> int : \"\"\" Converts a string to an integer. Returns `None` if the string cannot be converted. Parameters ---------- time : str String representation Returns ------- int Integer conversion. \"\"\" try : return int ( time ) except ValueError : return None","title":"str2int()"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.parsing.time2int_factory","text":"Factory for a time2int conversion function. PARAMETER DESCRIPTION patterns Dictionary of conversion/pattern. TYPE: Dict[str, int] RETURNS DESCRIPTION Callable[[str], int] String to integer function. Source code in edsnlp/pipelines/misc/dates/parsing.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def time2int_factory ( patterns : Dict [ str , int ]) -> Callable [[ str ], int ]: \"\"\" Factory for a `time2int` conversion function. Parameters ---------- patterns : Dict[str, int] Dictionary of conversion/pattern. Returns ------- Callable[[str], int] String to integer function. \"\"\" def time2int ( time : str ) -> int : \"\"\" Converts a string representation to the proper integer, iterating over a dictionnary of pattern/conversion. Parameters ---------- time : str String representation Returns ------- int Integer conversion \"\"\" m = str2int ( time ) if m is not None : return m for pattern , key in patterns . items (): if re . match ( f \"^ { pattern } $\" , time ): m = key break return m return time2int","title":"time2int_factory()"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.dates","text":"","title":"dates"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.dates.Dates","text":"Bases: BaseComponent Tags and normalizes dates, using the open-source dateparser library. The pipeline uses spaCy's filter_spans function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: spacy.language.Language absolute List of regular expressions for absolute dates. TYPE: Union[List[str], str] full List of regular expressions for full dates in YYYY-MM-DD format. TYPE: Union[List[str], str] relative List of regular expressions for relative dates (eg hier , la semaine prochaine ). TYPE: Union[List[str], str] no_year List of regular expressions for dates that do not display a year. TYPE: Union[List[str], str] no_day List of regular expressions for dates that do not display a day. TYPE: Union[List[str], str] year_only List of regular expressions for dates that only display a year. TYPE: Union[List[str], str] current List of regular expressions for dates that relate to the current month, week, year, etc. TYPE: Union[List[str], str] false_positive List of regular expressions for false positive (eg phone numbers, etc). TYPE: Union[List[str], str] on_ents_only Wether to look on dates in the whole document or in specific sentences: If True : Only look in the sentences of each entity in doc.ents If False: Look in the whole document If given a string key or list of string: Only look in the sentences of each entity in doc . spans [ key ] TYPE: Union[bool, str, List[str]] Source code in edsnlp/pipelines/misc/dates/dates.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 class Dates ( BaseComponent ): \"\"\" Tags and normalizes dates, using the open-source `dateparser` library. The pipeline uses spaCy's `filter_spans` function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. Parameters ---------- nlp : spacy.language.Language Language pipeline object absolute : Union[List[str], str] List of regular expressions for absolute dates. full : Union[List[str], str] List of regular expressions for full dates in YYYY-MM-DD format. relative : Union[List[str], str] List of regular expressions for relative dates (eg `hier`, `la semaine prochaine`). no_year : Union[List[str], str] List of regular expressions for dates that do not display a year. no_day : Union[List[str], str] List of regular expressions for dates that do not display a day. year_only : Union[List[str], str] List of regular expressions for dates that only display a year. current : Union[List[str], str] List of regular expressions for dates that relate to the current month, week, year, etc. false_positive : Union[List[str], str] List of regular expressions for false positive (eg phone numbers, etc). on_ents_only : Union[bool, str, List[str]] Wether to look on dates in the whole document or in specific sentences: - If `True`: Only look in the sentences of each entity in doc.ents - If False: Look in the whole document - If given a string `key` or list of string: Only look in the sentences of each entity in `#!python doc.spans[key]` \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , absolute : Optional [ List [ str ]], full : Optional [ List [ str ]], relative : Optional [ List [ str ]], no_year : Optional [ List [ str ]], no_day : Optional [ List [ str ]], year_only : Optional [ List [ str ]], current : Optional [ List [ str ]], false_positive : Optional [ List [ str ]], on_ents_only : bool , attr : str , ): self . nlp = nlp if no_year is None : no_year = patterns . no_year_pattern if year_only is None : year_only = patterns . full_year_pattern if no_day is None : no_day = patterns . no_day_pattern if absolute is None : absolute = patterns . absolute_date_pattern if relative is None : relative = patterns . relative_date_pattern if full is None : full = patterns . full_date_pattern if current is None : current = patterns . current_pattern if false_positive is None : false_positive = patterns . false_positive_pattern if isinstance ( absolute , str ): absolute = [ absolute ] if isinstance ( relative , str ): relative = [ relative ] if isinstance ( no_year , str ): no_year = [ no_year ] if isinstance ( no_day , str ): no_day = [ no_day ] if isinstance ( year_only , str ): year_only = [ year_only ] if isinstance ( full , str ): full = [ full ] if isinstance ( current , str ): current = [ current ] if isinstance ( false_positive , str ): false_positive = [ false_positive ] self . on_ents_only = on_ents_only self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"false_positive\" , false_positive ) self . regex_matcher . add ( \"full_date\" , full ) self . regex_matcher . add ( \"absolute\" , absolute ) self . regex_matcher . add ( \"relative\" , relative ) self . regex_matcher . add ( \"no_year\" , no_year ) self . regex_matcher . add ( \"no_day\" , no_day ) self . regex_matcher . add ( \"year_only\" , year_only ) self . regex_matcher . add ( \"current\" , current ) self . parser = date_parser self . set_extensions () @staticmethod def set_extensions () -> None : if not Doc . has_extension ( \"note_datetime\" ): Doc . set_extension ( \"note_datetime\" , default = None ) if not Span . has_extension ( \"parsed_date\" ): Span . set_extension ( \"parsed_date\" , default = None ) if not Span . has_extension ( \"parsed_delta\" ): Span . set_extension ( \"parsed_delta\" , default = None ) if not Span . has_extension ( \"date\" ): Span . set_extension ( \"date\" , getter = date_getter ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , # return_groupdict=True, ), ) else : dates = self . regex_matcher ( doc , as_spans = True , # return_groupdict=True, ) # dates = apply_groupdict(dates) dates = filter_spans ( dates ) dates = [ date for date in dates if date . label_ != \"false_positive\" ] return dates def get_date ( self , date : Span ) -> Optional [ datetime ]: \"\"\" Get normalised date using `dateparser`. Parameters ---------- date : Span Date span. Returns ------- Optional[datetime] If a date is recognised, returns a Python `datetime` object. Returns `None` otherwise. \"\"\" text_date = date . text if date . label_ == \"no_day\" : text_date = \"01/\" + re . sub ( r \"[\\.\\/\\s]\" , \"/\" , text_date ) elif date . label_ == \"full_date\" : text_date = re . sub ( r \"[\\.\\/\\s]\" , \"-\" , text_date ) try : return datetime . strptime ( text_date , \"%Y-%m- %d \" ) except ValueError : try : return datetime . strptime ( text_date , \"%Y- %d -%m\" ) except ValueError : return None # text_date = re.sub(r\"\\.\", \"-\", text_date) return self . parser ( text_date ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) for date in dates : d = self . get_date ( date ) if d is None : date . _ . parsed_date = None else : date . _ . parsed_date = d date . _ . parsed_delta = d - datetime . now () + timedelta ( seconds = 10 ) doc . spans [ \"dates\" ] = dates return doc","title":"Dates"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.dates.Dates.process","text":"Find dates in doc. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION dates list of date spans Source code in edsnlp/pipelines/misc/dates/dates.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , # return_groupdict=True, ), ) else : dates = self . regex_matcher ( doc , as_spans = True , # return_groupdict=True, ) # dates = apply_groupdict(dates) dates = filter_spans ( dates ) dates = [ date for date in dates if date . label_ != \"false_positive\" ] return dates","title":"process()"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.dates.Dates.get_date","text":"Get normalised date using dateparser . PARAMETER DESCRIPTION date Date span. TYPE: Span RETURNS DESCRIPTION Optional[datetime] If a date is recognised, returns a Python datetime object. Returns None otherwise. Source code in edsnlp/pipelines/misc/dates/dates.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def get_date ( self , date : Span ) -> Optional [ datetime ]: \"\"\" Get normalised date using `dateparser`. Parameters ---------- date : Span Date span. Returns ------- Optional[datetime] If a date is recognised, returns a Python `datetime` object. Returns `None` otherwise. \"\"\" text_date = date . text if date . label_ == \"no_day\" : text_date = \"01/\" + re . sub ( r \"[\\.\\/\\s]\" , \"/\" , text_date ) elif date . label_ == \"full_date\" : text_date = re . sub ( r \"[\\.\\/\\s]\" , \"-\" , text_date ) try : return datetime . strptime ( text_date , \"%Y-%m- %d \" ) except ValueError : try : return datetime . strptime ( text_date , \"%Y- %d -%m\" ) except ValueError : return None # text_date = re.sub(r\"\\.\", \"-\", text_date) return self . parser ( text_date )","title":"get_date()"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.dates.Dates.__call__","text":"Tags dates. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for dates Source code in edsnlp/pipelines/misc/dates/dates.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) for date in dates : d = self . get_date ( date ) if d is None : date . _ . parsed_date = None else : date . _ . parsed_date = d date . _ . parsed_delta = d - datetime . now () + timedelta ( seconds = 10 ) doc . spans [ \"dates\" ] = dates return doc","title":"__call__()"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.dates.td2str","text":"Transforms a timedelta object to a string representation. PARAMETER DESCRIPTION td The timedelta object to represent. TYPE: timedelta RETURNS DESCRIPTION str Usable representation for the timedelta object. Source code in edsnlp/pipelines/misc/dates/dates.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def td2str ( td : timedelta ): \"\"\" Transforms a timedelta object to a string representation. Parameters ---------- td : timedelta The timedelta object to represent. Returns ------- str Usable representation for the timedelta object. \"\"\" seconds = td . total_seconds () days = int ( seconds / 3600 / 24 ) return f \"TD { days : +d } \"","title":"td2str()"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.dates.date_getter","text":"Getter for dates. Uses the information from note_datetime . PARAMETER DESCRIPTION date Date detected by the pipeline. TYPE: Span RETURNS DESCRIPTION str Normalized date. Source code in edsnlp/pipelines/misc/dates/dates.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def date_getter ( date : Span ) -> str : \"\"\" Getter for dates. Uses the information from `note_datetime`. Parameters ---------- date : Span Date detected by the pipeline. Returns ------- str Normalized date. \"\"\" d = date . _ . parsed_date if d is None : # dateparser could not interpret the date. return \"????-??-??\" delta = date . _ . parsed_delta note_datetime = date . doc . _ . note_datetime if date . label_ in { \"absolute\" , \"full_date\" , \"no_day\" }: normalized = d . strftime ( \"%Y-%m- %d \" ) elif date . label_ == \"no_year\" : if note_datetime : year = note_datetime . strftime ( \"%Y\" ) else : year = \"????\" normalized = d . strftime ( f \" { year } -%m-%d\" ) else : if note_datetime : # We need to adjust the timedelta, since most dates are set at 00h00. # The slightest difference leads to a day difference. d = note_datetime + delta normalized = d . strftime ( \"%Y-%m- %d \" ) else : normalized = td2str ( d - datetime . now ()) return normalized","title":"date_getter()"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.dates.date_parser","text":"Function to parse dates. It try first all available parsers ('timestamp', 'custom-formats', 'absolute-time') but 'relative-time'. If no date is found, retries with 'relative-time'. When just the year is identified, it returns a datetime object with month and day equal to 1. PARAMETER DESCRIPTION text_date TYPE: str RETURNS DESCRIPTION datetime Source code in edsnlp/pipelines/misc/dates/dates.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def date_parser ( text_date : str ) -> datetime : \"\"\" Function to parse dates. It try first all available parsers ('timestamp', 'custom-formats', 'absolute-time') but 'relative-time'. If no date is found, retries with 'relative-time'. When just the year is identified, it returns a datetime object with month and day equal to 1. Parameters ---------- text_date : str Returns ------- datetime \"\"\" parsed_date = parser1 . get_date_data ( text_date ) if parsed_date . date_obj : if parsed_date . period == \"year\" : return datetime ( year = parsed_date . date_obj . year , month = 1 , day = 1 ) else : return parsed_date . date_obj else : parsed_date2 = parser2 . get_date_data ( text_date ) return parsed_date2 . date_obj","title":"date_parser()"},{"location":"reference/pipelines/misc/dates/#edsnlp.pipelines.misc.dates.dates.parse_groupdict","text":"Parse date groupdict. PARAMETER DESCRIPTION day String representation of the day, by default None TYPE: str, optional DEFAULT: None month String representation of the month, by default None TYPE: str, optional DEFAULT: None year String representation of the year, by default None TYPE: str, optional DEFAULT: None hour String representation of the hour, by default None TYPE: str, optional DEFAULT: None minute String representation of the minute, by default None TYPE: str, optional DEFAULT: None second String representation of the minute, by default None TYPE: str, optional DEFAULT: None RETURNS DESCRIPTION Dict[str, int] Parsed groupdict. Source code in edsnlp/pipelines/misc/dates/dates.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def parse_groupdict ( day : str = None , month : str = None , year : str = None , hour : str = None , minute : str = None , second : str = None , ** kwargs : Dict [ str , str ], ) -> Dict [ str , int ]: \"\"\" Parse date groupdict. Parameters ---------- day : str, optional String representation of the day, by default None month : str, optional String representation of the month, by default None year : str, optional String representation of the year, by default None hour : str, optional String representation of the hour, by default None minute : str, optional String representation of the minute, by default None second : str, optional String representation of the minute, by default None Returns ------- Dict[str, int] Parsed groupdict. \"\"\" result = dict () if day is not None : result [ \"day\" ] = day2int ( day ) if month is not None : result [ \"month\" ] = month2int ( month ) if year is not None : result [ \"year\" ] = str2int ( year ) if hour is not None : result [ \"hour\" ] = str2int ( hour ) if minute is not None : result [ \"minute\" ] = str2int ( minute ) if second is not None : result [ \"second\" ] = str2int ( second ) result . update ( ** kwargs ) return result","title":"parse_groupdict()"},{"location":"reference/pipelines/misc/dates/dates/","text":"edsnlp.pipelines.misc.dates.dates Dates Bases: BaseComponent Tags and normalizes dates, using the open-source dateparser library. The pipeline uses spaCy's filter_spans function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: spacy.language.Language absolute List of regular expressions for absolute dates. TYPE: Union[List[str], str] full List of regular expressions for full dates in YYYY-MM-DD format. TYPE: Union[List[str], str] relative List of regular expressions for relative dates (eg hier , la semaine prochaine ). TYPE: Union[List[str], str] no_year List of regular expressions for dates that do not display a year. TYPE: Union[List[str], str] no_day List of regular expressions for dates that do not display a day. TYPE: Union[List[str], str] year_only List of regular expressions for dates that only display a year. TYPE: Union[List[str], str] current List of regular expressions for dates that relate to the current month, week, year, etc. TYPE: Union[List[str], str] false_positive List of regular expressions for false positive (eg phone numbers, etc). TYPE: Union[List[str], str] on_ents_only Wether to look on dates in the whole document or in specific sentences: If True : Only look in the sentences of each entity in doc.ents If False: Look in the whole document If given a string key or list of string: Only look in the sentences of each entity in doc . spans [ key ] TYPE: Union[bool, str, List[str]] Source code in edsnlp/pipelines/misc/dates/dates.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 class Dates ( BaseComponent ): \"\"\" Tags and normalizes dates, using the open-source `dateparser` library. The pipeline uses spaCy's `filter_spans` function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. Parameters ---------- nlp : spacy.language.Language Language pipeline object absolute : Union[List[str], str] List of regular expressions for absolute dates. full : Union[List[str], str] List of regular expressions for full dates in YYYY-MM-DD format. relative : Union[List[str], str] List of regular expressions for relative dates (eg `hier`, `la semaine prochaine`). no_year : Union[List[str], str] List of regular expressions for dates that do not display a year. no_day : Union[List[str], str] List of regular expressions for dates that do not display a day. year_only : Union[List[str], str] List of regular expressions for dates that only display a year. current : Union[List[str], str] List of regular expressions for dates that relate to the current month, week, year, etc. false_positive : Union[List[str], str] List of regular expressions for false positive (eg phone numbers, etc). on_ents_only : Union[bool, str, List[str]] Wether to look on dates in the whole document or in specific sentences: - If `True`: Only look in the sentences of each entity in doc.ents - If False: Look in the whole document - If given a string `key` or list of string: Only look in the sentences of each entity in `#!python doc.spans[key]` \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , absolute : Optional [ List [ str ]], full : Optional [ List [ str ]], relative : Optional [ List [ str ]], no_year : Optional [ List [ str ]], no_day : Optional [ List [ str ]], year_only : Optional [ List [ str ]], current : Optional [ List [ str ]], false_positive : Optional [ List [ str ]], on_ents_only : bool , attr : str , ): self . nlp = nlp if no_year is None : no_year = patterns . no_year_pattern if year_only is None : year_only = patterns . full_year_pattern if no_day is None : no_day = patterns . no_day_pattern if absolute is None : absolute = patterns . absolute_date_pattern if relative is None : relative = patterns . relative_date_pattern if full is None : full = patterns . full_date_pattern if current is None : current = patterns . current_pattern if false_positive is None : false_positive = patterns . false_positive_pattern if isinstance ( absolute , str ): absolute = [ absolute ] if isinstance ( relative , str ): relative = [ relative ] if isinstance ( no_year , str ): no_year = [ no_year ] if isinstance ( no_day , str ): no_day = [ no_day ] if isinstance ( year_only , str ): year_only = [ year_only ] if isinstance ( full , str ): full = [ full ] if isinstance ( current , str ): current = [ current ] if isinstance ( false_positive , str ): false_positive = [ false_positive ] self . on_ents_only = on_ents_only self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"false_positive\" , false_positive ) self . regex_matcher . add ( \"full_date\" , full ) self . regex_matcher . add ( \"absolute\" , absolute ) self . regex_matcher . add ( \"relative\" , relative ) self . regex_matcher . add ( \"no_year\" , no_year ) self . regex_matcher . add ( \"no_day\" , no_day ) self . regex_matcher . add ( \"year_only\" , year_only ) self . regex_matcher . add ( \"current\" , current ) self . parser = date_parser self . set_extensions () @staticmethod def set_extensions () -> None : if not Doc . has_extension ( \"note_datetime\" ): Doc . set_extension ( \"note_datetime\" , default = None ) if not Span . has_extension ( \"parsed_date\" ): Span . set_extension ( \"parsed_date\" , default = None ) if not Span . has_extension ( \"parsed_delta\" ): Span . set_extension ( \"parsed_delta\" , default = None ) if not Span . has_extension ( \"date\" ): Span . set_extension ( \"date\" , getter = date_getter ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , # return_groupdict=True, ), ) else : dates = self . regex_matcher ( doc , as_spans = True , # return_groupdict=True, ) # dates = apply_groupdict(dates) dates = filter_spans ( dates ) dates = [ date for date in dates if date . label_ != \"false_positive\" ] return dates def get_date ( self , date : Span ) -> Optional [ datetime ]: \"\"\" Get normalised date using `dateparser`. Parameters ---------- date : Span Date span. Returns ------- Optional[datetime] If a date is recognised, returns a Python `datetime` object. Returns `None` otherwise. \"\"\" text_date = date . text if date . label_ == \"no_day\" : text_date = \"01/\" + re . sub ( r \"[\\.\\/\\s]\" , \"/\" , text_date ) elif date . label_ == \"full_date\" : text_date = re . sub ( r \"[\\.\\/\\s]\" , \"-\" , text_date ) try : return datetime . strptime ( text_date , \"%Y-%m- %d \" ) except ValueError : try : return datetime . strptime ( text_date , \"%Y- %d -%m\" ) except ValueError : return None # text_date = re.sub(r\"\\.\", \"-\", text_date) return self . parser ( text_date ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) for date in dates : d = self . get_date ( date ) if d is None : date . _ . parsed_date = None else : date . _ . parsed_date = d date . _ . parsed_delta = d - datetime . now () + timedelta ( seconds = 10 ) doc . spans [ \"dates\" ] = dates return doc process ( doc ) Find dates in doc. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION dates list of date spans Source code in edsnlp/pipelines/misc/dates/dates.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , # return_groupdict=True, ), ) else : dates = self . regex_matcher ( doc , as_spans = True , # return_groupdict=True, ) # dates = apply_groupdict(dates) dates = filter_spans ( dates ) dates = [ date for date in dates if date . label_ != \"false_positive\" ] return dates get_date ( date ) Get normalised date using dateparser . PARAMETER DESCRIPTION date Date span. TYPE: Span RETURNS DESCRIPTION Optional[datetime] If a date is recognised, returns a Python datetime object. Returns None otherwise. Source code in edsnlp/pipelines/misc/dates/dates.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def get_date ( self , date : Span ) -> Optional [ datetime ]: \"\"\" Get normalised date using `dateparser`. Parameters ---------- date : Span Date span. Returns ------- Optional[datetime] If a date is recognised, returns a Python `datetime` object. Returns `None` otherwise. \"\"\" text_date = date . text if date . label_ == \"no_day\" : text_date = \"01/\" + re . sub ( r \"[\\.\\/\\s]\" , \"/\" , text_date ) elif date . label_ == \"full_date\" : text_date = re . sub ( r \"[\\.\\/\\s]\" , \"-\" , text_date ) try : return datetime . strptime ( text_date , \"%Y-%m- %d \" ) except ValueError : try : return datetime . strptime ( text_date , \"%Y- %d -%m\" ) except ValueError : return None # text_date = re.sub(r\"\\.\", \"-\", text_date) return self . parser ( text_date ) __call__ ( doc ) Tags dates. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for dates Source code in edsnlp/pipelines/misc/dates/dates.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) for date in dates : d = self . get_date ( date ) if d is None : date . _ . parsed_date = None else : date . _ . parsed_date = d date . _ . parsed_delta = d - datetime . now () + timedelta ( seconds = 10 ) doc . spans [ \"dates\" ] = dates return doc td2str ( td ) Transforms a timedelta object to a string representation. PARAMETER DESCRIPTION td The timedelta object to represent. TYPE: timedelta RETURNS DESCRIPTION str Usable representation for the timedelta object. Source code in edsnlp/pipelines/misc/dates/dates.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def td2str ( td : timedelta ): \"\"\" Transforms a timedelta object to a string representation. Parameters ---------- td : timedelta The timedelta object to represent. Returns ------- str Usable representation for the timedelta object. \"\"\" seconds = td . total_seconds () days = int ( seconds / 3600 / 24 ) return f \"TD { days : +d } \" date_getter ( date ) Getter for dates. Uses the information from note_datetime . PARAMETER DESCRIPTION date Date detected by the pipeline. TYPE: Span RETURNS DESCRIPTION str Normalized date. Source code in edsnlp/pipelines/misc/dates/dates.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def date_getter ( date : Span ) -> str : \"\"\" Getter for dates. Uses the information from `note_datetime`. Parameters ---------- date : Span Date detected by the pipeline. Returns ------- str Normalized date. \"\"\" d = date . _ . parsed_date if d is None : # dateparser could not interpret the date. return \"????-??-??\" delta = date . _ . parsed_delta note_datetime = date . doc . _ . note_datetime if date . label_ in { \"absolute\" , \"full_date\" , \"no_day\" }: normalized = d . strftime ( \"%Y-%m- %d \" ) elif date . label_ == \"no_year\" : if note_datetime : year = note_datetime . strftime ( \"%Y\" ) else : year = \"????\" normalized = d . strftime ( f \" { year } -%m-%d\" ) else : if note_datetime : # We need to adjust the timedelta, since most dates are set at 00h00. # The slightest difference leads to a day difference. d = note_datetime + delta normalized = d . strftime ( \"%Y-%m- %d \" ) else : normalized = td2str ( d - datetime . now ()) return normalized date_parser ( text_date ) Function to parse dates. It try first all available parsers ('timestamp', 'custom-formats', 'absolute-time') but 'relative-time'. If no date is found, retries with 'relative-time'. When just the year is identified, it returns a datetime object with month and day equal to 1. PARAMETER DESCRIPTION text_date TYPE: str RETURNS DESCRIPTION datetime Source code in edsnlp/pipelines/misc/dates/dates.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def date_parser ( text_date : str ) -> datetime : \"\"\" Function to parse dates. It try first all available parsers ('timestamp', 'custom-formats', 'absolute-time') but 'relative-time'. If no date is found, retries with 'relative-time'. When just the year is identified, it returns a datetime object with month and day equal to 1. Parameters ---------- text_date : str Returns ------- datetime \"\"\" parsed_date = parser1 . get_date_data ( text_date ) if parsed_date . date_obj : if parsed_date . period == \"year\" : return datetime ( year = parsed_date . date_obj . year , month = 1 , day = 1 ) else : return parsed_date . date_obj else : parsed_date2 = parser2 . get_date_data ( text_date ) return parsed_date2 . date_obj parse_groupdict ( day = None , month = None , year = None , hour = None , minute = None , second = None , ** kwargs ) Parse date groupdict. PARAMETER DESCRIPTION day String representation of the day, by default None TYPE: str, optional DEFAULT: None month String representation of the month, by default None TYPE: str, optional DEFAULT: None year String representation of the year, by default None TYPE: str, optional DEFAULT: None hour String representation of the hour, by default None TYPE: str, optional DEFAULT: None minute String representation of the minute, by default None TYPE: str, optional DEFAULT: None second String representation of the minute, by default None TYPE: str, optional DEFAULT: None RETURNS DESCRIPTION Dict[str, int] Parsed groupdict. Source code in edsnlp/pipelines/misc/dates/dates.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def parse_groupdict ( day : str = None , month : str = None , year : str = None , hour : str = None , minute : str = None , second : str = None , ** kwargs : Dict [ str , str ], ) -> Dict [ str , int ]: \"\"\" Parse date groupdict. Parameters ---------- day : str, optional String representation of the day, by default None month : str, optional String representation of the month, by default None year : str, optional String representation of the year, by default None hour : str, optional String representation of the hour, by default None minute : str, optional String representation of the minute, by default None second : str, optional String representation of the minute, by default None Returns ------- Dict[str, int] Parsed groupdict. \"\"\" result = dict () if day is not None : result [ \"day\" ] = day2int ( day ) if month is not None : result [ \"month\" ] = month2int ( month ) if year is not None : result [ \"year\" ] = str2int ( year ) if hour is not None : result [ \"hour\" ] = str2int ( hour ) if minute is not None : result [ \"minute\" ] = str2int ( minute ) if second is not None : result [ \"second\" ] = str2int ( second ) result . update ( ** kwargs ) return result","title":"dates"},{"location":"reference/pipelines/misc/dates/dates/#edsnlppipelinesmiscdatesdates","text":"","title":"edsnlp.pipelines.misc.dates.dates"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates","text":"Bases: BaseComponent Tags and normalizes dates, using the open-source dateparser library. The pipeline uses spaCy's filter_spans function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: spacy.language.Language absolute List of regular expressions for absolute dates. TYPE: Union[List[str], str] full List of regular expressions for full dates in YYYY-MM-DD format. TYPE: Union[List[str], str] relative List of regular expressions for relative dates (eg hier , la semaine prochaine ). TYPE: Union[List[str], str] no_year List of regular expressions for dates that do not display a year. TYPE: Union[List[str], str] no_day List of regular expressions for dates that do not display a day. TYPE: Union[List[str], str] year_only List of regular expressions for dates that only display a year. TYPE: Union[List[str], str] current List of regular expressions for dates that relate to the current month, week, year, etc. TYPE: Union[List[str], str] false_positive List of regular expressions for false positive (eg phone numbers, etc). TYPE: Union[List[str], str] on_ents_only Wether to look on dates in the whole document or in specific sentences: If True : Only look in the sentences of each entity in doc.ents If False: Look in the whole document If given a string key or list of string: Only look in the sentences of each entity in doc . spans [ key ] TYPE: Union[bool, str, List[str]] Source code in edsnlp/pipelines/misc/dates/dates.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 class Dates ( BaseComponent ): \"\"\" Tags and normalizes dates, using the open-source `dateparser` library. The pipeline uses spaCy's `filter_spans` function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. Parameters ---------- nlp : spacy.language.Language Language pipeline object absolute : Union[List[str], str] List of regular expressions for absolute dates. full : Union[List[str], str] List of regular expressions for full dates in YYYY-MM-DD format. relative : Union[List[str], str] List of regular expressions for relative dates (eg `hier`, `la semaine prochaine`). no_year : Union[List[str], str] List of regular expressions for dates that do not display a year. no_day : Union[List[str], str] List of regular expressions for dates that do not display a day. year_only : Union[List[str], str] List of regular expressions for dates that only display a year. current : Union[List[str], str] List of regular expressions for dates that relate to the current month, week, year, etc. false_positive : Union[List[str], str] List of regular expressions for false positive (eg phone numbers, etc). on_ents_only : Union[bool, str, List[str]] Wether to look on dates in the whole document or in specific sentences: - If `True`: Only look in the sentences of each entity in doc.ents - If False: Look in the whole document - If given a string `key` or list of string: Only look in the sentences of each entity in `#!python doc.spans[key]` \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , absolute : Optional [ List [ str ]], full : Optional [ List [ str ]], relative : Optional [ List [ str ]], no_year : Optional [ List [ str ]], no_day : Optional [ List [ str ]], year_only : Optional [ List [ str ]], current : Optional [ List [ str ]], false_positive : Optional [ List [ str ]], on_ents_only : bool , attr : str , ): self . nlp = nlp if no_year is None : no_year = patterns . no_year_pattern if year_only is None : year_only = patterns . full_year_pattern if no_day is None : no_day = patterns . no_day_pattern if absolute is None : absolute = patterns . absolute_date_pattern if relative is None : relative = patterns . relative_date_pattern if full is None : full = patterns . full_date_pattern if current is None : current = patterns . current_pattern if false_positive is None : false_positive = patterns . false_positive_pattern if isinstance ( absolute , str ): absolute = [ absolute ] if isinstance ( relative , str ): relative = [ relative ] if isinstance ( no_year , str ): no_year = [ no_year ] if isinstance ( no_day , str ): no_day = [ no_day ] if isinstance ( year_only , str ): year_only = [ year_only ] if isinstance ( full , str ): full = [ full ] if isinstance ( current , str ): current = [ current ] if isinstance ( false_positive , str ): false_positive = [ false_positive ] self . on_ents_only = on_ents_only self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"false_positive\" , false_positive ) self . regex_matcher . add ( \"full_date\" , full ) self . regex_matcher . add ( \"absolute\" , absolute ) self . regex_matcher . add ( \"relative\" , relative ) self . regex_matcher . add ( \"no_year\" , no_year ) self . regex_matcher . add ( \"no_day\" , no_day ) self . regex_matcher . add ( \"year_only\" , year_only ) self . regex_matcher . add ( \"current\" , current ) self . parser = date_parser self . set_extensions () @staticmethod def set_extensions () -> None : if not Doc . has_extension ( \"note_datetime\" ): Doc . set_extension ( \"note_datetime\" , default = None ) if not Span . has_extension ( \"parsed_date\" ): Span . set_extension ( \"parsed_date\" , default = None ) if not Span . has_extension ( \"parsed_delta\" ): Span . set_extension ( \"parsed_delta\" , default = None ) if not Span . has_extension ( \"date\" ): Span . set_extension ( \"date\" , getter = date_getter ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , # return_groupdict=True, ), ) else : dates = self . regex_matcher ( doc , as_spans = True , # return_groupdict=True, ) # dates = apply_groupdict(dates) dates = filter_spans ( dates ) dates = [ date for date in dates if date . label_ != \"false_positive\" ] return dates def get_date ( self , date : Span ) -> Optional [ datetime ]: \"\"\" Get normalised date using `dateparser`. Parameters ---------- date : Span Date span. Returns ------- Optional[datetime] If a date is recognised, returns a Python `datetime` object. Returns `None` otherwise. \"\"\" text_date = date . text if date . label_ == \"no_day\" : text_date = \"01/\" + re . sub ( r \"[\\.\\/\\s]\" , \"/\" , text_date ) elif date . label_ == \"full_date\" : text_date = re . sub ( r \"[\\.\\/\\s]\" , \"-\" , text_date ) try : return datetime . strptime ( text_date , \"%Y-%m- %d \" ) except ValueError : try : return datetime . strptime ( text_date , \"%Y- %d -%m\" ) except ValueError : return None # text_date = re.sub(r\"\\.\", \"-\", text_date) return self . parser ( text_date ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) for date in dates : d = self . get_date ( date ) if d is None : date . _ . parsed_date = None else : date . _ . parsed_date = d date . _ . parsed_delta = d - datetime . now () + timedelta ( seconds = 10 ) doc . spans [ \"dates\" ] = dates return doc","title":"Dates"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.process","text":"Find dates in doc. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION dates list of date spans Source code in edsnlp/pipelines/misc/dates/dates.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , # return_groupdict=True, ), ) else : dates = self . regex_matcher ( doc , as_spans = True , # return_groupdict=True, ) # dates = apply_groupdict(dates) dates = filter_spans ( dates ) dates = [ date for date in dates if date . label_ != \"false_positive\" ] return dates","title":"process()"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.get_date","text":"Get normalised date using dateparser . PARAMETER DESCRIPTION date Date span. TYPE: Span RETURNS DESCRIPTION Optional[datetime] If a date is recognised, returns a Python datetime object. Returns None otherwise. Source code in edsnlp/pipelines/misc/dates/dates.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def get_date ( self , date : Span ) -> Optional [ datetime ]: \"\"\" Get normalised date using `dateparser`. Parameters ---------- date : Span Date span. Returns ------- Optional[datetime] If a date is recognised, returns a Python `datetime` object. Returns `None` otherwise. \"\"\" text_date = date . text if date . label_ == \"no_day\" : text_date = \"01/\" + re . sub ( r \"[\\.\\/\\s]\" , \"/\" , text_date ) elif date . label_ == \"full_date\" : text_date = re . sub ( r \"[\\.\\/\\s]\" , \"-\" , text_date ) try : return datetime . strptime ( text_date , \"%Y-%m- %d \" ) except ValueError : try : return datetime . strptime ( text_date , \"%Y- %d -%m\" ) except ValueError : return None # text_date = re.sub(r\"\\.\", \"-\", text_date) return self . parser ( text_date )","title":"get_date()"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.__call__","text":"Tags dates. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for dates Source code in edsnlp/pipelines/misc/dates/dates.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) for date in dates : d = self . get_date ( date ) if d is None : date . _ . parsed_date = None else : date . _ . parsed_date = d date . _ . parsed_delta = d - datetime . now () + timedelta ( seconds = 10 ) doc . spans [ \"dates\" ] = dates return doc","title":"__call__()"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.td2str","text":"Transforms a timedelta object to a string representation. PARAMETER DESCRIPTION td The timedelta object to represent. TYPE: timedelta RETURNS DESCRIPTION str Usable representation for the timedelta object. Source code in edsnlp/pipelines/misc/dates/dates.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def td2str ( td : timedelta ): \"\"\" Transforms a timedelta object to a string representation. Parameters ---------- td : timedelta The timedelta object to represent. Returns ------- str Usable representation for the timedelta object. \"\"\" seconds = td . total_seconds () days = int ( seconds / 3600 / 24 ) return f \"TD { days : +d } \"","title":"td2str()"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.date_getter","text":"Getter for dates. Uses the information from note_datetime . PARAMETER DESCRIPTION date Date detected by the pipeline. TYPE: Span RETURNS DESCRIPTION str Normalized date. Source code in edsnlp/pipelines/misc/dates/dates.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def date_getter ( date : Span ) -> str : \"\"\" Getter for dates. Uses the information from `note_datetime`. Parameters ---------- date : Span Date detected by the pipeline. Returns ------- str Normalized date. \"\"\" d = date . _ . parsed_date if d is None : # dateparser could not interpret the date. return \"????-??-??\" delta = date . _ . parsed_delta note_datetime = date . doc . _ . note_datetime if date . label_ in { \"absolute\" , \"full_date\" , \"no_day\" }: normalized = d . strftime ( \"%Y-%m- %d \" ) elif date . label_ == \"no_year\" : if note_datetime : year = note_datetime . strftime ( \"%Y\" ) else : year = \"????\" normalized = d . strftime ( f \" { year } -%m-%d\" ) else : if note_datetime : # We need to adjust the timedelta, since most dates are set at 00h00. # The slightest difference leads to a day difference. d = note_datetime + delta normalized = d . strftime ( \"%Y-%m- %d \" ) else : normalized = td2str ( d - datetime . now ()) return normalized","title":"date_getter()"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.date_parser","text":"Function to parse dates. It try first all available parsers ('timestamp', 'custom-formats', 'absolute-time') but 'relative-time'. If no date is found, retries with 'relative-time'. When just the year is identified, it returns a datetime object with month and day equal to 1. PARAMETER DESCRIPTION text_date TYPE: str RETURNS DESCRIPTION datetime Source code in edsnlp/pipelines/misc/dates/dates.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def date_parser ( text_date : str ) -> datetime : \"\"\" Function to parse dates. It try first all available parsers ('timestamp', 'custom-formats', 'absolute-time') but 'relative-time'. If no date is found, retries with 'relative-time'. When just the year is identified, it returns a datetime object with month and day equal to 1. Parameters ---------- text_date : str Returns ------- datetime \"\"\" parsed_date = parser1 . get_date_data ( text_date ) if parsed_date . date_obj : if parsed_date . period == \"year\" : return datetime ( year = parsed_date . date_obj . year , month = 1 , day = 1 ) else : return parsed_date . date_obj else : parsed_date2 = parser2 . get_date_data ( text_date ) return parsed_date2 . date_obj","title":"date_parser()"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.parse_groupdict","text":"Parse date groupdict. PARAMETER DESCRIPTION day String representation of the day, by default None TYPE: str, optional DEFAULT: None month String representation of the month, by default None TYPE: str, optional DEFAULT: None year String representation of the year, by default None TYPE: str, optional DEFAULT: None hour String representation of the hour, by default None TYPE: str, optional DEFAULT: None minute String representation of the minute, by default None TYPE: str, optional DEFAULT: None second String representation of the minute, by default None TYPE: str, optional DEFAULT: None RETURNS DESCRIPTION Dict[str, int] Parsed groupdict. Source code in edsnlp/pipelines/misc/dates/dates.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def parse_groupdict ( day : str = None , month : str = None , year : str = None , hour : str = None , minute : str = None , second : str = None , ** kwargs : Dict [ str , str ], ) -> Dict [ str , int ]: \"\"\" Parse date groupdict. Parameters ---------- day : str, optional String representation of the day, by default None month : str, optional String representation of the month, by default None year : str, optional String representation of the year, by default None hour : str, optional String representation of the hour, by default None minute : str, optional String representation of the minute, by default None second : str, optional String representation of the minute, by default None Returns ------- Dict[str, int] Parsed groupdict. \"\"\" result = dict () if day is not None : result [ \"day\" ] = day2int ( day ) if month is not None : result [ \"month\" ] = month2int ( month ) if year is not None : result [ \"year\" ] = str2int ( year ) if hour is not None : result [ \"hour\" ] = str2int ( hour ) if minute is not None : result [ \"minute\" ] = str2int ( minute ) if second is not None : result [ \"second\" ] = str2int ( second ) result . update ( ** kwargs ) return result","title":"parse_groupdict()"},{"location":"reference/pipelines/misc/dates/factory/","text":"edsnlp.pipelines.misc.dates.factory","title":"factory"},{"location":"reference/pipelines/misc/dates/factory/#edsnlppipelinesmiscdatesfactory","text":"","title":"edsnlp.pipelines.misc.dates.factory"},{"location":"reference/pipelines/misc/dates/parsing/","text":"edsnlp.pipelines.misc.dates.parsing str2int ( time ) Converts a string to an integer. Returns None if the string cannot be converted. PARAMETER DESCRIPTION time String representation TYPE: str RETURNS DESCRIPTION int Integer conversion. Source code in edsnlp/pipelines/misc/dates/parsing.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def str2int ( time : str ) -> int : \"\"\" Converts a string to an integer. Returns `None` if the string cannot be converted. Parameters ---------- time : str String representation Returns ------- int Integer conversion. \"\"\" try : return int ( time ) except ValueError : return None time2int_factory ( patterns ) Factory for a time2int conversion function. PARAMETER DESCRIPTION patterns Dictionary of conversion/pattern. TYPE: Dict[str, int] RETURNS DESCRIPTION Callable[[str], int] String to integer function. Source code in edsnlp/pipelines/misc/dates/parsing.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def time2int_factory ( patterns : Dict [ str , int ]) -> Callable [[ str ], int ]: \"\"\" Factory for a `time2int` conversion function. Parameters ---------- patterns : Dict[str, int] Dictionary of conversion/pattern. Returns ------- Callable[[str], int] String to integer function. \"\"\" def time2int ( time : str ) -> int : \"\"\" Converts a string representation to the proper integer, iterating over a dictionnary of pattern/conversion. Parameters ---------- time : str String representation Returns ------- int Integer conversion \"\"\" m = str2int ( time ) if m is not None : return m for pattern , key in patterns . items (): if re . match ( f \"^ { pattern } $\" , time ): m = key break return m return time2int","title":"parsing"},{"location":"reference/pipelines/misc/dates/parsing/#edsnlppipelinesmiscdatesparsing","text":"","title":"edsnlp.pipelines.misc.dates.parsing"},{"location":"reference/pipelines/misc/dates/parsing/#edsnlp.pipelines.misc.dates.parsing.str2int","text":"Converts a string to an integer. Returns None if the string cannot be converted. PARAMETER DESCRIPTION time String representation TYPE: str RETURNS DESCRIPTION int Integer conversion. Source code in edsnlp/pipelines/misc/dates/parsing.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def str2int ( time : str ) -> int : \"\"\" Converts a string to an integer. Returns `None` if the string cannot be converted. Parameters ---------- time : str String representation Returns ------- int Integer conversion. \"\"\" try : return int ( time ) except ValueError : return None","title":"str2int()"},{"location":"reference/pipelines/misc/dates/parsing/#edsnlp.pipelines.misc.dates.parsing.time2int_factory","text":"Factory for a time2int conversion function. PARAMETER DESCRIPTION patterns Dictionary of conversion/pattern. TYPE: Dict[str, int] RETURNS DESCRIPTION Callable[[str], int] String to integer function. Source code in edsnlp/pipelines/misc/dates/parsing.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def time2int_factory ( patterns : Dict [ str , int ]) -> Callable [[ str ], int ]: \"\"\" Factory for a `time2int` conversion function. Parameters ---------- patterns : Dict[str, int] Dictionary of conversion/pattern. Returns ------- Callable[[str], int] String to integer function. \"\"\" def time2int ( time : str ) -> int : \"\"\" Converts a string representation to the proper integer, iterating over a dictionnary of pattern/conversion. Parameters ---------- time : str String representation Returns ------- int Integer conversion \"\"\" m = str2int ( time ) if m is not None : return m for pattern , key in patterns . items (): if re . match ( f \"^ { pattern } $\" , time ): m = key break return m return time2int","title":"time2int_factory()"},{"location":"reference/pipelines/misc/dates/patterns/","text":"edsnlp.pipelines.misc.dates.patterns","title":"`edsnlp.pipelines.misc.dates.patterns`"},{"location":"reference/pipelines/misc/dates/patterns/#edsnlppipelinesmiscdatespatterns","text":"","title":"edsnlp.pipelines.misc.dates.patterns"},{"location":"reference/pipelines/misc/dates/patterns/current/","text":"edsnlp.pipelines.misc.dates.patterns.current","title":"current"},{"location":"reference/pipelines/misc/dates/patterns/current/#edsnlppipelinesmiscdatespatternscurrent","text":"","title":"edsnlp.pipelines.misc.dates.patterns.current"},{"location":"reference/pipelines/misc/dates/patterns/relative/","text":"edsnlp.pipelines.misc.dates.patterns.relative","title":"relative"},{"location":"reference/pipelines/misc/dates/patterns/relative/#edsnlppipelinesmiscdatespatternsrelative","text":"","title":"edsnlp.pipelines.misc.dates.patterns.relative"},{"location":"reference/pipelines/misc/dates/patterns/atomic/","text":"edsnlp.pipelines.misc.dates.patterns.atomic","title":"`edsnlp.pipelines.misc.dates.patterns.atomic`"},{"location":"reference/pipelines/misc/dates/patterns/atomic/#edsnlppipelinesmiscdatespatternsatomic","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic"},{"location":"reference/pipelines/misc/dates/patterns/atomic/days/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.days","title":"days"},{"location":"reference/pipelines/misc/dates/patterns/atomic/days/#edsnlppipelinesmiscdatespatternsatomicdays","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.days"},{"location":"reference/pipelines/misc/dates/patterns/atomic/months/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.months","title":"months"},{"location":"reference/pipelines/misc/dates/patterns/atomic/months/#edsnlppipelinesmiscdatespatternsatomicmonths","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.months"},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.time","title":"time"},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/#edsnlppipelinesmiscdatespatternsatomictime","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.time"},{"location":"reference/pipelines/misc/dates/patterns/atomic/years/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.years","title":"years"},{"location":"reference/pipelines/misc/dates/patterns/atomic/years/#edsnlppipelinesmiscdatespatternsatomicyears","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.years"},{"location":"reference/pipelines/misc/reason/","text":"edsnlp.pipelines.misc.reason","title":"`edsnlp.pipelines.misc.reason`"},{"location":"reference/pipelines/misc/reason/#edsnlppipelinesmiscreason","text":"","title":"edsnlp.pipelines.misc.reason"},{"location":"reference/pipelines/misc/reason/factory/","text":"edsnlp.pipelines.misc.reason.factory","title":"factory"},{"location":"reference/pipelines/misc/reason/factory/#edsnlppipelinesmiscreasonfactory","text":"","title":"edsnlp.pipelines.misc.reason.factory"},{"location":"reference/pipelines/misc/reason/patterns/","text":"edsnlp.pipelines.misc.reason.patterns","title":"patterns"},{"location":"reference/pipelines/misc/reason/patterns/#edsnlppipelinesmiscreasonpatterns","text":"","title":"edsnlp.pipelines.misc.reason.patterns"},{"location":"reference/pipelines/misc/reason/reason/","text":"edsnlp.pipelines.misc.reason.reason Reason Bases: GenericMatcher Pipeline to identify the reason of the hospitalisation. It declares a Span extension called ents_reason and adds the key reasons to doc.spans. It also declares the boolean extension is_reason . This extension is set to True for the Reason Spans but also for the entities that overlap the reason span. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language reasons The terminology of reasons. TYPE: Optional[Dict[str, Union[List[str], str]]] attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex. TYPE: str use_sections whether or not use the sections pipeline to improve results. TYPE: bool, ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/misc/reason/reason.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Reason ( GenericMatcher ): \"\"\"Pipeline to identify the reason of the hospitalisation. It declares a Span extension called `ents_reason` and adds the key `reasons` to doc.spans. It also declares the boolean extension `is_reason`. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. reasons : Optional[Dict[str, Union[List[str], str]]] The terminology of reasons. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex. use_sections : bool, whether or not use the `sections` pipeline to improve results. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , reasons : Optional [ Dict [ str , Union [ List [ str ], str ]]], attr : Union [ Dict [ str , str ], str ], use_sections : bool , ignore_excluded : bool , ): if reasons is None : reasons = patterns . reasons super () . __init__ ( nlp , terms = None , regex = reasons , attr = attr , ignore_excluded = ignore_excluded , ) self . use_sections = use_sections and ( \"eds.sections\" in self . nlp . pipe_names or \"sections\" in self . nlp . pipe_names ) if use_sections and not self . use_sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `eds.section` pipeline, but it was not set. \" \"Skipping that step.\" ) self . set_extensions () @staticmethod def set_extensions () -> None : if not Span . has_extension ( \"ents_reason\" ): Span . set_extension ( \"ents_reason\" , default = None ) if not Span . has_extension ( \"is_reason\" ): Span . set_extension ( \"is_reason\" , default = False ) def _enhance_with_sections ( self , sections : Iterable , reasons : Iterable ) -> List : \"\"\"Enhance the list of reasons with the section information. If the reason overlaps with history, so it will be removed from the list Parameters ---------- sections : Iterable Spans of sections identified with the `sections` pipeline reasons : Iterable Reasons list identified by the regex Returns ------- List Updated list of spans reasons \"\"\" for section in sections : if section . label_ in patterns . sections_reason : reasons . append ( section ) if section . label_ in patterns . section_exclude : for reason in reasons : if check_inclusion ( reason , section . start , section . end ): reasons . remove ( reason ) return reasons def __call__ ( self , doc : Doc ) -> Doc : \"\"\"Find spans related to the reasons of the hospitalisation Parameters ---------- doc : Doc Returns ------- Doc \"\"\" matches = self . process ( doc ) reasons = get_spans ( matches , \"reasons\" ) if self . use_sections : sections = doc . spans [ \"sections\" ] reasons = self . _enhance_with_sections ( sections = sections , reasons = reasons ) doc . spans [ \"reasons\" ] = reasons # Entities if len ( doc . ents ) > 0 : for reason in reasons : # TODO optimize this iteration ent_list = [] for ent in doc . ents : if check_inclusion ( ent , reason . start , reason . end ): ent_list . append ( ent ) ent . _ . is_reason = True reason . _ . ents_reason = ent_list reason . _ . is_reason = True return doc _enhance_with_sections ( sections , reasons ) Enhance the list of reasons with the section information. If the reason overlaps with history, so it will be removed from the list PARAMETER DESCRIPTION sections Spans of sections identified with the sections pipeline TYPE: Iterable reasons Reasons list identified by the regex TYPE: Iterable RETURNS DESCRIPTION List Updated list of spans reasons Source code in edsnlp/pipelines/misc/reason/reason.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def _enhance_with_sections ( self , sections : Iterable , reasons : Iterable ) -> List : \"\"\"Enhance the list of reasons with the section information. If the reason overlaps with history, so it will be removed from the list Parameters ---------- sections : Iterable Spans of sections identified with the `sections` pipeline reasons : Iterable Reasons list identified by the regex Returns ------- List Updated list of spans reasons \"\"\" for section in sections : if section . label_ in patterns . sections_reason : reasons . append ( section ) if section . label_ in patterns . section_exclude : for reason in reasons : if check_inclusion ( reason , section . start , section . end ): reasons . remove ( reason ) return reasons __call__ ( doc ) Find spans related to the reasons of the hospitalisation PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION Doc Source code in edsnlp/pipelines/misc/reason/reason.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __call__ ( self , doc : Doc ) -> Doc : \"\"\"Find spans related to the reasons of the hospitalisation Parameters ---------- doc : Doc Returns ------- Doc \"\"\" matches = self . process ( doc ) reasons = get_spans ( matches , \"reasons\" ) if self . use_sections : sections = doc . spans [ \"sections\" ] reasons = self . _enhance_with_sections ( sections = sections , reasons = reasons ) doc . spans [ \"reasons\" ] = reasons # Entities if len ( doc . ents ) > 0 : for reason in reasons : # TODO optimize this iteration ent_list = [] for ent in doc . ents : if check_inclusion ( ent , reason . start , reason . end ): ent_list . append ( ent ) ent . _ . is_reason = True reason . _ . ents_reason = ent_list reason . _ . is_reason = True return doc","title":"reason"},{"location":"reference/pipelines/misc/reason/reason/#edsnlppipelinesmiscreasonreason","text":"","title":"edsnlp.pipelines.misc.reason.reason"},{"location":"reference/pipelines/misc/reason/reason/#edsnlp.pipelines.misc.reason.reason.Reason","text":"Bases: GenericMatcher Pipeline to identify the reason of the hospitalisation. It declares a Span extension called ents_reason and adds the key reasons to doc.spans. It also declares the boolean extension is_reason . This extension is set to True for the Reason Spans but also for the entities that overlap the reason span. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language reasons The terminology of reasons. TYPE: Optional[Dict[str, Union[List[str], str]]] attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex. TYPE: str use_sections whether or not use the sections pipeline to improve results. TYPE: bool, ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/misc/reason/reason.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Reason ( GenericMatcher ): \"\"\"Pipeline to identify the reason of the hospitalisation. It declares a Span extension called `ents_reason` and adds the key `reasons` to doc.spans. It also declares the boolean extension `is_reason`. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. reasons : Optional[Dict[str, Union[List[str], str]]] The terminology of reasons. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex. use_sections : bool, whether or not use the `sections` pipeline to improve results. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , reasons : Optional [ Dict [ str , Union [ List [ str ], str ]]], attr : Union [ Dict [ str , str ], str ], use_sections : bool , ignore_excluded : bool , ): if reasons is None : reasons = patterns . reasons super () . __init__ ( nlp , terms = None , regex = reasons , attr = attr , ignore_excluded = ignore_excluded , ) self . use_sections = use_sections and ( \"eds.sections\" in self . nlp . pipe_names or \"sections\" in self . nlp . pipe_names ) if use_sections and not self . use_sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `eds.section` pipeline, but it was not set. \" \"Skipping that step.\" ) self . set_extensions () @staticmethod def set_extensions () -> None : if not Span . has_extension ( \"ents_reason\" ): Span . set_extension ( \"ents_reason\" , default = None ) if not Span . has_extension ( \"is_reason\" ): Span . set_extension ( \"is_reason\" , default = False ) def _enhance_with_sections ( self , sections : Iterable , reasons : Iterable ) -> List : \"\"\"Enhance the list of reasons with the section information. If the reason overlaps with history, so it will be removed from the list Parameters ---------- sections : Iterable Spans of sections identified with the `sections` pipeline reasons : Iterable Reasons list identified by the regex Returns ------- List Updated list of spans reasons \"\"\" for section in sections : if section . label_ in patterns . sections_reason : reasons . append ( section ) if section . label_ in patterns . section_exclude : for reason in reasons : if check_inclusion ( reason , section . start , section . end ): reasons . remove ( reason ) return reasons def __call__ ( self , doc : Doc ) -> Doc : \"\"\"Find spans related to the reasons of the hospitalisation Parameters ---------- doc : Doc Returns ------- Doc \"\"\" matches = self . process ( doc ) reasons = get_spans ( matches , \"reasons\" ) if self . use_sections : sections = doc . spans [ \"sections\" ] reasons = self . _enhance_with_sections ( sections = sections , reasons = reasons ) doc . spans [ \"reasons\" ] = reasons # Entities if len ( doc . ents ) > 0 : for reason in reasons : # TODO optimize this iteration ent_list = [] for ent in doc . ents : if check_inclusion ( ent , reason . start , reason . end ): ent_list . append ( ent ) ent . _ . is_reason = True reason . _ . ents_reason = ent_list reason . _ . is_reason = True return doc","title":"Reason"},{"location":"reference/pipelines/misc/reason/reason/#edsnlp.pipelines.misc.reason.reason.Reason._enhance_with_sections","text":"Enhance the list of reasons with the section information. If the reason overlaps with history, so it will be removed from the list PARAMETER DESCRIPTION sections Spans of sections identified with the sections pipeline TYPE: Iterable reasons Reasons list identified by the regex TYPE: Iterable RETURNS DESCRIPTION List Updated list of spans reasons Source code in edsnlp/pipelines/misc/reason/reason.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def _enhance_with_sections ( self , sections : Iterable , reasons : Iterable ) -> List : \"\"\"Enhance the list of reasons with the section information. If the reason overlaps with history, so it will be removed from the list Parameters ---------- sections : Iterable Spans of sections identified with the `sections` pipeline reasons : Iterable Reasons list identified by the regex Returns ------- List Updated list of spans reasons \"\"\" for section in sections : if section . label_ in patterns . sections_reason : reasons . append ( section ) if section . label_ in patterns . section_exclude : for reason in reasons : if check_inclusion ( reason , section . start , section . end ): reasons . remove ( reason ) return reasons","title":"_enhance_with_sections()"},{"location":"reference/pipelines/misc/reason/reason/#edsnlp.pipelines.misc.reason.reason.Reason.__call__","text":"Find spans related to the reasons of the hospitalisation PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION Doc Source code in edsnlp/pipelines/misc/reason/reason.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __call__ ( self , doc : Doc ) -> Doc : \"\"\"Find spans related to the reasons of the hospitalisation Parameters ---------- doc : Doc Returns ------- Doc \"\"\" matches = self . process ( doc ) reasons = get_spans ( matches , \"reasons\" ) if self . use_sections : sections = doc . spans [ \"sections\" ] reasons = self . _enhance_with_sections ( sections = sections , reasons = reasons ) doc . spans [ \"reasons\" ] = reasons # Entities if len ( doc . ents ) > 0 : for reason in reasons : # TODO optimize this iteration ent_list = [] for ent in doc . ents : if check_inclusion ( ent , reason . start , reason . end ): ent_list . append ( ent ) ent . _ . is_reason = True reason . _ . ents_reason = ent_list reason . _ . is_reason = True return doc","title":"__call__()"},{"location":"reference/pipelines/misc/sections/","text":"edsnlp.pipelines.misc.sections patterns These section titles were extracted from a work performed by Ivan Lerner at AP-HP. It supplied a number of documents annotated for section titles. The section titles were reviewed by Gilles Chatellier, who gave meaningful insights. See sections/section-dataset notebook for detail. sections Sections Bases: GenericMatcher Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : allergies ; ant\u00e9c\u00e9dents ; ant\u00e9c\u00e9dents familiaux ; traitements entr\u00e9e ; conclusion ; conclusion entr\u00e9e ; habitus ; correspondants ; diagnostic ; donn\u00e9es biom\u00e9triques entr\u00e9e ; examens ; examens compl\u00e9mentaires ; facteurs de risques ; histoire de la maladie ; actes ; motif ; prescriptions ; traitements sortie. The component looks for section titles within the document, and stores them in the section_title extension. For ease-of-use, the component also populates a section extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. PARAMETER DESCRIPTION nlp spaCy pipeline object. TYPE: Language sections Dictionary of terms to look for. TYPE: Dict[str, List[str]] attr Default attribute to match on. TYPE: str ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/misc/sections/sections.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class Sections ( GenericMatcher ): \"\"\" Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : - allergies ; - ant\u00e9c\u00e9dents ; - ant\u00e9c\u00e9dents familiaux ; - traitements entr\u00e9e ; - conclusion ; - conclusion entr\u00e9e ; - habitus ; - correspondants ; - diagnostic ; - donn\u00e9es biom\u00e9triques entr\u00e9e ; - examens ; - examens compl\u00e9mentaires ; - facteurs de risques ; - histoire de la maladie ; - actes ; - motif ; - prescriptions ; - traitements sortie. The component looks for section titles within the document, and stores them in the `section_title` extension. For ease-of-use, the component also populates a `section` extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. Parameters ---------- nlp : Language spaCy pipeline object. sections : Dict[str, List[str]] Dictionary of terms to look for. attr : str Default attribute to match on. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , sections : Dict [ str , List [ str ]], add_patterns : bool , attr : str , ignore_excluded : bool , ): logger . warning ( \"The component Sections is still in Beta. Use at your own risks.\" ) if sections is None : sections = patterns . sections self . add_patterns = add_patterns if add_patterns : for k , v in sections . items (): sections [ k ] = [ r \"\\n[^\\n]{0,5}\" + ent + r \"[^\\n]{0,5}\\n\" for ent in v ] super () . __init__ ( nlp , terms = None , regex = sections , attr = attr , ignore_excluded = ignore_excluded , ) self . set_extensions () if not nlp . has_pipe ( \"normalizer\" ) and not not nlp . has_pipe ( \"eds.normalizer\" ): logger . warning ( \"You should add pipe `eds.normalizer`\" ) @staticmethod def set_extensions (): if not Span . has_extension ( \"section_title\" ): Span . set_extension ( \"section_title\" , default = None ) if not Span . has_extension ( \"section\" ): Span . set_extension ( \"section\" , default = None ) # noinspection PyProtectedMember def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) if self . add_patterns : # Remove preceding newline titles = [ Span ( doc , title . start + 1 , title . end - 1 , label = title . label_ ) for title in titles ] sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles return doc __call__ ( doc ) Divides the doc into sections PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for sections Source code in edsnlp/pipelines/misc/sections/sections.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) if self . add_patterns : # Remove preceding newline titles = [ Span ( doc , title . start + 1 , title . end - 1 , label = title . label_ ) for title in titles ] sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles return doc","title":"`edsnlp.pipelines.misc.sections`"},{"location":"reference/pipelines/misc/sections/#edsnlppipelinesmiscsections","text":"","title":"edsnlp.pipelines.misc.sections"},{"location":"reference/pipelines/misc/sections/#edsnlp.pipelines.misc.sections.patterns","text":"These section titles were extracted from a work performed by Ivan Lerner at AP-HP. It supplied a number of documents annotated for section titles. The section titles were reviewed by Gilles Chatellier, who gave meaningful insights. See sections/section-dataset notebook for detail.","title":"patterns"},{"location":"reference/pipelines/misc/sections/#edsnlp.pipelines.misc.sections.sections","text":"","title":"sections"},{"location":"reference/pipelines/misc/sections/#edsnlp.pipelines.misc.sections.sections.Sections","text":"Bases: GenericMatcher Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : allergies ; ant\u00e9c\u00e9dents ; ant\u00e9c\u00e9dents familiaux ; traitements entr\u00e9e ; conclusion ; conclusion entr\u00e9e ; habitus ; correspondants ; diagnostic ; donn\u00e9es biom\u00e9triques entr\u00e9e ; examens ; examens compl\u00e9mentaires ; facteurs de risques ; histoire de la maladie ; actes ; motif ; prescriptions ; traitements sortie. The component looks for section titles within the document, and stores them in the section_title extension. For ease-of-use, the component also populates a section extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. PARAMETER DESCRIPTION nlp spaCy pipeline object. TYPE: Language sections Dictionary of terms to look for. TYPE: Dict[str, List[str]] attr Default attribute to match on. TYPE: str ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/misc/sections/sections.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class Sections ( GenericMatcher ): \"\"\" Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : - allergies ; - ant\u00e9c\u00e9dents ; - ant\u00e9c\u00e9dents familiaux ; - traitements entr\u00e9e ; - conclusion ; - conclusion entr\u00e9e ; - habitus ; - correspondants ; - diagnostic ; - donn\u00e9es biom\u00e9triques entr\u00e9e ; - examens ; - examens compl\u00e9mentaires ; - facteurs de risques ; - histoire de la maladie ; - actes ; - motif ; - prescriptions ; - traitements sortie. The component looks for section titles within the document, and stores them in the `section_title` extension. For ease-of-use, the component also populates a `section` extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. Parameters ---------- nlp : Language spaCy pipeline object. sections : Dict[str, List[str]] Dictionary of terms to look for. attr : str Default attribute to match on. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , sections : Dict [ str , List [ str ]], add_patterns : bool , attr : str , ignore_excluded : bool , ): logger . warning ( \"The component Sections is still in Beta. Use at your own risks.\" ) if sections is None : sections = patterns . sections self . add_patterns = add_patterns if add_patterns : for k , v in sections . items (): sections [ k ] = [ r \"\\n[^\\n]{0,5}\" + ent + r \"[^\\n]{0,5}\\n\" for ent in v ] super () . __init__ ( nlp , terms = None , regex = sections , attr = attr , ignore_excluded = ignore_excluded , ) self . set_extensions () if not nlp . has_pipe ( \"normalizer\" ) and not not nlp . has_pipe ( \"eds.normalizer\" ): logger . warning ( \"You should add pipe `eds.normalizer`\" ) @staticmethod def set_extensions (): if not Span . has_extension ( \"section_title\" ): Span . set_extension ( \"section_title\" , default = None ) if not Span . has_extension ( \"section\" ): Span . set_extension ( \"section\" , default = None ) # noinspection PyProtectedMember def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) if self . add_patterns : # Remove preceding newline titles = [ Span ( doc , title . start + 1 , title . end - 1 , label = title . label_ ) for title in titles ] sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles return doc","title":"Sections"},{"location":"reference/pipelines/misc/sections/#edsnlp.pipelines.misc.sections.sections.Sections.__call__","text":"Divides the doc into sections PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for sections Source code in edsnlp/pipelines/misc/sections/sections.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) if self . add_patterns : # Remove preceding newline titles = [ Span ( doc , title . start + 1 , title . end - 1 , label = title . label_ ) for title in titles ] sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles return doc","title":"__call__()"},{"location":"reference/pipelines/misc/sections/factory/","text":"edsnlp.pipelines.misc.sections.factory","title":"factory"},{"location":"reference/pipelines/misc/sections/factory/#edsnlppipelinesmiscsectionsfactory","text":"","title":"edsnlp.pipelines.misc.sections.factory"},{"location":"reference/pipelines/misc/sections/patterns/","text":"edsnlp.pipelines.misc.sections.patterns These section titles were extracted from a work performed by Ivan Lerner at AP-HP. It supplied a number of documents annotated for section titles. The section titles were reviewed by Gilles Chatellier, who gave meaningful insights. See sections/section-dataset notebook for detail.","title":"patterns"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlppipelinesmiscsectionspatterns","text":"These section titles were extracted from a work performed by Ivan Lerner at AP-HP. It supplied a number of documents annotated for section titles. The section titles were reviewed by Gilles Chatellier, who gave meaningful insights. See sections/section-dataset notebook for detail.","title":"edsnlp.pipelines.misc.sections.patterns"},{"location":"reference/pipelines/misc/sections/sections/","text":"edsnlp.pipelines.misc.sections.sections Sections Bases: GenericMatcher Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : allergies ; ant\u00e9c\u00e9dents ; ant\u00e9c\u00e9dents familiaux ; traitements entr\u00e9e ; conclusion ; conclusion entr\u00e9e ; habitus ; correspondants ; diagnostic ; donn\u00e9es biom\u00e9triques entr\u00e9e ; examens ; examens compl\u00e9mentaires ; facteurs de risques ; histoire de la maladie ; actes ; motif ; prescriptions ; traitements sortie. The component looks for section titles within the document, and stores them in the section_title extension. For ease-of-use, the component also populates a section extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. PARAMETER DESCRIPTION nlp spaCy pipeline object. TYPE: Language sections Dictionary of terms to look for. TYPE: Dict[str, List[str]] attr Default attribute to match on. TYPE: str ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/misc/sections/sections.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class Sections ( GenericMatcher ): \"\"\" Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : - allergies ; - ant\u00e9c\u00e9dents ; - ant\u00e9c\u00e9dents familiaux ; - traitements entr\u00e9e ; - conclusion ; - conclusion entr\u00e9e ; - habitus ; - correspondants ; - diagnostic ; - donn\u00e9es biom\u00e9triques entr\u00e9e ; - examens ; - examens compl\u00e9mentaires ; - facteurs de risques ; - histoire de la maladie ; - actes ; - motif ; - prescriptions ; - traitements sortie. The component looks for section titles within the document, and stores them in the `section_title` extension. For ease-of-use, the component also populates a `section` extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. Parameters ---------- nlp : Language spaCy pipeline object. sections : Dict[str, List[str]] Dictionary of terms to look for. attr : str Default attribute to match on. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , sections : Dict [ str , List [ str ]], add_patterns : bool , attr : str , ignore_excluded : bool , ): logger . warning ( \"The component Sections is still in Beta. Use at your own risks.\" ) if sections is None : sections = patterns . sections self . add_patterns = add_patterns if add_patterns : for k , v in sections . items (): sections [ k ] = [ r \"\\n[^\\n]{0,5}\" + ent + r \"[^\\n]{0,5}\\n\" for ent in v ] super () . __init__ ( nlp , terms = None , regex = sections , attr = attr , ignore_excluded = ignore_excluded , ) self . set_extensions () if not nlp . has_pipe ( \"normalizer\" ) and not not nlp . has_pipe ( \"eds.normalizer\" ): logger . warning ( \"You should add pipe `eds.normalizer`\" ) @staticmethod def set_extensions (): if not Span . has_extension ( \"section_title\" ): Span . set_extension ( \"section_title\" , default = None ) if not Span . has_extension ( \"section\" ): Span . set_extension ( \"section\" , default = None ) # noinspection PyProtectedMember def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) if self . add_patterns : # Remove preceding newline titles = [ Span ( doc , title . start + 1 , title . end - 1 , label = title . label_ ) for title in titles ] sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles return doc __call__ ( doc ) Divides the doc into sections PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for sections Source code in edsnlp/pipelines/misc/sections/sections.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) if self . add_patterns : # Remove preceding newline titles = [ Span ( doc , title . start + 1 , title . end - 1 , label = title . label_ ) for title in titles ] sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles return doc","title":"sections"},{"location":"reference/pipelines/misc/sections/sections/#edsnlppipelinesmiscsectionssections","text":"","title":"edsnlp.pipelines.misc.sections.sections"},{"location":"reference/pipelines/misc/sections/sections/#edsnlp.pipelines.misc.sections.sections.Sections","text":"Bases: GenericMatcher Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : allergies ; ant\u00e9c\u00e9dents ; ant\u00e9c\u00e9dents familiaux ; traitements entr\u00e9e ; conclusion ; conclusion entr\u00e9e ; habitus ; correspondants ; diagnostic ; donn\u00e9es biom\u00e9triques entr\u00e9e ; examens ; examens compl\u00e9mentaires ; facteurs de risques ; histoire de la maladie ; actes ; motif ; prescriptions ; traitements sortie. The component looks for section titles within the document, and stores them in the section_title extension. For ease-of-use, the component also populates a section extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. PARAMETER DESCRIPTION nlp spaCy pipeline object. TYPE: Language sections Dictionary of terms to look for. TYPE: Dict[str, List[str]] attr Default attribute to match on. TYPE: str ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/misc/sections/sections.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class Sections ( GenericMatcher ): \"\"\" Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : - allergies ; - ant\u00e9c\u00e9dents ; - ant\u00e9c\u00e9dents familiaux ; - traitements entr\u00e9e ; - conclusion ; - conclusion entr\u00e9e ; - habitus ; - correspondants ; - diagnostic ; - donn\u00e9es biom\u00e9triques entr\u00e9e ; - examens ; - examens compl\u00e9mentaires ; - facteurs de risques ; - histoire de la maladie ; - actes ; - motif ; - prescriptions ; - traitements sortie. The component looks for section titles within the document, and stores them in the `section_title` extension. For ease-of-use, the component also populates a `section` extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. Parameters ---------- nlp : Language spaCy pipeline object. sections : Dict[str, List[str]] Dictionary of terms to look for. attr : str Default attribute to match on. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , sections : Dict [ str , List [ str ]], add_patterns : bool , attr : str , ignore_excluded : bool , ): logger . warning ( \"The component Sections is still in Beta. Use at your own risks.\" ) if sections is None : sections = patterns . sections self . add_patterns = add_patterns if add_patterns : for k , v in sections . items (): sections [ k ] = [ r \"\\n[^\\n]{0,5}\" + ent + r \"[^\\n]{0,5}\\n\" for ent in v ] super () . __init__ ( nlp , terms = None , regex = sections , attr = attr , ignore_excluded = ignore_excluded , ) self . set_extensions () if not nlp . has_pipe ( \"normalizer\" ) and not not nlp . has_pipe ( \"eds.normalizer\" ): logger . warning ( \"You should add pipe `eds.normalizer`\" ) @staticmethod def set_extensions (): if not Span . has_extension ( \"section_title\" ): Span . set_extension ( \"section_title\" , default = None ) if not Span . has_extension ( \"section\" ): Span . set_extension ( \"section\" , default = None ) # noinspection PyProtectedMember def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) if self . add_patterns : # Remove preceding newline titles = [ Span ( doc , title . start + 1 , title . end - 1 , label = title . label_ ) for title in titles ] sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles return doc","title":"Sections"},{"location":"reference/pipelines/misc/sections/sections/#edsnlp.pipelines.misc.sections.sections.Sections.__call__","text":"Divides the doc into sections PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for sections Source code in edsnlp/pipelines/misc/sections/sections.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) if self . add_patterns : # Remove preceding newline titles = [ Span ( doc , title . start + 1 , title . end - 1 , label = title . label_ ) for title in titles ] sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles return doc","title":"__call__()"},{"location":"reference/pipelines/ner/","text":"edsnlp.pipelines.ner","title":"`edsnlp.pipelines.ner`"},{"location":"reference/pipelines/ner/#edsnlppipelinesner","text":"","title":"edsnlp.pipelines.ner"},{"location":"reference/pipelines/ner/covid/","text":"edsnlp.pipelines.ner.covid","title":"`edsnlp.pipelines.ner.covid`"},{"location":"reference/pipelines/ner/covid/#edsnlppipelinesnercovid","text":"","title":"edsnlp.pipelines.ner.covid"},{"location":"reference/pipelines/ner/covid/factory/","text":"edsnlp.pipelines.ner.covid.factory","title":"factory"},{"location":"reference/pipelines/ner/covid/factory/#edsnlppipelinesnercovidfactory","text":"","title":"edsnlp.pipelines.ner.covid.factory"},{"location":"reference/pipelines/ner/covid/patterns/","text":"edsnlp.pipelines.ner.covid.patterns","title":"patterns"},{"location":"reference/pipelines/ner/covid/patterns/#edsnlppipelinesnercovidpatterns","text":"","title":"edsnlp.pipelines.ner.covid.patterns"},{"location":"reference/pipelines/ner/scores/","text":"edsnlp.pipelines.ner.scores","title":"`edsnlp.pipelines.ner.scores`"},{"location":"reference/pipelines/ner/scores/#edsnlppipelinesnerscores","text":"","title":"edsnlp.pipelines.ner.scores"},{"location":"reference/pipelines/ner/scores/base_score/","text":"edsnlp.pipelines.ner.scores.base_score Score Bases: AdvancedRegex Matcher component to extract a numeric score PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language score_name The name of the extracted score TYPE: str regex A list of regexes to identify the score TYPE: List[str] attr Wether to match on the text ('TEXT') or on the normalized text ('NORM') TYPE: str after_extract Regex with capturing group to get the score value TYPE: str score_normalization Function that takes the \"raw\" value extracted from the after_extract regex, and should return - None if no score could be extracted - The desired score value else TYPE: Callable[[Union[str,None]], Any] window Number of token to include after the score's mention to find the score's value TYPE: int Source code in edsnlp/pipelines/ner/scores/base_score.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 class Score ( AdvancedRegex ): \"\"\" Matcher component to extract a numeric score Parameters ---------- nlp : Language The spaCy object. score_name : str The name of the extracted score regex : List[str] A list of regexes to identify the score attr : str Wether to match on the text ('TEXT') or on the normalized text ('NORM') after_extract : str Regex with capturing group to get the score value score_normalization : Callable[[Union[str,None]], Any] Function that takes the \"raw\" value extracted from the `after_extract` regex, and should return - None if no score could be extracted - The desired score value else window : int Number of token to include after the score's mention to find the score's value \"\"\" def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , after_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , verbose : int , ignore_excluded : bool , ): regex_config = { score_name : dict ( regex = regex , attr = attr , after_extract = after_extract ) } super () . __init__ ( nlp = nlp , regex_config = regex_config , window = window , verbose = verbose , ignore_excluded = ignore_excluded , attr = attr , ) self . score_name = score_name if isinstance ( score_normalization , str ): self . score_normalization = registry . get ( \"misc\" , score_normalization ) else : self . score_normalization = score_normalization self . set_extensions () @staticmethod def set_extensions () -> None : super ( Score , Score ) . set_extensions () if not Span . has_extension ( \"score_name\" ): Span . set_extension ( \"score_name\" , default = None ) if not Span . has_extension ( \"score_value\" ): Span . set_extension ( \"score_value\" , default = None ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = super ( Score , Score ) . process ( self , doc ) ents = self . score_filtering ( ents ) ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" to_keep_ents = [] for ent in ents : value = ent . _ . after_extract [ 0 ] normalized_value = self . score_normalization ( value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( value ) to_keep_ents . append ( ent ) return to_keep_ents __call__ ( doc ) Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/ner/scores/base_score.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = super ( Score , Score ) . process ( self , doc ) ents = self . score_filtering ( ents ) ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc score_filtering ( ents ) Extracts, if available, the value of the score. Normalizes the score via the provided self.score_normalization method. PARAMETER DESCRIPTION ents List of spaCy's spans extracted by the score matcher TYPE: List [ Span ] RETURNS DESCRIPTION ents List of spaCy's spans, with, if found, an added score_value extension Source code in edsnlp/pipelines/ner/scores/base_score.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" to_keep_ents = [] for ent in ents : value = ent . _ . after_extract [ 0 ] normalized_value = self . score_normalization ( value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( value ) to_keep_ents . append ( ent ) return to_keep_ents","title":"base_score"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlppipelinesnerscoresbase_score","text":"","title":"edsnlp.pipelines.ner.scores.base_score"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score","text":"Bases: AdvancedRegex Matcher component to extract a numeric score PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language score_name The name of the extracted score TYPE: str regex A list of regexes to identify the score TYPE: List[str] attr Wether to match on the text ('TEXT') or on the normalized text ('NORM') TYPE: str after_extract Regex with capturing group to get the score value TYPE: str score_normalization Function that takes the \"raw\" value extracted from the after_extract regex, and should return - None if no score could be extracted - The desired score value else TYPE: Callable[[Union[str,None]], Any] window Number of token to include after the score's mention to find the score's value TYPE: int Source code in edsnlp/pipelines/ner/scores/base_score.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 class Score ( AdvancedRegex ): \"\"\" Matcher component to extract a numeric score Parameters ---------- nlp : Language The spaCy object. score_name : str The name of the extracted score regex : List[str] A list of regexes to identify the score attr : str Wether to match on the text ('TEXT') or on the normalized text ('NORM') after_extract : str Regex with capturing group to get the score value score_normalization : Callable[[Union[str,None]], Any] Function that takes the \"raw\" value extracted from the `after_extract` regex, and should return - None if no score could be extracted - The desired score value else window : int Number of token to include after the score's mention to find the score's value \"\"\" def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , after_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , verbose : int , ignore_excluded : bool , ): regex_config = { score_name : dict ( regex = regex , attr = attr , after_extract = after_extract ) } super () . __init__ ( nlp = nlp , regex_config = regex_config , window = window , verbose = verbose , ignore_excluded = ignore_excluded , attr = attr , ) self . score_name = score_name if isinstance ( score_normalization , str ): self . score_normalization = registry . get ( \"misc\" , score_normalization ) else : self . score_normalization = score_normalization self . set_extensions () @staticmethod def set_extensions () -> None : super ( Score , Score ) . set_extensions () if not Span . has_extension ( \"score_name\" ): Span . set_extension ( \"score_name\" , default = None ) if not Span . has_extension ( \"score_value\" ): Span . set_extension ( \"score_value\" , default = None ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = super ( Score , Score ) . process ( self , doc ) ents = self . score_filtering ( ents ) ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" to_keep_ents = [] for ent in ents : value = ent . _ . after_extract [ 0 ] normalized_value = self . score_normalization ( value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( value ) to_keep_ents . append ( ent ) return to_keep_ents","title":"Score"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score.__call__","text":"Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/ner/scores/base_score.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = super ( Score , Score ) . process ( self , doc ) ents = self . score_filtering ( ents ) ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"__call__()"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score.score_filtering","text":"Extracts, if available, the value of the score. Normalizes the score via the provided self.score_normalization method. PARAMETER DESCRIPTION ents List of spaCy's spans extracted by the score matcher TYPE: List [ Span ] RETURNS DESCRIPTION ents List of spaCy's spans, with, if found, an added score_value extension Source code in edsnlp/pipelines/ner/scores/base_score.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" to_keep_ents = [] for ent in ents : value = ent . _ . after_extract [ 0 ] normalized_value = self . score_normalization ( value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( value ) to_keep_ents . append ( ent ) return to_keep_ents","title":"score_filtering()"},{"location":"reference/pipelines/ner/scores/factory/","text":"edsnlp.pipelines.ner.scores.factory","title":"factory"},{"location":"reference/pipelines/ner/scores/factory/#edsnlppipelinesnerscoresfactory","text":"","title":"edsnlp.pipelines.ner.scores.factory"},{"location":"reference/pipelines/ner/scores/charlson/","text":"edsnlp.pipelines.ner.scores.charlson","title":"`edsnlp.pipelines.ner.scores.charlson`"},{"location":"reference/pipelines/ner/scores/charlson/#edsnlppipelinesnerscorescharlson","text":"","title":"edsnlp.pipelines.ner.scores.charlson"},{"location":"reference/pipelines/ner/scores/charlson/factory/","text":"edsnlp.pipelines.ner.scores.charlson.factory","title":"factory"},{"location":"reference/pipelines/ner/scores/charlson/factory/#edsnlppipelinesnerscorescharlsonfactory","text":"","title":"edsnlp.pipelines.ner.scores.charlson.factory"},{"location":"reference/pipelines/ner/scores/charlson/patterns/","text":"edsnlp.pipelines.ner.scores.charlson.patterns score_normalization ( extracted_score ) Charlson score normalization. If available, returns the integer value of the Charlson score. Source code in edsnlp/pipelines/ner/scores/charlson/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Charlson score normalization. If available, returns the integer value of the Charlson score. \"\"\" score_range = list ( range ( 0 , 30 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"patterns"},{"location":"reference/pipelines/ner/scores/charlson/patterns/#edsnlppipelinesnerscorescharlsonpatterns","text":"","title":"edsnlp.pipelines.ner.scores.charlson.patterns"},{"location":"reference/pipelines/ner/scores/charlson/patterns/#edsnlp.pipelines.ner.scores.charlson.patterns.score_normalization","text":"Charlson score normalization. If available, returns the integer value of the Charlson score. Source code in edsnlp/pipelines/ner/scores/charlson/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Charlson score normalization. If available, returns the integer value of the Charlson score. \"\"\" score_range = list ( range ( 0 , 30 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"score_normalization()"},{"location":"reference/pipelines/ner/scores/emergency/","text":"edsnlp.pipelines.ner.scores.emergency","title":"`edsnlp.pipelines.ner.scores.emergency`"},{"location":"reference/pipelines/ner/scores/emergency/#edsnlppipelinesnerscoresemergency","text":"","title":"edsnlp.pipelines.ner.scores.emergency"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/","text":"edsnlp.pipelines.ner.scores.emergency.ccmu","title":"`edsnlp.pipelines.ner.scores.emergency.ccmu`"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/#edsnlppipelinesnerscoresemergencyccmu","text":"","title":"edsnlp.pipelines.ner.scores.emergency.ccmu"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/factory/","text":"edsnlp.pipelines.ner.scores.emergency.ccmu.factory","title":"factory"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/factory/#edsnlppipelinesnerscoresemergencyccmufactory","text":"","title":"edsnlp.pipelines.ner.scores.emergency.ccmu.factory"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/patterns/","text":"edsnlp.pipelines.ner.scores.emergency.ccmu.patterns score_normalization ( extracted_score ) CCMU score normalization. If available, returns the integer value of the CCMU score. Source code in edsnlp/pipelines/ner/scores/emergency/ccmu/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" CCMU score normalization. If available, returns the integer value of the CCMU score. \"\"\" score_range = [ 1 , 2 , 3 , 4 , 5 ] if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"patterns"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/patterns/#edsnlppipelinesnerscoresemergencyccmupatterns","text":"","title":"edsnlp.pipelines.ner.scores.emergency.ccmu.patterns"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/patterns/#edsnlp.pipelines.ner.scores.emergency.ccmu.patterns.score_normalization","text":"CCMU score normalization. If available, returns the integer value of the CCMU score. Source code in edsnlp/pipelines/ner/scores/emergency/ccmu/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" CCMU score normalization. If available, returns the integer value of the CCMU score. \"\"\" score_range = [ 1 , 2 , 3 , 4 , 5 ] if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"score_normalization()"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/","text":"edsnlp.pipelines.ner.scores.emergency.gemsa","title":"`edsnlp.pipelines.ner.scores.emergency.gemsa`"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/#edsnlppipelinesnerscoresemergencygemsa","text":"","title":"edsnlp.pipelines.ner.scores.emergency.gemsa"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/factory/","text":"edsnlp.pipelines.ner.scores.emergency.gemsa.factory","title":"factory"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/factory/#edsnlppipelinesnerscoresemergencygemsafactory","text":"","title":"edsnlp.pipelines.ner.scores.emergency.gemsa.factory"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/patterns/","text":"edsnlp.pipelines.ner.scores.emergency.gemsa.patterns score_normalization ( extracted_score ) GEMSA score normalization. If available, returns the integer value of the GEMSA score. Source code in edsnlp/pipelines/ner/scores/emergency/gemsa/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" GEMSA score normalization. If available, returns the integer value of the GEMSA score. \"\"\" score_range = [ 1 , 2 , 3 , 4 , 5 , 6 ] if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"patterns"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/patterns/#edsnlppipelinesnerscoresemergencygemsapatterns","text":"","title":"edsnlp.pipelines.ner.scores.emergency.gemsa.patterns"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/patterns/#edsnlp.pipelines.ner.scores.emergency.gemsa.patterns.score_normalization","text":"GEMSA score normalization. If available, returns the integer value of the GEMSA score. Source code in edsnlp/pipelines/ner/scores/emergency/gemsa/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" GEMSA score normalization. If available, returns the integer value of the GEMSA score. \"\"\" score_range = [ 1 , 2 , 3 , 4 , 5 , 6 ] if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"score_normalization()"},{"location":"reference/pipelines/ner/scores/emergency/priority/","text":"edsnlp.pipelines.ner.scores.emergency.priority","title":"`edsnlp.pipelines.ner.scores.emergency.priority`"},{"location":"reference/pipelines/ner/scores/emergency/priority/#edsnlppipelinesnerscoresemergencypriority","text":"","title":"edsnlp.pipelines.ner.scores.emergency.priority"},{"location":"reference/pipelines/ner/scores/emergency/priority/factory/","text":"edsnlp.pipelines.ner.scores.emergency.priority.factory","title":"factory"},{"location":"reference/pipelines/ner/scores/emergency/priority/factory/#edsnlppipelinesnerscoresemergencypriorityfactory","text":"","title":"edsnlp.pipelines.ner.scores.emergency.priority.factory"},{"location":"reference/pipelines/ner/scores/emergency/priority/patterns/","text":"edsnlp.pipelines.ner.scores.emergency.priority.patterns score_normalization ( extracted_score ) Priority score normalization. If available, returns the integer value of the priority score. Source code in edsnlp/pipelines/ner/scores/emergency/priority/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Priority score normalization. If available, returns the integer value of the priority score. \"\"\" score_range = list ( range ( 0 , 6 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"patterns"},{"location":"reference/pipelines/ner/scores/emergency/priority/patterns/#edsnlppipelinesnerscoresemergencyprioritypatterns","text":"","title":"edsnlp.pipelines.ner.scores.emergency.priority.patterns"},{"location":"reference/pipelines/ner/scores/emergency/priority/patterns/#edsnlp.pipelines.ner.scores.emergency.priority.patterns.score_normalization","text":"Priority score normalization. If available, returns the integer value of the priority score. Source code in edsnlp/pipelines/ner/scores/emergency/priority/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Priority score normalization. If available, returns the integer value of the priority score. \"\"\" score_range = list ( range ( 0 , 6 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"score_normalization()"},{"location":"reference/pipelines/ner/scores/sofa/","text":"edsnlp.pipelines.ner.scores.sofa","title":"`edsnlp.pipelines.ner.scores.sofa`"},{"location":"reference/pipelines/ner/scores/sofa/#edsnlppipelinesnerscoressofa","text":"","title":"edsnlp.pipelines.ner.scores.sofa"},{"location":"reference/pipelines/ner/scores/sofa/factory/","text":"edsnlp.pipelines.ner.scores.sofa.factory","title":"factory"},{"location":"reference/pipelines/ner/scores/sofa/factory/#edsnlppipelinesnerscoressofafactory","text":"","title":"edsnlp.pipelines.ner.scores.sofa.factory"},{"location":"reference/pipelines/ner/scores/sofa/patterns/","text":"edsnlp.pipelines.ner.scores.sofa.patterns score_normalization ( extracted_score ) Sofa score normalization. If available, returns the integer value of the SOFA score. Source code in edsnlp/pipelines/ner/scores/sofa/patterns.py 17 18 19 20 21 22 23 24 25 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Sofa score normalization. If available, returns the integer value of the SOFA score. \"\"\" score_range = list ( range ( 0 , 30 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"patterns"},{"location":"reference/pipelines/ner/scores/sofa/patterns/#edsnlppipelinesnerscoressofapatterns","text":"","title":"edsnlp.pipelines.ner.scores.sofa.patterns"},{"location":"reference/pipelines/ner/scores/sofa/patterns/#edsnlp.pipelines.ner.scores.sofa.patterns.score_normalization","text":"Sofa score normalization. If available, returns the integer value of the SOFA score. Source code in edsnlp/pipelines/ner/scores/sofa/patterns.py 17 18 19 20 21 22 23 24 25 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Sofa score normalization. If available, returns the integer value of the SOFA score. \"\"\" score_range = list ( range ( 0 , 30 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"score_normalization()"},{"location":"reference/pipelines/ner/scores/sofa/sofa/","text":"edsnlp.pipelines.ner.scores.sofa.sofa Sofa Bases: Score Matcher component to extract the SOFA score PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language score_name The name of the extracted score TYPE: str regex A list of regexes to identify the SOFA score TYPE: List[str] attr Wether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM') TYPE: str method_regex Regex with capturing group to get the score extraction method (e.g. \"\u00e0 l'admission\", \"\u00e0 24H\", \"Maximum\") TYPE: str value_regex Regex to extract the score value TYPE: str score_normalization Function that takes the \"raw\" value extracted from the after_extract regex, and should return - None if no score could be extracted - The desired score value else TYPE: Callable[[Union[str,None]], Any] window Number of token to include after the score's mention to find the score's value TYPE: int Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class Sofa ( Score ): \"\"\" Matcher component to extract the SOFA score Parameters ---------- nlp : Language The spaCy object. score_name : str The name of the extracted score regex : List[str] A list of regexes to identify the SOFA score attr : str Wether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM') method_regex : str Regex with capturing group to get the score extraction method (e.g. \"\u00e0 l'admission\", \"\u00e0 24H\", \"Maximum\") value_regex : str Regex to extract the score value score_normalization : Callable[[Union[str,None]], Any] Function that takes the \"raw\" value extracted from the `after_extract` regex, and should return - None if no score could be extracted - The desired score value else window : int Number of token to include after the score's mention to find the score's value \"\"\" def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , method_regex : str , value_regex : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , verbose : int , ignore_excluded : bool , ): super () . __init__ ( nlp , score_name = score_name , regex = regex , after_extract = [], score_normalization = score_normalization , attr = attr , window = window , verbose = verbose , ignore_excluded = ignore_excluded , ) self . method_regex = method_regex self . value_regex = value_regex self . set_extensions () @staticmethod def set_extensions () -> None : super ( Sofa , Sofa ) . set_extensions () if not Span . has_extension ( \"score_method\" ): Span . set_extension ( \"score_method\" , default = None ) def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" to_keep_ents = [] for ent in ents : after_snippet = get_text ( ent . _ . after_snippet , attr = self . attr , ignore_excluded = self . ignore_excluded , ) matches = re . search ( self . method_regex , after_snippet ) if matches is None : method = \"Non pr\u00e9cis\u00e9e\" value = after_snippet else : groups = matches . groupdict () value = groups [ \"after_value\" ] if groups [ \"max\" ] is not None : method = \"Maximum\" elif groups [ \"vqheures\" ] is not None : method = \"24H\" elif groups [ \"admission\" ] is not None : method = \"A l'admission\" digit_value = re . match ( self . value_regex , value ) # Use match instead of search to only look at the beginning digit_value = None if digit_value is None else digit_value . groups ()[ 0 ] normalized_value = self . score_normalization ( digit_value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( normalized_value ) ent . _ . score_method = method to_keep_ents . append ( ent ) return to_keep_ents score_filtering ( ents ) Extracts, if available, the value of the score. Normalizes the score via the provided self.score_normalization method. PARAMETER DESCRIPTION ents List of spaCy's spans extracted by the score matcher TYPE: List [ Span ] RETURNS DESCRIPTION ents List of spaCy's spans, with, if found, an added score_value extension Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" to_keep_ents = [] for ent in ents : after_snippet = get_text ( ent . _ . after_snippet , attr = self . attr , ignore_excluded = self . ignore_excluded , ) matches = re . search ( self . method_regex , after_snippet ) if matches is None : method = \"Non pr\u00e9cis\u00e9e\" value = after_snippet else : groups = matches . groupdict () value = groups [ \"after_value\" ] if groups [ \"max\" ] is not None : method = \"Maximum\" elif groups [ \"vqheures\" ] is not None : method = \"24H\" elif groups [ \"admission\" ] is not None : method = \"A l'admission\" digit_value = re . match ( self . value_regex , value ) # Use match instead of search to only look at the beginning digit_value = None if digit_value is None else digit_value . groups ()[ 0 ] normalized_value = self . score_normalization ( digit_value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( normalized_value ) ent . _ . score_method = method to_keep_ents . append ( ent ) return to_keep_ents","title":"sofa"},{"location":"reference/pipelines/ner/scores/sofa/sofa/#edsnlppipelinesnerscoressofasofa","text":"","title":"edsnlp.pipelines.ner.scores.sofa.sofa"},{"location":"reference/pipelines/ner/scores/sofa/sofa/#edsnlp.pipelines.ner.scores.sofa.sofa.Sofa","text":"Bases: Score Matcher component to extract the SOFA score PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language score_name The name of the extracted score TYPE: str regex A list of regexes to identify the SOFA score TYPE: List[str] attr Wether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM') TYPE: str method_regex Regex with capturing group to get the score extraction method (e.g. \"\u00e0 l'admission\", \"\u00e0 24H\", \"Maximum\") TYPE: str value_regex Regex to extract the score value TYPE: str score_normalization Function that takes the \"raw\" value extracted from the after_extract regex, and should return - None if no score could be extracted - The desired score value else TYPE: Callable[[Union[str,None]], Any] window Number of token to include after the score's mention to find the score's value TYPE: int Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class Sofa ( Score ): \"\"\" Matcher component to extract the SOFA score Parameters ---------- nlp : Language The spaCy object. score_name : str The name of the extracted score regex : List[str] A list of regexes to identify the SOFA score attr : str Wether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM') method_regex : str Regex with capturing group to get the score extraction method (e.g. \"\u00e0 l'admission\", \"\u00e0 24H\", \"Maximum\") value_regex : str Regex to extract the score value score_normalization : Callable[[Union[str,None]], Any] Function that takes the \"raw\" value extracted from the `after_extract` regex, and should return - None if no score could be extracted - The desired score value else window : int Number of token to include after the score's mention to find the score's value \"\"\" def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , method_regex : str , value_regex : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , verbose : int , ignore_excluded : bool , ): super () . __init__ ( nlp , score_name = score_name , regex = regex , after_extract = [], score_normalization = score_normalization , attr = attr , window = window , verbose = verbose , ignore_excluded = ignore_excluded , ) self . method_regex = method_regex self . value_regex = value_regex self . set_extensions () @staticmethod def set_extensions () -> None : super ( Sofa , Sofa ) . set_extensions () if not Span . has_extension ( \"score_method\" ): Span . set_extension ( \"score_method\" , default = None ) def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" to_keep_ents = [] for ent in ents : after_snippet = get_text ( ent . _ . after_snippet , attr = self . attr , ignore_excluded = self . ignore_excluded , ) matches = re . search ( self . method_regex , after_snippet ) if matches is None : method = \"Non pr\u00e9cis\u00e9e\" value = after_snippet else : groups = matches . groupdict () value = groups [ \"after_value\" ] if groups [ \"max\" ] is not None : method = \"Maximum\" elif groups [ \"vqheures\" ] is not None : method = \"24H\" elif groups [ \"admission\" ] is not None : method = \"A l'admission\" digit_value = re . match ( self . value_regex , value ) # Use match instead of search to only look at the beginning digit_value = None if digit_value is None else digit_value . groups ()[ 0 ] normalized_value = self . score_normalization ( digit_value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( normalized_value ) ent . _ . score_method = method to_keep_ents . append ( ent ) return to_keep_ents","title":"Sofa"},{"location":"reference/pipelines/ner/scores/sofa/sofa/#edsnlp.pipelines.ner.scores.sofa.sofa.Sofa.score_filtering","text":"Extracts, if available, the value of the score. Normalizes the score via the provided self.score_normalization method. PARAMETER DESCRIPTION ents List of spaCy's spans extracted by the score matcher TYPE: List [ Span ] RETURNS DESCRIPTION ents List of spaCy's spans, with, if found, an added score_value extension Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" to_keep_ents = [] for ent in ents : after_snippet = get_text ( ent . _ . after_snippet , attr = self . attr , ignore_excluded = self . ignore_excluded , ) matches = re . search ( self . method_regex , after_snippet ) if matches is None : method = \"Non pr\u00e9cis\u00e9e\" value = after_snippet else : groups = matches . groupdict () value = groups [ \"after_value\" ] if groups [ \"max\" ] is not None : method = \"Maximum\" elif groups [ \"vqheures\" ] is not None : method = \"24H\" elif groups [ \"admission\" ] is not None : method = \"A l'admission\" digit_value = re . match ( self . value_regex , value ) # Use match instead of search to only look at the beginning digit_value = None if digit_value is None else digit_value . groups ()[ 0 ] normalized_value = self . score_normalization ( digit_value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( normalized_value ) ent . _ . score_method = method to_keep_ents . append ( ent ) return to_keep_ents","title":"score_filtering()"},{"location":"reference/pipelines/qualifiers/","text":"edsnlp.pipelines.qualifiers","title":"`edsnlp.pipelines.qualifiers`"},{"location":"reference/pipelines/qualifiers/#edsnlppipelinesqualifiers","text":"","title":"edsnlp.pipelines.qualifiers"},{"location":"reference/pipelines/qualifiers/base/","text":"edsnlp.pipelines.qualifiers.base Qualifier Bases: BaseComponent Implements the NegEx algorithm. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool **terms Terms to look for. TYPE: Dict[str, Optional[List[str]]] Source code in edsnlp/pipelines/qualifiers/base.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class Qualifier ( BaseComponent ): \"\"\" Implements the NegEx algorithm. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. explain : bool Whether to keep track of cues for each entity. **terms : Dict[str, Optional[List[str]]] Terms to look for. \"\"\" defaults = dict () def __init__ ( self , nlp : Language , attr : str , on_ents_only : bool , explain : bool , ** terms : Dict [ str , Optional [ List [ str ]]], ): if attr . upper () == \"NORM\" : check_normalizer ( nlp ) self . phrase_matcher = EDSPhraseMatcher ( vocab = nlp . vocab , attr = attr ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . on_ents_only = on_ents_only self . explain = explain def get_defaults ( self , ** kwargs : Dict [ str , Optional [ List [ str ]]] ) -> Dict [ str , List [ str ]]: \"\"\" Merge terms with their defaults. Null keys are replaced with defaults. Returns ------- Dict[str, List[str]] Merged dictionary \"\"\" # Filter out empty keys kwargs = { k : v for k , v in kwargs . items () if v is not None } # Update defaults terms = self . defaults . copy () terms . update ( kwargs ) return terms def get_matches ( self , doc : Doc ) -> List [ Span ]: \"\"\" Extract matches. Parameters ---------- doc : Doc spaCy `Doc` object. Returns ------- List[Span] List of detected spans \"\"\" if self . on_ents_only : sents = set ([ ent . sent for ent in doc . ents ]) match_iterator = map ( lambda sent : self . phrase_matcher ( sent , as_spans = True ), sents ) matches = chain . from_iterable ( match_iterator ) else : matches = self . phrase_matcher ( doc , as_spans = True ) return list ( matches ) def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc ) get_defaults ( ** kwargs ) Merge terms with their defaults. Null keys are replaced with defaults. RETURNS DESCRIPTION Dict[str, List[str]] Merged dictionary Source code in edsnlp/pipelines/qualifiers/base.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_defaults ( self , ** kwargs : Dict [ str , Optional [ List [ str ]]] ) -> Dict [ str , List [ str ]]: \"\"\" Merge terms with their defaults. Null keys are replaced with defaults. Returns ------- Dict[str, List[str]] Merged dictionary \"\"\" # Filter out empty keys kwargs = { k : v for k , v in kwargs . items () if v is not None } # Update defaults terms = self . defaults . copy () terms . update ( kwargs ) return terms get_matches ( doc ) Extract matches. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION List[Span] List of detected spans Source code in edsnlp/pipelines/qualifiers/base.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def get_matches ( self , doc : Doc ) -> List [ Span ]: \"\"\" Extract matches. Parameters ---------- doc : Doc spaCy `Doc` object. Returns ------- List[Span] List of detected spans \"\"\" if self . on_ents_only : sents = set ([ ent . sent for ent in doc . ents ]) match_iterator = map ( lambda sent : self . phrase_matcher ( sent , as_spans = True ), sents ) matches = chain . from_iterable ( match_iterator ) else : matches = self . phrase_matcher ( doc , as_spans = True ) return list ( matches )","title":"base"},{"location":"reference/pipelines/qualifiers/base/#edsnlppipelinesqualifiersbase","text":"","title":"edsnlp.pipelines.qualifiers.base"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier","text":"Bases: BaseComponent Implements the NegEx algorithm. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool **terms Terms to look for. TYPE: Dict[str, Optional[List[str]]] Source code in edsnlp/pipelines/qualifiers/base.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class Qualifier ( BaseComponent ): \"\"\" Implements the NegEx algorithm. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. explain : bool Whether to keep track of cues for each entity. **terms : Dict[str, Optional[List[str]]] Terms to look for. \"\"\" defaults = dict () def __init__ ( self , nlp : Language , attr : str , on_ents_only : bool , explain : bool , ** terms : Dict [ str , Optional [ List [ str ]]], ): if attr . upper () == \"NORM\" : check_normalizer ( nlp ) self . phrase_matcher = EDSPhraseMatcher ( vocab = nlp . vocab , attr = attr ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . on_ents_only = on_ents_only self . explain = explain def get_defaults ( self , ** kwargs : Dict [ str , Optional [ List [ str ]]] ) -> Dict [ str , List [ str ]]: \"\"\" Merge terms with their defaults. Null keys are replaced with defaults. Returns ------- Dict[str, List[str]] Merged dictionary \"\"\" # Filter out empty keys kwargs = { k : v for k , v in kwargs . items () if v is not None } # Update defaults terms = self . defaults . copy () terms . update ( kwargs ) return terms def get_matches ( self , doc : Doc ) -> List [ Span ]: \"\"\" Extract matches. Parameters ---------- doc : Doc spaCy `Doc` object. Returns ------- List[Span] List of detected spans \"\"\" if self . on_ents_only : sents = set ([ ent . sent for ent in doc . ents ]) match_iterator = map ( lambda sent : self . phrase_matcher ( sent , as_spans = True ), sents ) matches = chain . from_iterable ( match_iterator ) else : matches = self . phrase_matcher ( doc , as_spans = True ) return list ( matches ) def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc )","title":"Qualifier"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.get_defaults","text":"Merge terms with their defaults. Null keys are replaced with defaults. RETURNS DESCRIPTION Dict[str, List[str]] Merged dictionary Source code in edsnlp/pipelines/qualifiers/base.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_defaults ( self , ** kwargs : Dict [ str , Optional [ List [ str ]]] ) -> Dict [ str , List [ str ]]: \"\"\" Merge terms with their defaults. Null keys are replaced with defaults. Returns ------- Dict[str, List[str]] Merged dictionary \"\"\" # Filter out empty keys kwargs = { k : v for k , v in kwargs . items () if v is not None } # Update defaults terms = self . defaults . copy () terms . update ( kwargs ) return terms","title":"get_defaults()"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.get_matches","text":"Extract matches. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION List[Span] List of detected spans Source code in edsnlp/pipelines/qualifiers/base.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def get_matches ( self , doc : Doc ) -> List [ Span ]: \"\"\" Extract matches. Parameters ---------- doc : Doc spaCy `Doc` object. Returns ------- List[Span] List of detected spans \"\"\" if self . on_ents_only : sents = set ([ ent . sent for ent in doc . ents ]) match_iterator = map ( lambda sent : self . phrase_matcher ( sent , as_spans = True ), sents ) matches = chain . from_iterable ( match_iterator ) else : matches = self . phrase_matcher ( doc , as_spans = True ) return list ( matches )","title":"get_matches()"},{"location":"reference/pipelines/qualifiers/factories/","text":"edsnlp.pipelines.qualifiers.factories","title":"factories"},{"location":"reference/pipelines/qualifiers/factories/#edsnlppipelinesqualifiersfactories","text":"","title":"edsnlp.pipelines.qualifiers.factories"},{"location":"reference/pipelines/qualifiers/family/","text":"edsnlp.pipelines.qualifiers.family","title":"`edsnlp.pipelines.qualifiers.family`"},{"location":"reference/pipelines/qualifiers/family/#edsnlppipelinesqualifiersfamily","text":"","title":"edsnlp.pipelines.qualifiers.family"},{"location":"reference/pipelines/qualifiers/family/factory/","text":"edsnlp.pipelines.qualifiers.family.factory","title":"factory"},{"location":"reference/pipelines/qualifiers/family/factory/#edsnlppipelinesqualifiersfamilyfactory","text":"","title":"edsnlp.pipelines.qualifiers.family.factory"},{"location":"reference/pipelines/qualifiers/family/family/","text":"edsnlp.pipelines.qualifiers.family.family FamilyContext Bases: Qualifier Implements a family context detection algorithm. The components looks for terms indicating family references in the text. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language family List of terms indicating family reference. TYPE: Optional[List[str]] terminations List of termination terms, to separate syntagmas. TYPE: Optional[List[str]] attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool regex A dictionnary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] explain Whether to keep track of cues for each entity. TYPE: bool use_sections Whether to use annotated sections (namely ant\u00e9c\u00e9dents familiaux ). TYPE: bool, by default Source code in edsnlp/pipelines/qualifiers/family/family.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class FamilyContext ( Qualifier ): \"\"\" Implements a family context detection algorithm. The components looks for terms indicating family references in the text. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. family : Optional[List[str]] List of terms indicating family reference. terminations : Optional[List[str]] List of termination terms, to separate syntagmas. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. regex : Optional[Dict[str, Union[List[str], str]]] A dictionnary of regex patterns. explain : bool Whether to keep track of cues for each entity. use_sections : bool, by default `False` Whether to use annotated sections (namely `ant\u00e9c\u00e9dents familiaux`). \"\"\" defaults = dict ( family = family , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , family : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( family = family , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" ) @staticmethod def set_extensions () -> None : if not Token . has_extension ( \"family\" ): Token . set_extension ( \"family\" , default = False ) if not Token . has_extension ( \"family_\" ): Token . set_extension ( \"family_\" , getter = lambda token : \"FAMILY\" if token . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family\" ): Span . set_extension ( \"family\" , default = False ) if not Span . has_extension ( \"family_\" ): Span . set_extension ( \"family_\" , getter = lambda span : \"FAMILY\" if span . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family_cues\" ): Span . set_extension ( \"family_cues\" , default = []) if not Doc . has_extension ( \"family\" ): Doc . set_extension ( \"family\" , default = []) def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to family context. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for context \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"FAMILY\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents familiaux\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"family\" ) cues += sub_sections if not cues : continue family = bool ( cues ) if not family : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . family = True for ent in ents : ent . _ . family = True if self . explain : ent . _ . family_cues += cues if not self . on_ents_only : for token in ent : token . _ . family = True return doc process ( doc ) Finds entities related to family context. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/family/family.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to family context. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for context \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"FAMILY\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents familiaux\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"family\" ) cues += sub_sections if not cues : continue family = bool ( cues ) if not family : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . family = True for ent in ents : ent . _ . family = True if self . explain : ent . _ . family_cues += cues if not self . on_ents_only : for token in ent : token . _ . family = True return doc","title":"family"},{"location":"reference/pipelines/qualifiers/family/family/#edsnlppipelinesqualifiersfamilyfamily","text":"","title":"edsnlp.pipelines.qualifiers.family.family"},{"location":"reference/pipelines/qualifiers/family/family/#edsnlp.pipelines.qualifiers.family.family.FamilyContext","text":"Bases: Qualifier Implements a family context detection algorithm. The components looks for terms indicating family references in the text. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language family List of terms indicating family reference. TYPE: Optional[List[str]] terminations List of termination terms, to separate syntagmas. TYPE: Optional[List[str]] attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool regex A dictionnary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] explain Whether to keep track of cues for each entity. TYPE: bool use_sections Whether to use annotated sections (namely ant\u00e9c\u00e9dents familiaux ). TYPE: bool, by default Source code in edsnlp/pipelines/qualifiers/family/family.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class FamilyContext ( Qualifier ): \"\"\" Implements a family context detection algorithm. The components looks for terms indicating family references in the text. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. family : Optional[List[str]] List of terms indicating family reference. terminations : Optional[List[str]] List of termination terms, to separate syntagmas. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. regex : Optional[Dict[str, Union[List[str], str]]] A dictionnary of regex patterns. explain : bool Whether to keep track of cues for each entity. use_sections : bool, by default `False` Whether to use annotated sections (namely `ant\u00e9c\u00e9dents familiaux`). \"\"\" defaults = dict ( family = family , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , family : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( family = family , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" ) @staticmethod def set_extensions () -> None : if not Token . has_extension ( \"family\" ): Token . set_extension ( \"family\" , default = False ) if not Token . has_extension ( \"family_\" ): Token . set_extension ( \"family_\" , getter = lambda token : \"FAMILY\" if token . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family\" ): Span . set_extension ( \"family\" , default = False ) if not Span . has_extension ( \"family_\" ): Span . set_extension ( \"family_\" , getter = lambda span : \"FAMILY\" if span . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family_cues\" ): Span . set_extension ( \"family_cues\" , default = []) if not Doc . has_extension ( \"family\" ): Doc . set_extension ( \"family\" , default = []) def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to family context. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for context \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"FAMILY\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents familiaux\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"family\" ) cues += sub_sections if not cues : continue family = bool ( cues ) if not family : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . family = True for ent in ents : ent . _ . family = True if self . explain : ent . _ . family_cues += cues if not self . on_ents_only : for token in ent : token . _ . family = True return doc","title":"FamilyContext"},{"location":"reference/pipelines/qualifiers/family/family/#edsnlp.pipelines.qualifiers.family.family.FamilyContext.process","text":"Finds entities related to family context. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/family/family.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to family context. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for context \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"FAMILY\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents familiaux\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"family\" ) cues += sub_sections if not cues : continue family = bool ( cues ) if not family : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . family = True for ent in ents : ent . _ . family = True if self . explain : ent . _ . family_cues += cues if not self . on_ents_only : for token in ent : token . _ . family = True return doc","title":"process()"},{"location":"reference/pipelines/qualifiers/family/patterns/","text":"edsnlp.pipelines.qualifiers.family.patterns","title":"patterns"},{"location":"reference/pipelines/qualifiers/family/patterns/#edsnlppipelinesqualifiersfamilypatterns","text":"","title":"edsnlp.pipelines.qualifiers.family.patterns"},{"location":"reference/pipelines/qualifiers/history/","text":"edsnlp.pipelines.qualifiers.history","title":"`edsnlp.pipelines.qualifiers.history`"},{"location":"reference/pipelines/qualifiers/history/#edsnlppipelinesqualifiershistory","text":"","title":"edsnlp.pipelines.qualifiers.history"},{"location":"reference/pipelines/qualifiers/history/factory/","text":"edsnlp.pipelines.qualifiers.history.factory","title":"factory"},{"location":"reference/pipelines/qualifiers/history/factory/#edsnlppipelinesqualifiershistoryfactory","text":"","title":"edsnlp.pipelines.qualifiers.history.factory"},{"location":"reference/pipelines/qualifiers/history/history/","text":"edsnlp.pipelines.qualifiers.history.history History Bases: Qualifier Implements an history detection algorithm. The components looks for terms indicating history in the text. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language history List of terms indicating medical history reference. TYPE: Optional[List[str]] termination List of syntagme termination terms. TYPE: Optional[List[str]] use_sections Whether to use section pipeline to detect medical history section. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool regex A dictionnary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/history/history.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class History ( Qualifier ): \"\"\" Implements an history detection algorithm. The components looks for terms indicating history in the text. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. history : Optional[List[str]] List of terms indicating medical history reference. termination : Optional[List[str]] List of syntagme termination terms. use_sections : bool Whether to use section pipeline to detect medical history section. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. regex : Optional[Dict[str, Union[List[str], str]]] A dictionnary of regex patterns. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( history = history , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , history : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( history = history , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" ) @staticmethod def set_extensions () -> None : if not Token . has_extension ( \"history\" ): Token . set_extension ( \"history\" , default = False ) if not Token . has_extension ( \"antecedents\" ): Token . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Token . has_extension ( \"antecedent\" ): Token . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Token . has_extension ( \"history_\" ): Token . set_extension ( \"history_\" , getter = lambda token : \"ATCD\" if token . _ . history else \"CURRENT\" , ) if not Token . has_extension ( \"antecedents_\" ): Token . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Token . has_extension ( \"antecedent_\" ): Token . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) if not Span . has_extension ( \"history\" ): Span . set_extension ( \"history\" , default = False ) if not Span . has_extension ( \"antecedents\" ): Span . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Span . has_extension ( \"antecedent\" ): Span . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Span . has_extension ( \"history_\" ): Span . set_extension ( \"history_\" , getter = lambda span : \"ATCD\" if span . _ . history else \"CURRENT\" , ) if not Span . has_extension ( \"antecedents_\" ): Span . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Span . has_extension ( \"antecedent_\" ): Span . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) if not Span . has_extension ( \"history_cues\" ): Span . set_extension ( \"history_cues\" , default = []) if not Span . has_extension ( \"antecedents_cues\" ): Span . set_extension ( \"antecedents_cues\" , getter = deprecated_getter_factory ( \"antecedents_cues\" , \"history_cues\" ), ) if not Span . has_extension ( \"antecedent_cues\" ): Span . set_extension ( \"antecedent_cues\" , getter = deprecated_getter_factory ( \"antecedent_cues\" , \"history_cues\" ), ) def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to history. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for history \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"ATCD\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"history\" ) cues += sub_sections history = bool ( cues ) if not self . on_ents_only : for token in doc [ start : end ]: token . _ . history = history for ent in ents : ent . _ . history = ent . _ . history or history if self . explain : ent . _ . history_cues += cues if not self . on_ents_only and ent . _ . history : for token in ent : token . _ . history = True return doc process ( doc ) Finds entities related to history. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for history Source code in edsnlp/pipelines/qualifiers/history/history.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to history. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for history \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"ATCD\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"history\" ) cues += sub_sections history = bool ( cues ) if not self . on_ents_only : for token in doc [ start : end ]: token . _ . history = history for ent in ents : ent . _ . history = ent . _ . history or history if self . explain : ent . _ . history_cues += cues if not self . on_ents_only and ent . _ . history : for token in ent : token . _ . history = True return doc","title":"history"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlppipelinesqualifiershistoryhistory","text":"","title":"edsnlp.pipelines.qualifiers.history.history"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History","text":"Bases: Qualifier Implements an history detection algorithm. The components looks for terms indicating history in the text. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language history List of terms indicating medical history reference. TYPE: Optional[List[str]] termination List of syntagme termination terms. TYPE: Optional[List[str]] use_sections Whether to use section pipeline to detect medical history section. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool regex A dictionnary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/history/history.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class History ( Qualifier ): \"\"\" Implements an history detection algorithm. The components looks for terms indicating history in the text. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. history : Optional[List[str]] List of terms indicating medical history reference. termination : Optional[List[str]] List of syntagme termination terms. use_sections : bool Whether to use section pipeline to detect medical history section. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. regex : Optional[Dict[str, Union[List[str], str]]] A dictionnary of regex patterns. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( history = history , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , history : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( history = history , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" ) @staticmethod def set_extensions () -> None : if not Token . has_extension ( \"history\" ): Token . set_extension ( \"history\" , default = False ) if not Token . has_extension ( \"antecedents\" ): Token . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Token . has_extension ( \"antecedent\" ): Token . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Token . has_extension ( \"history_\" ): Token . set_extension ( \"history_\" , getter = lambda token : \"ATCD\" if token . _ . history else \"CURRENT\" , ) if not Token . has_extension ( \"antecedents_\" ): Token . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Token . has_extension ( \"antecedent_\" ): Token . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) if not Span . has_extension ( \"history\" ): Span . set_extension ( \"history\" , default = False ) if not Span . has_extension ( \"antecedents\" ): Span . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Span . has_extension ( \"antecedent\" ): Span . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Span . has_extension ( \"history_\" ): Span . set_extension ( \"history_\" , getter = lambda span : \"ATCD\" if span . _ . history else \"CURRENT\" , ) if not Span . has_extension ( \"antecedents_\" ): Span . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Span . has_extension ( \"antecedent_\" ): Span . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) if not Span . has_extension ( \"history_cues\" ): Span . set_extension ( \"history_cues\" , default = []) if not Span . has_extension ( \"antecedents_cues\" ): Span . set_extension ( \"antecedents_cues\" , getter = deprecated_getter_factory ( \"antecedents_cues\" , \"history_cues\" ), ) if not Span . has_extension ( \"antecedent_cues\" ): Span . set_extension ( \"antecedent_cues\" , getter = deprecated_getter_factory ( \"antecedent_cues\" , \"history_cues\" ), ) def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to history. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for history \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"ATCD\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"history\" ) cues += sub_sections history = bool ( cues ) if not self . on_ents_only : for token in doc [ start : end ]: token . _ . history = history for ent in ents : ent . _ . history = ent . _ . history or history if self . explain : ent . _ . history_cues += cues if not self . on_ents_only and ent . _ . history : for token in ent : token . _ . history = True return doc","title":"History"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History.process","text":"Finds entities related to history. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for history Source code in edsnlp/pipelines/qualifiers/history/history.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to history. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for history \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"ATCD\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"history\" ) cues += sub_sections history = bool ( cues ) if not self . on_ents_only : for token in doc [ start : end ]: token . _ . history = history for ent in ents : ent . _ . history = ent . _ . history or history if self . explain : ent . _ . history_cues += cues if not self . on_ents_only and ent . _ . history : for token in ent : token . _ . history = True return doc","title":"process()"},{"location":"reference/pipelines/qualifiers/history/patterns/","text":"edsnlp.pipelines.qualifiers.history.patterns","title":"patterns"},{"location":"reference/pipelines/qualifiers/history/patterns/#edsnlppipelinesqualifiershistorypatterns","text":"","title":"edsnlp.pipelines.qualifiers.history.patterns"},{"location":"reference/pipelines/qualifiers/hypothesis/","text":"edsnlp.pipelines.qualifiers.hypothesis","title":"`edsnlp.pipelines.qualifiers.hypothesis`"},{"location":"reference/pipelines/qualifiers/hypothesis/#edsnlppipelinesqualifiershypothesis","text":"","title":"edsnlp.pipelines.qualifiers.hypothesis"},{"location":"reference/pipelines/qualifiers/hypothesis/factory/","text":"edsnlp.pipelines.qualifiers.hypothesis.factory","title":"factory"},{"location":"reference/pipelines/qualifiers/hypothesis/factory/#edsnlppipelinesqualifiershypothesisfactory","text":"","title":"edsnlp.pipelines.qualifiers.hypothesis.factory"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/","text":"edsnlp.pipelines.qualifiers.hypothesis.hypothesis Hypothesis Bases: Qualifier Hypothesis detection with spaCy. The component looks for five kinds of expressions in the text : preceding hypothesis, ie cues that precede a hypothetic expression following hypothesis, ie cues that follow a hypothetic expression pseudo hypothesis : contain a hypothesis cue, but are not hypothesis (eg \"pas de doute\"/\"no doubt\") hypothetic verbs : verbs indicating hypothesis (eg \"douter\") classic verbs conjugated to the conditional, thus indicating hypothesis PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language pseudo List of pseudo hypothesis cues. TYPE: Optional[List[str]] preceding List of preceding hypothesis cues TYPE: Optional[List[str]] following List of following hypothesis cues. TYPE: Optional[List[str]] verbs_hyp List of hypothetic verbs. TYPE: Optional[List[str]] verbs_eds List of mainstream verbs. TYPE: Optional[List[str]] filter_matches Whether to filter out overlapping matches. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool regex A dictionnary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 class Hypothesis ( Qualifier ): \"\"\" Hypothesis detection with spaCy. The component looks for five kinds of expressions in the text : - preceding hypothesis, ie cues that precede a hypothetic expression - following hypothesis, ie cues that follow a hypothetic expression - pseudo hypothesis : contain a hypothesis cue, but are not hypothesis (eg \"pas de doute\"/\"no doubt\") - hypothetic verbs : verbs indicating hypothesis (eg \"douter\") - classic verbs conjugated to the conditional, thus indicating hypothesis Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. pseudo : Optional[List[str]] List of pseudo hypothesis cues. preceding : Optional[List[str]] List of preceding hypothesis cues following : Optional[List[str]] List of following hypothesis cues. verbs_hyp : Optional[List[str]] List of hypothetic verbs. verbs_eds : Optional[List[str]] List of mainstream verbs. filter_matches : bool Whether to filter out overlapping matches. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. regex : Optional[Dict[str, Union[List[str], str]]] A dictionnary of regex patterns. \"\"\" defaults = dict ( following = following , preceding = preceding , pseudo = pseudo , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs_eds : Optional [ List [ str ]], verbs_hyp : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , ) terms [ \"verbs\" ] = self . load_verbs ( verbs_hyp = terms . pop ( \"verbs_hyp\" ), verbs_eds = terms . pop ( \"verbs_eds\" ), ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions () @staticmethod def set_extensions () -> None : if not Token . has_extension ( \"hypothesis\" ): Token . set_extension ( \"hypothesis\" , default = False ) if not Token . has_extension ( \"hypothesis_\" ): Token . set_extension ( \"hypothesis_\" , getter = lambda token : \"HYP\" if token . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis\" ): Span . set_extension ( \"hypothesis\" , default = False ) if not Span . has_extension ( \"hypothesis_\" ): Span . set_extension ( \"hypothesis_\" , getter = lambda span : \"HYP\" if span . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis_cues\" ): Span . set_extension ( \"hypothesis_cues\" , default = []) if not Doc . has_extension ( \"hypothesis\" ): Doc . set_extension ( \"hypothesis\" , default = []) def load_verbs ( self , verbs_hyp : List [ str ], verbs_eds : List [ str ], ) -> List [ str ]: \"\"\" Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. Parameters ---------- verbs_hyp: List of verbs that specifically imply an hypothesis. verbs_eds: List of general verbs. Returns ------- list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. \"\"\" classic_verbs = get_verbs ( verbs_eds ) classic_verbs = classic_verbs . loc [ classic_verbs [ \"mode\" ] == \"Conditionnel\" ] list_classic_verbs = list ( classic_verbs [ \"term\" ] . unique ()) hypo_verbs = get_verbs ( verbs_hyp ) list_hypo_verbs = list ( hypo_verbs [ \"term\" ] . unique ()) return list_hypo_verbs + list_classic_verbs def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to hypothesis. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for hypothesis \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) if not sub_preceding + sub_following + sub_verbs : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . hypothesis = any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] hypothesis = ent . _ . hypothesis or bool ( cues ) ent . _ . hypothesis = hypothesis if self . explain and hypothesis : ent . _ . hypothesis_cues += cues if not self . on_ents_only and hypothesis : for token in ent : token . _ . hypothesis = True return doc load_verbs ( verbs_hyp , verbs_eds ) Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. PARAMETER DESCRIPTION verbs_hyp TYPE: List [ str ] verbs_eds TYPE: List [ str ] RETURNS DESCRIPTION list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def load_verbs ( self , verbs_hyp : List [ str ], verbs_eds : List [ str ], ) -> List [ str ]: \"\"\" Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. Parameters ---------- verbs_hyp: List of verbs that specifically imply an hypothesis. verbs_eds: List of general verbs. Returns ------- list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. \"\"\" classic_verbs = get_verbs ( verbs_eds ) classic_verbs = classic_verbs . loc [ classic_verbs [ \"mode\" ] == \"Conditionnel\" ] list_classic_verbs = list ( classic_verbs [ \"term\" ] . unique ()) hypo_verbs = get_verbs ( verbs_hyp ) list_hypo_verbs = list ( hypo_verbs [ \"term\" ] . unique ()) return list_hypo_verbs + list_classic_verbs process ( doc ) Finds entities related to hypothesis. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to hypothesis. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for hypothesis \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) if not sub_preceding + sub_following + sub_verbs : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . hypothesis = any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] hypothesis = ent . _ . hypothesis or bool ( cues ) ent . _ . hypothesis = hypothesis if self . explain and hypothesis : ent . _ . hypothesis_cues += cues if not self . on_ents_only and hypothesis : for token in ent : token . _ . hypothesis = True return doc","title":"hypothesis"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlppipelinesqualifiershypothesishypothesis","text":"","title":"edsnlp.pipelines.qualifiers.hypothesis.hypothesis"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis","text":"Bases: Qualifier Hypothesis detection with spaCy. The component looks for five kinds of expressions in the text : preceding hypothesis, ie cues that precede a hypothetic expression following hypothesis, ie cues that follow a hypothetic expression pseudo hypothesis : contain a hypothesis cue, but are not hypothesis (eg \"pas de doute\"/\"no doubt\") hypothetic verbs : verbs indicating hypothesis (eg \"douter\") classic verbs conjugated to the conditional, thus indicating hypothesis PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language pseudo List of pseudo hypothesis cues. TYPE: Optional[List[str]] preceding List of preceding hypothesis cues TYPE: Optional[List[str]] following List of following hypothesis cues. TYPE: Optional[List[str]] verbs_hyp List of hypothetic verbs. TYPE: Optional[List[str]] verbs_eds List of mainstream verbs. TYPE: Optional[List[str]] filter_matches Whether to filter out overlapping matches. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool regex A dictionnary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 class Hypothesis ( Qualifier ): \"\"\" Hypothesis detection with spaCy. The component looks for five kinds of expressions in the text : - preceding hypothesis, ie cues that precede a hypothetic expression - following hypothesis, ie cues that follow a hypothetic expression - pseudo hypothesis : contain a hypothesis cue, but are not hypothesis (eg \"pas de doute\"/\"no doubt\") - hypothetic verbs : verbs indicating hypothesis (eg \"douter\") - classic verbs conjugated to the conditional, thus indicating hypothesis Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. pseudo : Optional[List[str]] List of pseudo hypothesis cues. preceding : Optional[List[str]] List of preceding hypothesis cues following : Optional[List[str]] List of following hypothesis cues. verbs_hyp : Optional[List[str]] List of hypothetic verbs. verbs_eds : Optional[List[str]] List of mainstream verbs. filter_matches : bool Whether to filter out overlapping matches. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. regex : Optional[Dict[str, Union[List[str], str]]] A dictionnary of regex patterns. \"\"\" defaults = dict ( following = following , preceding = preceding , pseudo = pseudo , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs_eds : Optional [ List [ str ]], verbs_hyp : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , ) terms [ \"verbs\" ] = self . load_verbs ( verbs_hyp = terms . pop ( \"verbs_hyp\" ), verbs_eds = terms . pop ( \"verbs_eds\" ), ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions () @staticmethod def set_extensions () -> None : if not Token . has_extension ( \"hypothesis\" ): Token . set_extension ( \"hypothesis\" , default = False ) if not Token . has_extension ( \"hypothesis_\" ): Token . set_extension ( \"hypothesis_\" , getter = lambda token : \"HYP\" if token . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis\" ): Span . set_extension ( \"hypothesis\" , default = False ) if not Span . has_extension ( \"hypothesis_\" ): Span . set_extension ( \"hypothesis_\" , getter = lambda span : \"HYP\" if span . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis_cues\" ): Span . set_extension ( \"hypothesis_cues\" , default = []) if not Doc . has_extension ( \"hypothesis\" ): Doc . set_extension ( \"hypothesis\" , default = []) def load_verbs ( self , verbs_hyp : List [ str ], verbs_eds : List [ str ], ) -> List [ str ]: \"\"\" Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. Parameters ---------- verbs_hyp: List of verbs that specifically imply an hypothesis. verbs_eds: List of general verbs. Returns ------- list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. \"\"\" classic_verbs = get_verbs ( verbs_eds ) classic_verbs = classic_verbs . loc [ classic_verbs [ \"mode\" ] == \"Conditionnel\" ] list_classic_verbs = list ( classic_verbs [ \"term\" ] . unique ()) hypo_verbs = get_verbs ( verbs_hyp ) list_hypo_verbs = list ( hypo_verbs [ \"term\" ] . unique ()) return list_hypo_verbs + list_classic_verbs def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to hypothesis. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for hypothesis \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) if not sub_preceding + sub_following + sub_verbs : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . hypothesis = any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] hypothesis = ent . _ . hypothesis or bool ( cues ) ent . _ . hypothesis = hypothesis if self . explain and hypothesis : ent . _ . hypothesis_cues += cues if not self . on_ents_only and hypothesis : for token in ent : token . _ . hypothesis = True return doc","title":"Hypothesis"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis.load_verbs","text":"Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. PARAMETER DESCRIPTION verbs_hyp TYPE: List [ str ] verbs_eds TYPE: List [ str ] RETURNS DESCRIPTION list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def load_verbs ( self , verbs_hyp : List [ str ], verbs_eds : List [ str ], ) -> List [ str ]: \"\"\" Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. Parameters ---------- verbs_hyp: List of verbs that specifically imply an hypothesis. verbs_eds: List of general verbs. Returns ------- list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. \"\"\" classic_verbs = get_verbs ( verbs_eds ) classic_verbs = classic_verbs . loc [ classic_verbs [ \"mode\" ] == \"Conditionnel\" ] list_classic_verbs = list ( classic_verbs [ \"term\" ] . unique ()) hypo_verbs = get_verbs ( verbs_hyp ) list_hypo_verbs = list ( hypo_verbs [ \"term\" ] . unique ()) return list_hypo_verbs + list_classic_verbs","title":"load_verbs()"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis.process","text":"Finds entities related to hypothesis. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to hypothesis. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for hypothesis \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) if not sub_preceding + sub_following + sub_verbs : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . hypothesis = any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] hypothesis = ent . _ . hypothesis or bool ( cues ) ent . _ . hypothesis = hypothesis if self . explain and hypothesis : ent . _ . hypothesis_cues += cues if not self . on_ents_only and hypothesis : for token in ent : token . _ . hypothesis = True return doc","title":"process()"},{"location":"reference/pipelines/qualifiers/hypothesis/patterns/","text":"edsnlp.pipelines.qualifiers.hypothesis.patterns","title":"patterns"},{"location":"reference/pipelines/qualifiers/hypothesis/patterns/#edsnlppipelinesqualifiershypothesispatterns","text":"","title":"edsnlp.pipelines.qualifiers.hypothesis.patterns"},{"location":"reference/pipelines/qualifiers/negation/","text":"edsnlp.pipelines.qualifiers.negation","title":"`edsnlp.pipelines.qualifiers.negation`"},{"location":"reference/pipelines/qualifiers/negation/#edsnlppipelinesqualifiersnegation","text":"","title":"edsnlp.pipelines.qualifiers.negation"},{"location":"reference/pipelines/qualifiers/negation/factory/","text":"edsnlp.pipelines.qualifiers.negation.factory","title":"factory"},{"location":"reference/pipelines/qualifiers/negation/factory/#edsnlppipelinesqualifiersnegationfactory","text":"","title":"edsnlp.pipelines.qualifiers.negation.factory"},{"location":"reference/pipelines/qualifiers/negation/negation/","text":"edsnlp.pipelines.qualifiers.negation.negation Negation Bases: Qualifier Implements the NegEx algorithm. The component looks for five kinds of expressions in the text : preceding negations, ie cues that precede a negated expression following negations, ie cues that follow a negated expression pseudo negations : contain a negation cue, but are not negations (eg \"pas de doute\"/\"no doubt\") negation verbs, ie verbs that indicate a negation terminations, ie words that delimit propositions. The negation spans from the preceding cue to the termination. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language attr spaCy's attribute to use TYPE: str pseudo List of pseudo negation terms. TYPE: Optional[List[str]] preceding List of preceding negation terms TYPE: Optional[List[str]] following List of following negation terms. TYPE: Optional[List[str]] termination List of termination terms. TYPE: Optional[List[str]] verbs List of negation verbs. TYPE: Optional[List[str]] on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/negation/negation.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 class Negation ( Qualifier ): \"\"\" Implements the NegEx algorithm. The component looks for five kinds of expressions in the text : - preceding negations, ie cues that precede a negated expression - following negations, ie cues that follow a negated expression - pseudo negations : contain a negation cue, but are not negations (eg \"pas de doute\"/\"no doubt\") - negation verbs, ie verbs that indicate a negation - terminations, ie words that delimit propositions. The negation spans from the preceding cue to the termination. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. attr : str spaCy's attribute to use pseudo : Optional[List[str]] List of pseudo negation terms. preceding : Optional[List[str]] List of preceding negation terms following : Optional[List[str]] List of following negation terms. termination : Optional[List[str]] List of termination terms. verbs : Optional[List[str]] List of negation verbs. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( following = following , preceding = preceding , pseudo = pseudo , verbs = verbs , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs = verbs , ) terms [ \"verbs\" ] = self . load_verbs ( terms [ \"verbs\" ]) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions () @staticmethod def set_extensions () -> None : if not Token . has_extension ( \"negation\" ): Token . set_extension ( \"negation\" , default = False ) if not Token . has_extension ( \"negated\" ): Token . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Token . has_extension ( \"negation_\" ): Token . set_extension ( \"negation_\" , getter = lambda token : \"NEG\" if token . _ . negation else \"AFF\" , ) if not Token . has_extension ( \"polarity_\" ): Token . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Span . has_extension ( \"negation\" ): Span . set_extension ( \"negation\" , default = False ) if not Span . has_extension ( \"negated\" ): Span . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Span . has_extension ( \"negation_cues\" ): Span . set_extension ( \"negation_cues\" , default = []) if not Span . has_extension ( \"negation_\" ): Span . set_extension ( \"negation_\" , getter = lambda span : \"NEG\" if span . _ . negation else \"AFF\" , ) if not Span . has_extension ( \"polarity_\" ): Span . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Doc . has_extension ( \"negations\" ): Doc . set_extension ( \"negations\" , default = []) def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate negating verbs to specific tenses. Parameters ---------- verbs: list of negating verbs to conjugate Returns ------- list_neg_verbs: List of negating verbs conjugated to specific tenses. \"\"\" neg_verbs = get_verbs ( verbs ) neg_verbs = neg_verbs . loc [ (( neg_verbs [ \"mode\" ] == \"Indicatif\" ) & ( neg_verbs [ \"tense\" ] == \"Pr\u00e9sent\" )) | ( neg_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_neg_verbs = list ( neg_verbs [ \"term\" ] . unique ()) return list_neg_verbs def annotate_entity ( self , ent : Span , sub_preceding : List [ Span ], sub_following : List [ Span ], ) -> None : \"\"\" Annotate entities using preceding and following negations. Parameters ---------- ent : Span Entity to annotate sub_preceding : List[Span] List of preceding negations cues sub_following : List[Span] List of following negations cues \"\"\" if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] negation = ent . _ . negation or bool ( cues ) ent . _ . negation = negation if self . explain and negation : ent . _ . negation_cues += cues if not self . on_ents_only and negation : for token in ent : token . _ . negation = True def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to negation. Parameters ---------- doc: spaCy `Doc` object Returns ------- doc: spaCy `Doc` object, annotated for negation \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) # Verbs precede negated content sub_preceding += get_spans ( sub_matches , \"verbs\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . negation = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : self . annotate_entity ( ent = ent , sub_preceding = sub_preceding , sub_following = sub_following , ) return doc def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc ) load_verbs ( verbs ) Conjugate negating verbs to specific tenses. PARAMETER DESCRIPTION verbs TYPE: List [ str ] RETURNS DESCRIPTION list_neg_verbs Source code in edsnlp/pipelines/qualifiers/negation/negation.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate negating verbs to specific tenses. Parameters ---------- verbs: list of negating verbs to conjugate Returns ------- list_neg_verbs: List of negating verbs conjugated to specific tenses. \"\"\" neg_verbs = get_verbs ( verbs ) neg_verbs = neg_verbs . loc [ (( neg_verbs [ \"mode\" ] == \"Indicatif\" ) & ( neg_verbs [ \"tense\" ] == \"Pr\u00e9sent\" )) | ( neg_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_neg_verbs = list ( neg_verbs [ \"term\" ] . unique ()) return list_neg_verbs annotate_entity ( ent , sub_preceding , sub_following ) Annotate entities using preceding and following negations. PARAMETER DESCRIPTION ent Entity to annotate TYPE: Span sub_preceding List of preceding negations cues TYPE: List[Span] sub_following List of following negations cues TYPE: List[Span] Source code in edsnlp/pipelines/qualifiers/negation/negation.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def annotate_entity ( self , ent : Span , sub_preceding : List [ Span ], sub_following : List [ Span ], ) -> None : \"\"\" Annotate entities using preceding and following negations. Parameters ---------- ent : Span Entity to annotate sub_preceding : List[Span] List of preceding negations cues sub_following : List[Span] List of following negations cues \"\"\" if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] negation = ent . _ . negation or bool ( cues ) ent . _ . negation = negation if self . explain and negation : ent . _ . negation_cues += cues if not self . on_ents_only and negation : for token in ent : token . _ . negation = True process ( doc ) Finds entities related to negation. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/negation/negation.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to negation. Parameters ---------- doc: spaCy `Doc` object Returns ------- doc: spaCy `Doc` object, annotated for negation \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) # Verbs precede negated content sub_preceding += get_spans ( sub_matches , \"verbs\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . negation = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : self . annotate_entity ( ent = ent , sub_preceding = sub_preceding , sub_following = sub_following , ) return doc","title":"negation"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlppipelinesqualifiersnegationnegation","text":"","title":"edsnlp.pipelines.qualifiers.negation.negation"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation","text":"Bases: Qualifier Implements the NegEx algorithm. The component looks for five kinds of expressions in the text : preceding negations, ie cues that precede a negated expression following negations, ie cues that follow a negated expression pseudo negations : contain a negation cue, but are not negations (eg \"pas de doute\"/\"no doubt\") negation verbs, ie verbs that indicate a negation terminations, ie words that delimit propositions. The negation spans from the preceding cue to the termination. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language attr spaCy's attribute to use TYPE: str pseudo List of pseudo negation terms. TYPE: Optional[List[str]] preceding List of preceding negation terms TYPE: Optional[List[str]] following List of following negation terms. TYPE: Optional[List[str]] termination List of termination terms. TYPE: Optional[List[str]] verbs List of negation verbs. TYPE: Optional[List[str]] on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/negation/negation.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 class Negation ( Qualifier ): \"\"\" Implements the NegEx algorithm. The component looks for five kinds of expressions in the text : - preceding negations, ie cues that precede a negated expression - following negations, ie cues that follow a negated expression - pseudo negations : contain a negation cue, but are not negations (eg \"pas de doute\"/\"no doubt\") - negation verbs, ie verbs that indicate a negation - terminations, ie words that delimit propositions. The negation spans from the preceding cue to the termination. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. attr : str spaCy's attribute to use pseudo : Optional[List[str]] List of pseudo negation terms. preceding : Optional[List[str]] List of preceding negation terms following : Optional[List[str]] List of following negation terms. termination : Optional[List[str]] List of termination terms. verbs : Optional[List[str]] List of negation verbs. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( following = following , preceding = preceding , pseudo = pseudo , verbs = verbs , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs = verbs , ) terms [ \"verbs\" ] = self . load_verbs ( terms [ \"verbs\" ]) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions () @staticmethod def set_extensions () -> None : if not Token . has_extension ( \"negation\" ): Token . set_extension ( \"negation\" , default = False ) if not Token . has_extension ( \"negated\" ): Token . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Token . has_extension ( \"negation_\" ): Token . set_extension ( \"negation_\" , getter = lambda token : \"NEG\" if token . _ . negation else \"AFF\" , ) if not Token . has_extension ( \"polarity_\" ): Token . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Span . has_extension ( \"negation\" ): Span . set_extension ( \"negation\" , default = False ) if not Span . has_extension ( \"negated\" ): Span . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Span . has_extension ( \"negation_cues\" ): Span . set_extension ( \"negation_cues\" , default = []) if not Span . has_extension ( \"negation_\" ): Span . set_extension ( \"negation_\" , getter = lambda span : \"NEG\" if span . _ . negation else \"AFF\" , ) if not Span . has_extension ( \"polarity_\" ): Span . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Doc . has_extension ( \"negations\" ): Doc . set_extension ( \"negations\" , default = []) def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate negating verbs to specific tenses. Parameters ---------- verbs: list of negating verbs to conjugate Returns ------- list_neg_verbs: List of negating verbs conjugated to specific tenses. \"\"\" neg_verbs = get_verbs ( verbs ) neg_verbs = neg_verbs . loc [ (( neg_verbs [ \"mode\" ] == \"Indicatif\" ) & ( neg_verbs [ \"tense\" ] == \"Pr\u00e9sent\" )) | ( neg_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_neg_verbs = list ( neg_verbs [ \"term\" ] . unique ()) return list_neg_verbs def annotate_entity ( self , ent : Span , sub_preceding : List [ Span ], sub_following : List [ Span ], ) -> None : \"\"\" Annotate entities using preceding and following negations. Parameters ---------- ent : Span Entity to annotate sub_preceding : List[Span] List of preceding negations cues sub_following : List[Span] List of following negations cues \"\"\" if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] negation = ent . _ . negation or bool ( cues ) ent . _ . negation = negation if self . explain and negation : ent . _ . negation_cues += cues if not self . on_ents_only and negation : for token in ent : token . _ . negation = True def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to negation. Parameters ---------- doc: spaCy `Doc` object Returns ------- doc: spaCy `Doc` object, annotated for negation \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) # Verbs precede negated content sub_preceding += get_spans ( sub_matches , \"verbs\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . negation = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : self . annotate_entity ( ent = ent , sub_preceding = sub_preceding , sub_following = sub_following , ) return doc def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc )","title":"Negation"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.load_verbs","text":"Conjugate negating verbs to specific tenses. PARAMETER DESCRIPTION verbs TYPE: List [ str ] RETURNS DESCRIPTION list_neg_verbs Source code in edsnlp/pipelines/qualifiers/negation/negation.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate negating verbs to specific tenses. Parameters ---------- verbs: list of negating verbs to conjugate Returns ------- list_neg_verbs: List of negating verbs conjugated to specific tenses. \"\"\" neg_verbs = get_verbs ( verbs ) neg_verbs = neg_verbs . loc [ (( neg_verbs [ \"mode\" ] == \"Indicatif\" ) & ( neg_verbs [ \"tense\" ] == \"Pr\u00e9sent\" )) | ( neg_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_neg_verbs = list ( neg_verbs [ \"term\" ] . unique ()) return list_neg_verbs","title":"load_verbs()"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.annotate_entity","text":"Annotate entities using preceding and following negations. PARAMETER DESCRIPTION ent Entity to annotate TYPE: Span sub_preceding List of preceding negations cues TYPE: List[Span] sub_following List of following negations cues TYPE: List[Span] Source code in edsnlp/pipelines/qualifiers/negation/negation.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def annotate_entity ( self , ent : Span , sub_preceding : List [ Span ], sub_following : List [ Span ], ) -> None : \"\"\" Annotate entities using preceding and following negations. Parameters ---------- ent : Span Entity to annotate sub_preceding : List[Span] List of preceding negations cues sub_following : List[Span] List of following negations cues \"\"\" if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] negation = ent . _ . negation or bool ( cues ) ent . _ . negation = negation if self . explain and negation : ent . _ . negation_cues += cues if not self . on_ents_only and negation : for token in ent : token . _ . negation = True","title":"annotate_entity()"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.process","text":"Finds entities related to negation. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/negation/negation.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to negation. Parameters ---------- doc: spaCy `Doc` object Returns ------- doc: spaCy `Doc` object, annotated for negation \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) # Verbs precede negated content sub_preceding += get_spans ( sub_matches , \"verbs\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . negation = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : self . annotate_entity ( ent = ent , sub_preceding = sub_preceding , sub_following = sub_following , ) return doc","title":"process()"},{"location":"reference/pipelines/qualifiers/negation/patterns/","text":"edsnlp.pipelines.qualifiers.negation.patterns","title":"patterns"},{"location":"reference/pipelines/qualifiers/negation/patterns/#edsnlppipelinesqualifiersnegationpatterns","text":"","title":"edsnlp.pipelines.qualifiers.negation.patterns"},{"location":"reference/pipelines/qualifiers/reported_speech/","text":"edsnlp.pipelines.qualifiers.reported_speech","title":"`edsnlp.pipelines.qualifiers.reported_speech`"},{"location":"reference/pipelines/qualifiers/reported_speech/#edsnlppipelinesqualifiersreported_speech","text":"","title":"edsnlp.pipelines.qualifiers.reported_speech"},{"location":"reference/pipelines/qualifiers/reported_speech/factory/","text":"edsnlp.pipelines.qualifiers.reported_speech.factory","title":"factory"},{"location":"reference/pipelines/qualifiers/reported_speech/factory/#edsnlppipelinesqualifiersreported_speechfactory","text":"","title":"edsnlp.pipelines.qualifiers.reported_speech.factory"},{"location":"reference/pipelines/qualifiers/reported_speech/patterns/","text":"edsnlp.pipelines.qualifiers.reported_speech.patterns","title":"patterns"},{"location":"reference/pipelines/qualifiers/reported_speech/patterns/#edsnlppipelinesqualifiersreported_speechpatterns","text":"","title":"edsnlp.pipelines.qualifiers.reported_speech.patterns"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/","text":"edsnlp.pipelines.qualifiers.reported_speech.reported_speech ReportedSpeech Bases: Qualifier Implements a reported speech detection algorithm. The components looks for terms indicating patient statements, and quotations to detect patient speech. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language quotation String gathering all quotation cues. TYPE: str verbs List of reported speech verbs. TYPE: List[str] following List of terms following a reported speech. TYPE: List[str] preceding List of terms preceding a reported speech. TYPE: List[str] filter_matches Whether to filter out overlapping matches. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class ReportedSpeech ( Qualifier ): \"\"\" Implements a reported speech detection algorithm. The components looks for terms indicating patient statements, and quotations to detect patient speech. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. quotation : str String gathering all quotation cues. verbs : List[str] List of reported speech verbs. following : List[str] List of terms following a reported speech. preceding : List[str] List of terms preceding a reported speech. filter_matches : bool Whether to filter out overlapping matches. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( following = following , preceding = preceding , verbs = verbs , quotation = quotation , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], quotation : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , quotation = quotation , verbs = verbs , ) terms [ \"verbs\" ] = self . load_verbs ( terms [ \"verbs\" ]) quotation = terms . pop ( \"quotation\" ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . regex_matcher = RegexMatcher ( attr = attr ) self . regex_matcher . build_patterns ( dict ( quotation = quotation )) self . within_ents = within_ents self . set_extensions () @staticmethod def set_extensions () -> None : if not Token . has_extension ( \"reported_speech\" ): Token . set_extension ( \"reported_speech\" , default = False ) if not Token . has_extension ( \"reported_speech_\" ): Token . set_extension ( \"reported_speech_\" , getter = lambda token : \"REPORTED\" if token . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech\" ): Span . set_extension ( \"reported_speech\" , default = False ) if not Span . has_extension ( \"reported_speech_\" ): Span . set_extension ( \"reported_speech_\" , getter = lambda span : \"REPORTED\" if span . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech_cues\" ): Span . set_extension ( \"reported_speech_cues\" , default = []) if not Doc . has_extension ( \"rspeechs\" ): Doc . set_extension ( \"rspeechs\" , default = []) def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate reporting verbs to specific tenses (trhid person) Parameters ---------- verbs: list of reporting verbs to conjugate Returns ------- list_rep_verbs: List of reporting verbs conjugated to specific tenses. \"\"\" rep_verbs = get_verbs ( verbs ) rep_verbs = rep_verbs . loc [ ( ( rep_verbs [ \"mode\" ] == \"Indicatif\" ) & ( rep_verbs [ \"tense\" ] == \"Pr\u00e9sent\" ) & ( rep_verbs [ \"person\" ] . isin ([ \"3s\" , \"3p\" ])) ) | ( rep_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( rep_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_rep_verbs = list ( rep_verbs [ \"term\" ] . unique ()) return list_rep_verbs def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to reported speech. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for negation \"\"\" matches = self . get_matches ( doc ) matches += list ( self . regex_matcher ( doc , as_spans = True )) boundaries = self . _boundaries ( doc ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) sub_quotation = get_spans ( sub_matches , \"quotation\" ) if not sub_preceding + sub_following + sub_verbs + sub_quotation : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . reported_speech = ( any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) or any ( (( m . start < token . i ) & ( m . end > token . i + 1 )) for m in sub_quotation ) ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] cues += [ m for m in sub_quotation if ( m . start < ent . start ) & ( m . end > ent . end ) ] reported_speech = ent . _ . reported_speech or bool ( cues ) ent . _ . reported_speech = reported_speech if self . explain : ent . _ . reported_speech_cues += cues if not self . on_ents_only and reported_speech : for token in ent : token . _ . reported_speech = True return doc load_verbs ( verbs ) Conjugate reporting verbs to specific tenses (trhid person) PARAMETER DESCRIPTION verbs TYPE: List [ str ] RETURNS DESCRIPTION list_rep_verbs Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate reporting verbs to specific tenses (trhid person) Parameters ---------- verbs: list of reporting verbs to conjugate Returns ------- list_rep_verbs: List of reporting verbs conjugated to specific tenses. \"\"\" rep_verbs = get_verbs ( verbs ) rep_verbs = rep_verbs . loc [ ( ( rep_verbs [ \"mode\" ] == \"Indicatif\" ) & ( rep_verbs [ \"tense\" ] == \"Pr\u00e9sent\" ) & ( rep_verbs [ \"person\" ] . isin ([ \"3s\" , \"3p\" ])) ) | ( rep_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( rep_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_rep_verbs = list ( rep_verbs [ \"term\" ] . unique ()) return list_rep_verbs process ( doc ) Finds entities related to reported speech. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to reported speech. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for negation \"\"\" matches = self . get_matches ( doc ) matches += list ( self . regex_matcher ( doc , as_spans = True )) boundaries = self . _boundaries ( doc ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) sub_quotation = get_spans ( sub_matches , \"quotation\" ) if not sub_preceding + sub_following + sub_verbs + sub_quotation : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . reported_speech = ( any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) or any ( (( m . start < token . i ) & ( m . end > token . i + 1 )) for m in sub_quotation ) ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] cues += [ m for m in sub_quotation if ( m . start < ent . start ) & ( m . end > ent . end ) ] reported_speech = ent . _ . reported_speech or bool ( cues ) ent . _ . reported_speech = reported_speech if self . explain : ent . _ . reported_speech_cues += cues if not self . on_ents_only and reported_speech : for token in ent : token . _ . reported_speech = True return doc","title":"reported_speech"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlppipelinesqualifiersreported_speechreported_speech","text":"","title":"edsnlp.pipelines.qualifiers.reported_speech.reported_speech"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech","text":"Bases: Qualifier Implements a reported speech detection algorithm. The components looks for terms indicating patient statements, and quotations to detect patient speech. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language quotation String gathering all quotation cues. TYPE: str verbs List of reported speech verbs. TYPE: List[str] following List of terms following a reported speech. TYPE: List[str] preceding List of terms preceding a reported speech. TYPE: List[str] filter_matches Whether to filter out overlapping matches. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class ReportedSpeech ( Qualifier ): \"\"\" Implements a reported speech detection algorithm. The components looks for terms indicating patient statements, and quotations to detect patient speech. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. quotation : str String gathering all quotation cues. verbs : List[str] List of reported speech verbs. following : List[str] List of terms following a reported speech. preceding : List[str] List of terms preceding a reported speech. filter_matches : bool Whether to filter out overlapping matches. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( following = following , preceding = preceding , verbs = verbs , quotation = quotation , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], quotation : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , quotation = quotation , verbs = verbs , ) terms [ \"verbs\" ] = self . load_verbs ( terms [ \"verbs\" ]) quotation = terms . pop ( \"quotation\" ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . regex_matcher = RegexMatcher ( attr = attr ) self . regex_matcher . build_patterns ( dict ( quotation = quotation )) self . within_ents = within_ents self . set_extensions () @staticmethod def set_extensions () -> None : if not Token . has_extension ( \"reported_speech\" ): Token . set_extension ( \"reported_speech\" , default = False ) if not Token . has_extension ( \"reported_speech_\" ): Token . set_extension ( \"reported_speech_\" , getter = lambda token : \"REPORTED\" if token . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech\" ): Span . set_extension ( \"reported_speech\" , default = False ) if not Span . has_extension ( \"reported_speech_\" ): Span . set_extension ( \"reported_speech_\" , getter = lambda span : \"REPORTED\" if span . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech_cues\" ): Span . set_extension ( \"reported_speech_cues\" , default = []) if not Doc . has_extension ( \"rspeechs\" ): Doc . set_extension ( \"rspeechs\" , default = []) def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate reporting verbs to specific tenses (trhid person) Parameters ---------- verbs: list of reporting verbs to conjugate Returns ------- list_rep_verbs: List of reporting verbs conjugated to specific tenses. \"\"\" rep_verbs = get_verbs ( verbs ) rep_verbs = rep_verbs . loc [ ( ( rep_verbs [ \"mode\" ] == \"Indicatif\" ) & ( rep_verbs [ \"tense\" ] == \"Pr\u00e9sent\" ) & ( rep_verbs [ \"person\" ] . isin ([ \"3s\" , \"3p\" ])) ) | ( rep_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( rep_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_rep_verbs = list ( rep_verbs [ \"term\" ] . unique ()) return list_rep_verbs def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to reported speech. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for negation \"\"\" matches = self . get_matches ( doc ) matches += list ( self . regex_matcher ( doc , as_spans = True )) boundaries = self . _boundaries ( doc ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) sub_quotation = get_spans ( sub_matches , \"quotation\" ) if not sub_preceding + sub_following + sub_verbs + sub_quotation : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . reported_speech = ( any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) or any ( (( m . start < token . i ) & ( m . end > token . i + 1 )) for m in sub_quotation ) ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] cues += [ m for m in sub_quotation if ( m . start < ent . start ) & ( m . end > ent . end ) ] reported_speech = ent . _ . reported_speech or bool ( cues ) ent . _ . reported_speech = reported_speech if self . explain : ent . _ . reported_speech_cues += cues if not self . on_ents_only and reported_speech : for token in ent : token . _ . reported_speech = True return doc","title":"ReportedSpeech"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech.load_verbs","text":"Conjugate reporting verbs to specific tenses (trhid person) PARAMETER DESCRIPTION verbs TYPE: List [ str ] RETURNS DESCRIPTION list_rep_verbs Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate reporting verbs to specific tenses (trhid person) Parameters ---------- verbs: list of reporting verbs to conjugate Returns ------- list_rep_verbs: List of reporting verbs conjugated to specific tenses. \"\"\" rep_verbs = get_verbs ( verbs ) rep_verbs = rep_verbs . loc [ ( ( rep_verbs [ \"mode\" ] == \"Indicatif\" ) & ( rep_verbs [ \"tense\" ] == \"Pr\u00e9sent\" ) & ( rep_verbs [ \"person\" ] . isin ([ \"3s\" , \"3p\" ])) ) | ( rep_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( rep_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_rep_verbs = list ( rep_verbs [ \"term\" ] . unique ()) return list_rep_verbs","title":"load_verbs()"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech.process","text":"Finds entities related to reported speech. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to reported speech. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for negation \"\"\" matches = self . get_matches ( doc ) matches += list ( self . regex_matcher ( doc , as_spans = True )) boundaries = self . _boundaries ( doc ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) sub_quotation = get_spans ( sub_matches , \"quotation\" ) if not sub_preceding + sub_following + sub_verbs + sub_quotation : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . reported_speech = ( any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) or any ( (( m . start < token . i ) & ( m . end > token . i + 1 )) for m in sub_quotation ) ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] cues += [ m for m in sub_quotation if ( m . start < ent . start ) & ( m . end > ent . end ) ] reported_speech = ent . _ . reported_speech or bool ( cues ) ent . _ . reported_speech = reported_speech if self . explain : ent . _ . reported_speech_cues += cues if not self . on_ents_only and reported_speech : for token in ent : token . _ . reported_speech = True return doc","title":"process()"},{"location":"reference/processing/","text":"edsnlp.processing","title":"`edsnlp.processing`"},{"location":"reference/processing/#edsnlpprocessing","text":"","title":"edsnlp.processing"},{"location":"reference/processing/parallel/","text":"edsnlp.processing.parallel _define_nlp ( new_nlp ) Set the global nlp variable Doing it this way saves non negligeable amount of time Source code in edsnlp/processing/parallel.py 13 14 15 16 17 18 19 def _define_nlp ( new_nlp : Language ): \"\"\" Set the global nlp variable Doing it this way saves non negligeable amount of time \"\"\" global nlp nlp = new_nlp _chunker ( iterable , total_length , chunksize ) Takes an iterable and chunk it. Source code in edsnlp/processing/parallel.py 22 23 24 25 26 27 28 29 30 31 32 def _chunker ( iterable : Iterable , total_length : int , chunksize : int , ): \"\"\" Takes an iterable and chunk it. \"\"\" return ( iterable [ pos : pos + chunksize ] for pos in range ( 0 , total_length , chunksize ) ) pipe ( note , nlp , additional_spans = 'discarded' , extensions = [], chunksize = 100 , n_jobs =- 2 , progress_bar = True , ** pipe_kwargs ) Function to apply a spaCy pipe to a pandas DataFrame note by using multiprocessing PARAMETER DESCRIPTION note A pandas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: 'discarded' extensions Spans extensions to add to the extracted results: FOr instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] chunksize Batch size used to split tasks TYPE: int DEFAULT: 100 n_jobs Max number of parallel jobs. The default value uses the maximum number of available cores. TYPE: int DEFAULT: -2 progress_bar Whether to display a progress bar or not TYPE: bool DEFAULT: True **pipe_kwargs Arguments exposed in processing.pipe_generator are also available here DEFAULT: {} RETURNS DESCRIPTION DataFrame A pandas DataFrame with one line per extraction Source code in edsnlp/processing/parallel.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def pipe ( note : pd . DataFrame , nlp : Language , additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : ExtensionSchema = [], chunksize : int = 100 , n_jobs : int = - 2 , progress_bar : bool = True , ** pipe_kwargs , ): \"\"\" Function to apply a spaCy pipe to a pandas DataFrame note by using multiprocessing Parameters ---------- note : DataFrame A pandas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: FOr instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. chunksize: int, by default 100 Batch size used to split tasks n_jobs: int, by default -2 Max number of parallel jobs. The default value uses the maximum number of available cores. progress_bar: bool, by default True Whether to display a progress bar or not **pipe_kwargs: Arguments exposed in `processing.pipe_generator` are also available here Returns ------- DataFrame A pandas DataFrame with one line per extraction \"\"\" # Setting the nlp variable _define_nlp ( nlp ) verbose = 10 if progress_bar else 0 executor = Parallel ( n_jobs , backend = \"multiprocessing\" , prefer = \"processes\" , verbose = verbose ) executor . warn ( f \"Used nlp components: { nlp . component_names } \" ) pipe_kwargs [ \"additional_spans\" ] = additional_spans pipe_kwargs [ \"extensions\" ] = extensions if verbose : executor . warn ( f \" { int ( len ( note ) / chunksize ) } tasks to complete\" ) do = delayed ( _process_chunk ) tasks = ( do ( chunk , ** pipe_kwargs ) for chunk in _chunker ( note , len ( note ), chunksize = chunksize ) ) result = executor ( tasks ) out = _flatten ( result ) return pd . DataFrame ( out )","title":"parallel"},{"location":"reference/processing/parallel/#edsnlpprocessingparallel","text":"","title":"edsnlp.processing.parallel"},{"location":"reference/processing/parallel/#edsnlp.processing.parallel._define_nlp","text":"Set the global nlp variable Doing it this way saves non negligeable amount of time Source code in edsnlp/processing/parallel.py 13 14 15 16 17 18 19 def _define_nlp ( new_nlp : Language ): \"\"\" Set the global nlp variable Doing it this way saves non negligeable amount of time \"\"\" global nlp nlp = new_nlp","title":"_define_nlp()"},{"location":"reference/processing/parallel/#edsnlp.processing.parallel._chunker","text":"Takes an iterable and chunk it. Source code in edsnlp/processing/parallel.py 22 23 24 25 26 27 28 29 30 31 32 def _chunker ( iterable : Iterable , total_length : int , chunksize : int , ): \"\"\" Takes an iterable and chunk it. \"\"\" return ( iterable [ pos : pos + chunksize ] for pos in range ( 0 , total_length , chunksize ) )","title":"_chunker()"},{"location":"reference/processing/parallel/#edsnlp.processing.parallel.pipe","text":"Function to apply a spaCy pipe to a pandas DataFrame note by using multiprocessing PARAMETER DESCRIPTION note A pandas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: 'discarded' extensions Spans extensions to add to the extracted results: FOr instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] chunksize Batch size used to split tasks TYPE: int DEFAULT: 100 n_jobs Max number of parallel jobs. The default value uses the maximum number of available cores. TYPE: int DEFAULT: -2 progress_bar Whether to display a progress bar or not TYPE: bool DEFAULT: True **pipe_kwargs Arguments exposed in processing.pipe_generator are also available here DEFAULT: {} RETURNS DESCRIPTION DataFrame A pandas DataFrame with one line per extraction Source code in edsnlp/processing/parallel.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def pipe ( note : pd . DataFrame , nlp : Language , additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : ExtensionSchema = [], chunksize : int = 100 , n_jobs : int = - 2 , progress_bar : bool = True , ** pipe_kwargs , ): \"\"\" Function to apply a spaCy pipe to a pandas DataFrame note by using multiprocessing Parameters ---------- note : DataFrame A pandas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: FOr instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. chunksize: int, by default 100 Batch size used to split tasks n_jobs: int, by default -2 Max number of parallel jobs. The default value uses the maximum number of available cores. progress_bar: bool, by default True Whether to display a progress bar or not **pipe_kwargs: Arguments exposed in `processing.pipe_generator` are also available here Returns ------- DataFrame A pandas DataFrame with one line per extraction \"\"\" # Setting the nlp variable _define_nlp ( nlp ) verbose = 10 if progress_bar else 0 executor = Parallel ( n_jobs , backend = \"multiprocessing\" , prefer = \"processes\" , verbose = verbose ) executor . warn ( f \"Used nlp components: { nlp . component_names } \" ) pipe_kwargs [ \"additional_spans\" ] = additional_spans pipe_kwargs [ \"extensions\" ] = extensions if verbose : executor . warn ( f \" { int ( len ( note ) / chunksize ) } tasks to complete\" ) do = delayed ( _process_chunk ) tasks = ( do ( chunk , ** pipe_kwargs ) for chunk in _chunker ( note , len ( note ), chunksize = chunksize ) ) result = executor ( tasks ) out = _flatten ( result ) return pd . DataFrame ( out )","title":"pipe()"},{"location":"reference/processing/simple/","text":"edsnlp.processing.simple _df_to_spacy ( note ) Takes a pandas DataFrame and return a generator that can be used in nlp.pipe() . PARAMETER DESCRIPTION note A pandas DataFrame with at least note_text and note_id columns. A Doc object will be created for each line. TYPE: pd . DataFrame RETURNS DESCRIPTION generator A generator which items are of the form (text, context), with text being a string and context a dictionnary Source code in edsnlp/processing/simple.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def _df_to_spacy ( note : pd . DataFrame , ): \"\"\" Takes a pandas DataFrame and return a generator that can be used in `nlp.pipe()`. Parameters ---------- note: pd.DataFrame A pandas DataFrame with at least `note_text` and `note_id` columns. A `Doc` object will be created for each line. Returns ------- generator: A generator which items are of the form (text, context), with `text` being a string and `context` a dictionnary \"\"\" for col in [ \"note_text\" , \"note_id\" ]: if col not in note . columns : raise ValueError ( f \"No column named { repr ( col ) } found in df\" ) kept_note = note [[ \"note_text\" , \"note_id\" ]] if not Doc . has_extension ( \"note_id\" ): Doc . set_extension ( \"note_id\" , default = None ) yield from zip ( kept_note . note_text , kept_note . note_id ) _flatten ( list_of_lists ) Flatten a list of lists to a combined list. Source code in edsnlp/processing/simple.py 49 50 51 52 53 def _flatten ( list_of_lists : List [ List [ Any ]]): \"\"\" Flatten a list of lists to a combined list. \"\"\" return [ item for sublist in list_of_lists for item in sublist ] _full_schema ( doc , additional_spans = [], extensions = []) Function used when Parallelising tasks via joblib. Takes a Doc as input, and returns a list of serializable objects Note The parallelisation needs for output objects to be serializable : after splitting the task into separate jobs, intermediate results are saved on memory before being aggregated, thus the need to be serializable. For instance, spaCy's spans aren't serializable since they are merely a view of the parent document. Check the source code of this function for an example. Source code in edsnlp/processing/simple.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def _full_schema ( doc : Doc , additional_spans : List [ str ] = [], extensions : List [ str ] = [], ): \"\"\" Function used when Parallelising tasks via joblib. Takes a Doc as input, and returns a list of serializable objects !!! note The parallelisation needs for output objects to be **serializable**: after splitting the task into separate jobs, intermediate results are saved on memory before being aggregated, thus the need to be serializable. For instance, spaCy's spans aren't serializable since they are merely a *view* of the parent document. Check the source code of this function for an example. \"\"\" results = [] results . extend ( [ _single_schema ( ent , extensions = extensions , ) for ent in doc . ents if doc . ents ] ) for span_type in additional_spans : results . extend ( [ _single_schema ( ent , span_type = span_type , extensions = extensions , ) for ent in doc . spans [ span_type ] if doc . spans [ span_type ] ] ) return results pipe ( note , nlp , additional_spans = 'discarded' , extensions = [], batch_size = 1000 , progress_bar = True ) Function to apply a spaCy pipe to a pandas DataFrame note For a large DataFrame, prefer the parallel version. PARAMETER DESCRIPTION note A pandas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: 'discarded' extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] batch_size Batch size used by spaCy's pipe TYPE: int, by default 1000 DEFAULT: 1000 progress_bar Whether to display a progress bar or not TYPE: bool DEFAULT: True RETURNS DESCRIPTION DataFrame A pandas DataFrame with one line per extraction Source code in edsnlp/processing/simple.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def pipe ( note : pd . DataFrame , nlp : Language , additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : Union [ List [ str ], str ] = [], batch_size : int = 1000 , progress_bar : bool = True , ): \"\"\" Function to apply a spaCy pipe to a pandas DataFrame note For a large DataFrame, prefer the parallel version. Parameters ---------- note : DataFrame A pandas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. batch_size : int, by default 1000 Batch size used by spaCy's pipe progress_bar: bool, by default True Whether to display a progress bar or not Returns ------- DataFrame A pandas DataFrame with one line per extraction \"\"\" return pd . DataFrame ( _flatten ( _pipe_generator ( note = note , nlp = nlp , additional_spans = additional_spans , extensions = extensions , batch_size = batch_size , progress_bar = progress_bar , ) ) )","title":"simple"},{"location":"reference/processing/simple/#edsnlpprocessingsimple","text":"","title":"edsnlp.processing.simple"},{"location":"reference/processing/simple/#edsnlp.processing.simple._df_to_spacy","text":"Takes a pandas DataFrame and return a generator that can be used in nlp.pipe() . PARAMETER DESCRIPTION note A pandas DataFrame with at least note_text and note_id columns. A Doc object will be created for each line. TYPE: pd . DataFrame RETURNS DESCRIPTION generator A generator which items are of the form (text, context), with text being a string and context a dictionnary Source code in edsnlp/processing/simple.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def _df_to_spacy ( note : pd . DataFrame , ): \"\"\" Takes a pandas DataFrame and return a generator that can be used in `nlp.pipe()`. Parameters ---------- note: pd.DataFrame A pandas DataFrame with at least `note_text` and `note_id` columns. A `Doc` object will be created for each line. Returns ------- generator: A generator which items are of the form (text, context), with `text` being a string and `context` a dictionnary \"\"\" for col in [ \"note_text\" , \"note_id\" ]: if col not in note . columns : raise ValueError ( f \"No column named { repr ( col ) } found in df\" ) kept_note = note [[ \"note_text\" , \"note_id\" ]] if not Doc . has_extension ( \"note_id\" ): Doc . set_extension ( \"note_id\" , default = None ) yield from zip ( kept_note . note_text , kept_note . note_id )","title":"_df_to_spacy()"},{"location":"reference/processing/simple/#edsnlp.processing.simple._flatten","text":"Flatten a list of lists to a combined list. Source code in edsnlp/processing/simple.py 49 50 51 52 53 def _flatten ( list_of_lists : List [ List [ Any ]]): \"\"\" Flatten a list of lists to a combined list. \"\"\" return [ item for sublist in list_of_lists for item in sublist ]","title":"_flatten()"},{"location":"reference/processing/simple/#edsnlp.processing.simple._full_schema","text":"Function used when Parallelising tasks via joblib. Takes a Doc as input, and returns a list of serializable objects Note The parallelisation needs for output objects to be serializable : after splitting the task into separate jobs, intermediate results are saved on memory before being aggregated, thus the need to be serializable. For instance, spaCy's spans aren't serializable since they are merely a view of the parent document. Check the source code of this function for an example. Source code in edsnlp/processing/simple.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def _full_schema ( doc : Doc , additional_spans : List [ str ] = [], extensions : List [ str ] = [], ): \"\"\" Function used when Parallelising tasks via joblib. Takes a Doc as input, and returns a list of serializable objects !!! note The parallelisation needs for output objects to be **serializable**: after splitting the task into separate jobs, intermediate results are saved on memory before being aggregated, thus the need to be serializable. For instance, spaCy's spans aren't serializable since they are merely a *view* of the parent document. Check the source code of this function for an example. \"\"\" results = [] results . extend ( [ _single_schema ( ent , extensions = extensions , ) for ent in doc . ents if doc . ents ] ) for span_type in additional_spans : results . extend ( [ _single_schema ( ent , span_type = span_type , extensions = extensions , ) for ent in doc . spans [ span_type ] if doc . spans [ span_type ] ] ) return results","title":"_full_schema()"},{"location":"reference/processing/simple/#edsnlp.processing.simple.pipe","text":"Function to apply a spaCy pipe to a pandas DataFrame note For a large DataFrame, prefer the parallel version. PARAMETER DESCRIPTION note A pandas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: 'discarded' extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] batch_size Batch size used by spaCy's pipe TYPE: int, by default 1000 DEFAULT: 1000 progress_bar Whether to display a progress bar or not TYPE: bool DEFAULT: True RETURNS DESCRIPTION DataFrame A pandas DataFrame with one line per extraction Source code in edsnlp/processing/simple.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def pipe ( note : pd . DataFrame , nlp : Language , additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : Union [ List [ str ], str ] = [], batch_size : int = 1000 , progress_bar : bool = True , ): \"\"\" Function to apply a spaCy pipe to a pandas DataFrame note For a large DataFrame, prefer the parallel version. Parameters ---------- note : DataFrame A pandas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. batch_size : int, by default 1000 Batch size used by spaCy's pipe progress_bar: bool, by default True Whether to display a progress bar or not Returns ------- DataFrame A pandas DataFrame with one line per extraction \"\"\" return pd . DataFrame ( _flatten ( _pipe_generator ( note = note , nlp = nlp , additional_spans = additional_spans , extensions = extensions , batch_size = batch_size , progress_bar = progress_bar , ) ) )","title":"pipe()"},{"location":"reference/processing/spark/","text":"edsnlp.processing.spark pyspark_type_finder ( obj ) Returns (when possible) the PySpark type of any python object Source code in edsnlp/processing/spark.py 12 13 14 15 16 17 18 19 20 21 def pyspark_type_finder ( obj ): \"\"\" Returns (when possible) the PySpark type of any python object \"\"\" try : infered_type = T . _infer_type ( obj ) print ( f \"Infered type is { repr ( infered_type ) } \" ) return infered_type except TypeError : raise TypeError ( \"Cannot infer type for this object.\" ) pipe ( note , nlp , additional_spans = 'discarded' , extensions = []) Function to apply a spaCy pipe to a pyspark DataFrame note PARAMETER DESCRIPTION note A pyspark DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: 'discarded' extensions Spans extensions to add to the extracted results: FOr instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] RETURNS DESCRIPTION DataFrame A pyspark DataFrame with one line per extraction Source code in edsnlp/processing/spark.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def pipe ( note : DataFrame , nlp : Language , additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : List [ Tuple [ str , T . DataType ]] = [], ) -> DataFrame : \"\"\" Function to apply a spaCy pipe to a pyspark DataFrame note Parameters ---------- note : DataFrame A pyspark DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: FOr instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. Returns ------- DataFrame A pyspark DataFrame with one line per extraction \"\"\" spark = SparkSession . builder . enableHiveSupport () . getOrCreate () sc = spark . sparkContext nlp_bc = sc . broadcast ( nlp ) def _udf_factory ( additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : Dict [ str , T . DataType ] = dict (), ): schema = T . ArrayType ( T . StructType ( [ T . StructField ( \"lexical_variant\" , T . StringType (), False ), T . StructField ( \"label\" , T . StringType (), False ), T . StructField ( \"span_type\" , T . StringType (), True ), T . StructField ( \"start\" , T . IntegerType (), False ), T . StructField ( \"end\" , T . IntegerType (), False ), * [ T . StructField ( extension_name , extension_type , True ) for extension_name , extension_type in extensions . items () ], ] ) ) def f ( text , additional_spans = additional_spans , extensions = extensions , ): if text is None : return [] nlp = nlp_bc . value for _ , pipe in nlp . pipeline : if isinstance ( pipe , BaseComponent ): pipe . set_extensions () doc = nlp ( text ) ents = [] for ent in doc . ents : parsed_extensions = [ getattr ( ent . _ , extension ) for extension in extensions . keys () ] ents . append ( ( ent . text , ent . label_ , \"ents\" , ent . start_char , ent . end_char , * parsed_extensions , ) ) if additional_spans is None : return ents if type ( additional_spans ) == str : additional_spans = [ additional_spans ] for spans_name in additional_spans : for ent in doc . spans . get ( spans_name , []): parsed_extensions = [ getattr ( ent . _ , extension ) for extension in extensions . keys () ] ents . append ( ( ent . text , ent . label_ , spans_name , ent . start_char , ent . end_char , * parsed_extensions , ) ) return ents f_udf = F . udf ( partial ( f , additional_spans = additional_spans , extensions = extensions , ), schema , ) return f_udf matcher = _udf_factory ( additional_spans = additional_spans , extensions = extensions , ) note_nlp = note . withColumn ( \"matches\" , matcher ( note . note_text )) note_nlp = note_nlp . withColumn ( \"matches\" , F . explode ( note_nlp . matches )) note_nlp = note_nlp . select ( \"note_id\" , \"matches.*\" ) return note_nlp","title":"spark"},{"location":"reference/processing/spark/#edsnlpprocessingspark","text":"","title":"edsnlp.processing.spark"},{"location":"reference/processing/spark/#edsnlp.processing.spark.pyspark_type_finder","text":"Returns (when possible) the PySpark type of any python object Source code in edsnlp/processing/spark.py 12 13 14 15 16 17 18 19 20 21 def pyspark_type_finder ( obj ): \"\"\" Returns (when possible) the PySpark type of any python object \"\"\" try : infered_type = T . _infer_type ( obj ) print ( f \"Infered type is { repr ( infered_type ) } \" ) return infered_type except TypeError : raise TypeError ( \"Cannot infer type for this object.\" )","title":"pyspark_type_finder()"},{"location":"reference/processing/spark/#edsnlp.processing.spark.pipe","text":"Function to apply a spaCy pipe to a pyspark DataFrame note PARAMETER DESCRIPTION note A pyspark DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: 'discarded' extensions Spans extensions to add to the extracted results: FOr instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] RETURNS DESCRIPTION DataFrame A pyspark DataFrame with one line per extraction Source code in edsnlp/processing/spark.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def pipe ( note : DataFrame , nlp : Language , additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : List [ Tuple [ str , T . DataType ]] = [], ) -> DataFrame : \"\"\" Function to apply a spaCy pipe to a pyspark DataFrame note Parameters ---------- note : DataFrame A pyspark DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: FOr instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. Returns ------- DataFrame A pyspark DataFrame with one line per extraction \"\"\" spark = SparkSession . builder . enableHiveSupport () . getOrCreate () sc = spark . sparkContext nlp_bc = sc . broadcast ( nlp ) def _udf_factory ( additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : Dict [ str , T . DataType ] = dict (), ): schema = T . ArrayType ( T . StructType ( [ T . StructField ( \"lexical_variant\" , T . StringType (), False ), T . StructField ( \"label\" , T . StringType (), False ), T . StructField ( \"span_type\" , T . StringType (), True ), T . StructField ( \"start\" , T . IntegerType (), False ), T . StructField ( \"end\" , T . IntegerType (), False ), * [ T . StructField ( extension_name , extension_type , True ) for extension_name , extension_type in extensions . items () ], ] ) ) def f ( text , additional_spans = additional_spans , extensions = extensions , ): if text is None : return [] nlp = nlp_bc . value for _ , pipe in nlp . pipeline : if isinstance ( pipe , BaseComponent ): pipe . set_extensions () doc = nlp ( text ) ents = [] for ent in doc . ents : parsed_extensions = [ getattr ( ent . _ , extension ) for extension in extensions . keys () ] ents . append ( ( ent . text , ent . label_ , \"ents\" , ent . start_char , ent . end_char , * parsed_extensions , ) ) if additional_spans is None : return ents if type ( additional_spans ) == str : additional_spans = [ additional_spans ] for spans_name in additional_spans : for ent in doc . spans . get ( spans_name , []): parsed_extensions = [ getattr ( ent . _ , extension ) for extension in extensions . keys () ] ents . append ( ( ent . text , ent . label_ , spans_name , ent . start_char , ent . end_char , * parsed_extensions , ) ) return ents f_udf = F . udf ( partial ( f , additional_spans = additional_spans , extensions = extensions , ), schema , ) return f_udf matcher = _udf_factory ( additional_spans = additional_spans , extensions = extensions , ) note_nlp = note . withColumn ( \"matches\" , matcher ( note . note_text )) note_nlp = note_nlp . withColumn ( \"matches\" , F . explode ( note_nlp . matches )) note_nlp = note_nlp . select ( \"note_id\" , \"matches.*\" ) return note_nlp","title":"pipe()"},{"location":"reference/processing/wrapper/","text":"edsnlp.processing.wrapper pipe ( note , nlp , how = 'parallel' , additional_spans = 'discarded' , extensions = [], ** kwargs ) Function to apply a spaCy pipe to a pandas or pyspark DataFrame PARAMETER DESCRIPTION note A pandas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language how 3 methods are available here: how='simple' : Single process on a pandas DataFrame how='parallel' : Parallelised processes on a pandas DataFrame how='spark' : Distributed processes on a pyspark DataFrame TYPE: str, by default \"parallel\" DEFAULT: 'parallel' additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: 'discarded' extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] kwargs Additional parameters depending on the how argument. TYPE: Dict[str, Any] RETURNS DESCRIPTION Union[pd.DataFrame, ps.DataFrame] A DataFrame with one line per extraction Source code in edsnlp/processing/wrapper.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def pipe ( note : Union [ pd . DataFrame , ps . DataFrame ], nlp : Language , how : str = \"parallel\" , additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : ExtensionSchema = [], ** kwargs : Dict [ str , Any ], ) -> Union [ pd . DataFrame , ps . DataFrame ]: \"\"\" Function to apply a spaCy pipe to a pandas or pyspark DataFrame Parameters ---------- note : DataFrame A pandas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe how : str, by default \"parallel\" 3 methods are available here: - `how='simple'`: Single process on a pandas DataFrame - `how='parallel'`: Parallelised processes on a pandas DataFrame - `how='spark'`: Distributed processes on a pyspark DataFrame additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. kwargs : Dict[str, Any] Additional parameters depending on the `how` argument. Returns ------- Union[pd.DataFrame, ps.DataFrame] A DataFrame with one line per extraction \"\"\" if ( type ( note ) == ps . DataFrame ) and ( how != \"spark\" ): raise ValueError ( \"You are providing a pyspark DataFrame, please use `how='spark'`\" ) if how == \"simple\" : return simple_pipe ( note = note , nlp = nlp , additional_spans = additional_spans , extensions = extensions , ** kwargs , ) if how == \"parallel\" : return parallel_pipe ( note = note , nlp = nlp , additional_spans = additional_spans , extensions = extensions , ** kwargs , ) if how == \"spark\" : if type ( note ) == pd . DataFrame : raise ValueError ( \"\"\" You are providing a pandas DataFrame with `how='spark'`, which is incompatible. \"\"\" ) if extensions and type ( extensions ) != dict : raise ValueError ( \"\"\" When using Spark, you should provide extension names along with the extension type (as a dictionnary): `d[extension_name] = extension_type` \"\"\" # noqa W291 ) return spark_pipe ( note = note , nlp = nlp , additional_spans = additional_spans , extensions = extensions , ** kwargs , )","title":"wrapper"},{"location":"reference/processing/wrapper/#edsnlpprocessingwrapper","text":"","title":"edsnlp.processing.wrapper"},{"location":"reference/processing/wrapper/#edsnlp.processing.wrapper.pipe","text":"Function to apply a spaCy pipe to a pandas or pyspark DataFrame PARAMETER DESCRIPTION note A pandas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language how 3 methods are available here: how='simple' : Single process on a pandas DataFrame how='parallel' : Parallelised processes on a pandas DataFrame how='spark' : Distributed processes on a pyspark DataFrame TYPE: str, by default \"parallel\" DEFAULT: 'parallel' additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: 'discarded' extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] kwargs Additional parameters depending on the how argument. TYPE: Dict[str, Any] RETURNS DESCRIPTION Union[pd.DataFrame, ps.DataFrame] A DataFrame with one line per extraction Source code in edsnlp/processing/wrapper.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def pipe ( note : Union [ pd . DataFrame , ps . DataFrame ], nlp : Language , how : str = \"parallel\" , additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : ExtensionSchema = [], ** kwargs : Dict [ str , Any ], ) -> Union [ pd . DataFrame , ps . DataFrame ]: \"\"\" Function to apply a spaCy pipe to a pandas or pyspark DataFrame Parameters ---------- note : DataFrame A pandas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe how : str, by default \"parallel\" 3 methods are available here: - `how='simple'`: Single process on a pandas DataFrame - `how='parallel'`: Parallelised processes on a pandas DataFrame - `how='spark'`: Distributed processes on a pyspark DataFrame additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. kwargs : Dict[str, Any] Additional parameters depending on the `how` argument. Returns ------- Union[pd.DataFrame, ps.DataFrame] A DataFrame with one line per extraction \"\"\" if ( type ( note ) == ps . DataFrame ) and ( how != \"spark\" ): raise ValueError ( \"You are providing a pyspark DataFrame, please use `how='spark'`\" ) if how == \"simple\" : return simple_pipe ( note = note , nlp = nlp , additional_spans = additional_spans , extensions = extensions , ** kwargs , ) if how == \"parallel\" : return parallel_pipe ( note = note , nlp = nlp , additional_spans = additional_spans , extensions = extensions , ** kwargs , ) if how == \"spark\" : if type ( note ) == pd . DataFrame : raise ValueError ( \"\"\" You are providing a pandas DataFrame with `how='spark'`, which is incompatible. \"\"\" ) if extensions and type ( extensions ) != dict : raise ValueError ( \"\"\" When using Spark, you should provide extension names along with the extension type (as a dictionnary): `d[extension_name] = extension_type` \"\"\" # noqa W291 ) return spark_pipe ( note = note , nlp = nlp , additional_spans = additional_spans , extensions = extensions , ** kwargs , )","title":"pipe()"},{"location":"reference/utils/","text":"edsnlp.utils","title":"`edsnlp.utils`"},{"location":"reference/utils/#edsnlputils","text":"","title":"edsnlp.utils"},{"location":"reference/utils/deprecation/","text":"edsnlp.utils.deprecation deprecated_factory ( name , new_name = None , default_config = None , func = None ) Execute the Language.factory method on a modified factory function. The modification adds a deprecation warning. PARAMETER DESCRIPTION name The deprecated name for the pipeline TYPE: str new_name The new name for the pipeline, which should be used, by default None TYPE: Optional[str], optional DEFAULT: None default_config The configuration that should be passed to Language.factory, by default None TYPE: Optional[Dict[str, Any]], optional DEFAULT: None func The function to decorate, by default None TYPE: Optional[Callable], optional DEFAULT: None RETURNS DESCRIPTION Callable Source code in edsnlp/utils/deprecation.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def deprecated_factory ( name : str , new_name : Optional [ str ] = None , default_config : Optional [ Dict [ str , Any ]] = None , func : Optional [ Callable ] = None , ) -> Callable : \"\"\" Execute the Language.factory method on a modified factory function. The modification adds a deprecation warning. Parameters ---------- name : str The deprecated name for the pipeline new_name : Optional[str], optional The new name for the pipeline, which should be used, by default None default_config : Optional[Dict[str, Any]], optional The configuration that should be passed to Language.factory, by default None func : Optional[Callable], optional The function to decorate, by default None Returns ------- Callable \"\"\" if default_config is None : default_config = dict () wrapper = Language . factory ( name , default_config = default_config ) def wrap ( factory ): # Define decorator # We use micheles' decorator package to keep the same signature # See https://github.com/micheles/decorator/ @decorator def decorate ( f , * args , ** kwargs , ): deprecation ( name , new_name ) return f ( * args , ** kwargs , ) decorated = decorate ( factory ) wrapper ( decorated ) return factory if func is not None : return wrap ( func ) return wrap","title":"deprecation"},{"location":"reference/utils/deprecation/#edsnlputilsdeprecation","text":"","title":"edsnlp.utils.deprecation"},{"location":"reference/utils/deprecation/#edsnlp.utils.deprecation.deprecated_factory","text":"Execute the Language.factory method on a modified factory function. The modification adds a deprecation warning. PARAMETER DESCRIPTION name The deprecated name for the pipeline TYPE: str new_name The new name for the pipeline, which should be used, by default None TYPE: Optional[str], optional DEFAULT: None default_config The configuration that should be passed to Language.factory, by default None TYPE: Optional[Dict[str, Any]], optional DEFAULT: None func The function to decorate, by default None TYPE: Optional[Callable], optional DEFAULT: None RETURNS DESCRIPTION Callable Source code in edsnlp/utils/deprecation.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def deprecated_factory ( name : str , new_name : Optional [ str ] = None , default_config : Optional [ Dict [ str , Any ]] = None , func : Optional [ Callable ] = None , ) -> Callable : \"\"\" Execute the Language.factory method on a modified factory function. The modification adds a deprecation warning. Parameters ---------- name : str The deprecated name for the pipeline new_name : Optional[str], optional The new name for the pipeline, which should be used, by default None default_config : Optional[Dict[str, Any]], optional The configuration that should be passed to Language.factory, by default None func : Optional[Callable], optional The function to decorate, by default None Returns ------- Callable \"\"\" if default_config is None : default_config = dict () wrapper = Language . factory ( name , default_config = default_config ) def wrap ( factory ): # Define decorator # We use micheles' decorator package to keep the same signature # See https://github.com/micheles/decorator/ @decorator def decorate ( f , * args , ** kwargs , ): deprecation ( name , new_name ) return f ( * args , ** kwargs , ) decorated = decorate ( factory ) wrapper ( decorated ) return factory if func is not None : return wrap ( func ) return wrap","title":"deprecated_factory()"},{"location":"reference/utils/examples/","text":"edsnlp.utils.examples find_matches ( example ) Finds entities within the example. PARAMETER DESCRIPTION example Example to process. TYPE: str RETURNS DESCRIPTION List[re.Match] List of matches for entities. Source code in edsnlp/utils/examples.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def find_matches ( example : str ) -> List [ re . Match ]: \"\"\" Finds entities within the example. Parameters ---------- example : str Example to process. Returns ------- List[re.Match] List of matches for entities. \"\"\" return list ( entity_pattern . finditer ( example )) parse_match ( match ) Parse a regex match representing an entity. PARAMETER DESCRIPTION match Match for an entity. TYPE: re.Match RETURNS DESCRIPTION Match Usable representation for the entity match. Source code in edsnlp/utils/examples.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def parse_match ( match : re . Match ) -> Match : \"\"\" Parse a regex match representing an entity. Parameters ---------- match : re.Match Match for an entity. Returns ------- Match Usable representation for the entity match. \"\"\" lexical_variant = match . group () start_char = match . start () end_char = match . end () text = text_pattern . findall ( lexical_variant )[ 0 ] modifiers = modifiers_pattern . findall ( lexical_variant )[ 0 ] m = Match ( start_char = start_char , end_char = end_char , text = text , modifiers = modifiers ) return m parse_example ( example ) Parses an example : finds examples and removes the tags. PARAMETER DESCRIPTION example Example to process. TYPE: str RETURNS DESCRIPTION Tuple[str, List[Entity]] Cleaned text and extracted entities. Source code in edsnlp/utils/examples.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def parse_example ( example : str ) -> Tuple [ str , List [ Entity ]]: \"\"\" Parses an example : finds examples and removes the tags. Parameters ---------- example : str Example to process. Returns ------- Tuple[str, List[Entity]] Cleaned text and extracted entities. \"\"\" matches = [ parse_match ( match ) for match in find_matches ( example = example )] text = \"\" entities = [] cursor = 0 for match in matches : text += example [ cursor : match . start_char ] start_char = len ( text ) text += match . text end_char = len ( text ) modifiers = [ m . split ( \"=\" ) for m in match . modifiers . split ()] cursor = match . end_char entity = Entity ( start_char = start_char , end_char = end_char , modifiers = [ Modifier ( key = k , value = v ) for k , v in modifiers ], ) entities . append ( entity ) text += example [ cursor :] return text , entities","title":"examples"},{"location":"reference/utils/examples/#edsnlputilsexamples","text":"","title":"edsnlp.utils.examples"},{"location":"reference/utils/examples/#edsnlp.utils.examples.find_matches","text":"Finds entities within the example. PARAMETER DESCRIPTION example Example to process. TYPE: str RETURNS DESCRIPTION List[re.Match] List of matches for entities. Source code in edsnlp/utils/examples.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def find_matches ( example : str ) -> List [ re . Match ]: \"\"\" Finds entities within the example. Parameters ---------- example : str Example to process. Returns ------- List[re.Match] List of matches for entities. \"\"\" return list ( entity_pattern . finditer ( example ))","title":"find_matches()"},{"location":"reference/utils/examples/#edsnlp.utils.examples.parse_match","text":"Parse a regex match representing an entity. PARAMETER DESCRIPTION match Match for an entity. TYPE: re.Match RETURNS DESCRIPTION Match Usable representation for the entity match. Source code in edsnlp/utils/examples.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def parse_match ( match : re . Match ) -> Match : \"\"\" Parse a regex match representing an entity. Parameters ---------- match : re.Match Match for an entity. Returns ------- Match Usable representation for the entity match. \"\"\" lexical_variant = match . group () start_char = match . start () end_char = match . end () text = text_pattern . findall ( lexical_variant )[ 0 ] modifiers = modifiers_pattern . findall ( lexical_variant )[ 0 ] m = Match ( start_char = start_char , end_char = end_char , text = text , modifiers = modifiers ) return m","title":"parse_match()"},{"location":"reference/utils/examples/#edsnlp.utils.examples.parse_example","text":"Parses an example : finds examples and removes the tags. PARAMETER DESCRIPTION example Example to process. TYPE: str RETURNS DESCRIPTION Tuple[str, List[Entity]] Cleaned text and extracted entities. Source code in edsnlp/utils/examples.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def parse_example ( example : str ) -> Tuple [ str , List [ Entity ]]: \"\"\" Parses an example : finds examples and removes the tags. Parameters ---------- example : str Example to process. Returns ------- Tuple[str, List[Entity]] Cleaned text and extracted entities. \"\"\" matches = [ parse_match ( match ) for match in find_matches ( example = example )] text = \"\" entities = [] cursor = 0 for match in matches : text += example [ cursor : match . start_char ] start_char = len ( text ) text += match . text end_char = len ( text ) modifiers = [ m . split ( \"=\" ) for m in match . modifiers . split ()] cursor = match . end_char entity = Entity ( start_char = start_char , end_char = end_char , modifiers = [ Modifier ( key = k , value = v ) for k , v in modifiers ], ) entities . append ( entity ) text += example [ cursor :] return text , entities","title":"parse_example()"},{"location":"reference/utils/filter/","text":"edsnlp.utils.filter get_sort_key ( span ) Returns the sort key for filtering spans. PARAMETER DESCRIPTION span Span to sort. TYPE: Span RETURNS DESCRIPTION key Sort key. TYPE: Tuple(int, int) Source code in edsnlp/utils/filter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_sort_key ( span : Span ) -> Tuple [ int , int ]: \"\"\" Returns the sort key for filtering spans. Parameters ---------- span : Span Span to sort. Returns ------- key : Tuple(int, int) Sort key. \"\"\" return span . end - span . start , - span . start filter_spans ( spans , label_to_remove = None , return_discarded = False ) Re-definition of spacy's filtering function, that returns discarded spans as well as filtered ones. Can also accept a label_to_remove argument, useful for filtering out pseudo cues. If set, results can contain overlapping spans: only spans overlapping with excluded labels are removed. The main expected use case is for pseudo-cues. The spaCy documentation states : Filter a sequence of spans and remove duplicates or overlaps. Useful for creating named entities (where one token can only be part of one entity) or when merging spans with Retokenizer.merge . When spans overlap, the (first) longest span is preferred over shorter spans. Filtering out spans If the label_to_remove argument is supplied, it might be tempting to filter overlapping spans that are not part of a label to remove. The reason we keep all other possibly overlapping labels is that in qualifier pipelines, the same cue can precede and follow a marked entity. Hence we need to keep every example. PARAMETER DESCRIPTION spans Spans to filter. TYPE: List[Span] return_discarded Whether to return discarded spans. TYPE: bool DEFAULT: False label_to_remove Label to remove. If set, results can contain overlapping spans. TYPE: str, optional DEFAULT: None RETURNS DESCRIPTION results Filtered spans TYPE: List[Span] discarded Discarded spans TYPE: List[Span], optional Source code in edsnlp/utils/filter.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def filter_spans ( spans : Iterable [ Union [ \"Span\" , Tuple [ \"Span\" , Any ]]], label_to_remove : Optional [ str ] = None , return_discarded : bool = False , ) -> Union [ List [ \"Span\" ], Tuple [ List [ \"Span\" ], List [ \"Span\" ]]]: \"\"\" Re-definition of spacy's filtering function, that returns discarded spans as well as filtered ones. Can also accept a `label_to_remove` argument, useful for filtering out pseudo cues. If set, `results` can contain overlapping spans: only spans overlapping with excluded labels are removed. The main expected use case is for pseudo-cues. !!! note \"\" The **spaCy documentation states**: > Filter a sequence of spans and remove duplicates or overlaps. > Useful for creating named entities (where one token can only > be part of one entity) or when merging spans with > `Retokenizer.merge`. When spans overlap, the (first) > longest span is preferred over shorter spans. !!! danger \"Filtering out spans\" If the `label_to_remove` argument is supplied, it might be tempting to filter overlapping spans that are not part of a label to remove. The reason we keep all other possibly overlapping labels is that in qualifier pipelines, the same cue can precede **and** follow a marked entity. Hence we need to keep every example. Parameters ---------- spans : List[Span] Spans to filter. return_discarded : bool Whether to return discarded spans. label_to_remove : str, optional Label to remove. If set, results can contain overlapping spans. Returns ------- results : List[Span] Filtered spans discarded : List[Span], optional Discarded spans \"\"\" sorted_spans = sorted ( spans , key = get_sort_key , reverse = True ) result = [] discarded = [] seen_tokens = set () for span in sorted_spans : # Check for end - 1 here because boundaries are inclusive if span . start not in seen_tokens and span . end - 1 not in seen_tokens : if label_to_remove is None or span . label_ != label_to_remove : result . append ( span ) if label_to_remove is None or span . label_ == label_to_remove : seen_tokens . update ( range ( span . start , span . end )) elif label_to_remove is None or span . label_ != label_to_remove : discarded . append ( span ) result = sorted ( result , key = lambda span : span . start ) discarded = sorted ( discarded , key = lambda span : span . start ) if return_discarded : return result , discarded return result consume_spans ( spans , filter , second_chance = None ) Consume a list of span, according to a filter. Warning This method makes the hard hypothesis that: Spans are sorted. Spans are consumed in sequence and only once. The second item is problematic for the way we treat long entities, hence the second_chance parameter, which lets entities be seen more than once. PARAMETER DESCRIPTION spans List of spans to filter TYPE: List of spans filter Filtering function. Should return True when the item is to be included. TYPE: Callable second_chance Optional list of spans to include again (useful for long entities), by default None TYPE: List of spans, optional DEFAULT: None RETURNS DESCRIPTION matches List of spans consumed by the filter. TYPE: List of spans remainder List of remaining spans in the original spans parameter. TYPE: List of spans Source code in edsnlp/utils/filter.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def consume_spans ( spans : List [ Span ], filter : Callable , second_chance : Optional [ List [ Span ]] = None , ) -> Tuple [ List [ Span ], List [ Span ]]: \"\"\" Consume a list of span, according to a filter. !!! warning This method makes the hard hypothesis that: 1. Spans are sorted. 2. Spans are consumed in sequence and only once. The second item is problematic for the way we treat long entities, hence the `second_chance` parameter, which lets entities be seen more than once. Parameters ---------- spans : List of spans List of spans to filter filter : Callable Filtering function. Should return True when the item is to be included. second_chance : List of spans, optional Optional list of spans to include again (useful for long entities), by default None Returns ------- matches : List of spans List of spans consumed by the filter. remainder : List of spans List of remaining spans in the original `spans` parameter. \"\"\" if not second_chance : second_chance = [] else : second_chance = [ m for m in second_chance if filter ( m )] if not spans : return second_chance , [] for i , span in enumerate ( spans ): if not filter ( span ): break else : i += 1 matches = spans [: i ] remainder = spans [ i :] matches . extend ( second_chance ) return matches , remainder get_spans ( spans , label ) Extracts spans with a given label. Prefer using hash label for performance reasons. PARAMETER DESCRIPTION spans List of spans to filter. TYPE: List[Span] label Label to filter on. TYPE: Union[int, str] RETURNS DESCRIPTION List[Span] Filtered spans. Source code in edsnlp/utils/filter.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def get_spans ( spans : List [ Span ], label : Union [ int , str ]) -> List [ Span ]: \"\"\" Extracts spans with a given label. Prefer using hash label for performance reasons. Parameters ---------- spans : List[Span] List of spans to filter. label : Union[int, str] Label to filter on. Returns ------- List[Span] Filtered spans. \"\"\" if isinstance ( label , int ): return [ span for span in spans if span . label == label ] else : return [ span for span in spans if span . label_ == label ]","title":"filter"},{"location":"reference/utils/filter/#edsnlputilsfilter","text":"","title":"edsnlp.utils.filter"},{"location":"reference/utils/filter/#edsnlp.utils.filter.get_sort_key","text":"Returns the sort key for filtering spans. PARAMETER DESCRIPTION span Span to sort. TYPE: Span RETURNS DESCRIPTION key Sort key. TYPE: Tuple(int, int) Source code in edsnlp/utils/filter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_sort_key ( span : Span ) -> Tuple [ int , int ]: \"\"\" Returns the sort key for filtering spans. Parameters ---------- span : Span Span to sort. Returns ------- key : Tuple(int, int) Sort key. \"\"\" return span . end - span . start , - span . start","title":"get_sort_key()"},{"location":"reference/utils/filter/#edsnlp.utils.filter.filter_spans","text":"Re-definition of spacy's filtering function, that returns discarded spans as well as filtered ones. Can also accept a label_to_remove argument, useful for filtering out pseudo cues. If set, results can contain overlapping spans: only spans overlapping with excluded labels are removed. The main expected use case is for pseudo-cues. The spaCy documentation states : Filter a sequence of spans and remove duplicates or overlaps. Useful for creating named entities (where one token can only be part of one entity) or when merging spans with Retokenizer.merge . When spans overlap, the (first) longest span is preferred over shorter spans. Filtering out spans If the label_to_remove argument is supplied, it might be tempting to filter overlapping spans that are not part of a label to remove. The reason we keep all other possibly overlapping labels is that in qualifier pipelines, the same cue can precede and follow a marked entity. Hence we need to keep every example. PARAMETER DESCRIPTION spans Spans to filter. TYPE: List[Span] return_discarded Whether to return discarded spans. TYPE: bool DEFAULT: False label_to_remove Label to remove. If set, results can contain overlapping spans. TYPE: str, optional DEFAULT: None RETURNS DESCRIPTION results Filtered spans TYPE: List[Span] discarded Discarded spans TYPE: List[Span], optional Source code in edsnlp/utils/filter.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def filter_spans ( spans : Iterable [ Union [ \"Span\" , Tuple [ \"Span\" , Any ]]], label_to_remove : Optional [ str ] = None , return_discarded : bool = False , ) -> Union [ List [ \"Span\" ], Tuple [ List [ \"Span\" ], List [ \"Span\" ]]]: \"\"\" Re-definition of spacy's filtering function, that returns discarded spans as well as filtered ones. Can also accept a `label_to_remove` argument, useful for filtering out pseudo cues. If set, `results` can contain overlapping spans: only spans overlapping with excluded labels are removed. The main expected use case is for pseudo-cues. !!! note \"\" The **spaCy documentation states**: > Filter a sequence of spans and remove duplicates or overlaps. > Useful for creating named entities (where one token can only > be part of one entity) or when merging spans with > `Retokenizer.merge`. When spans overlap, the (first) > longest span is preferred over shorter spans. !!! danger \"Filtering out spans\" If the `label_to_remove` argument is supplied, it might be tempting to filter overlapping spans that are not part of a label to remove. The reason we keep all other possibly overlapping labels is that in qualifier pipelines, the same cue can precede **and** follow a marked entity. Hence we need to keep every example. Parameters ---------- spans : List[Span] Spans to filter. return_discarded : bool Whether to return discarded spans. label_to_remove : str, optional Label to remove. If set, results can contain overlapping spans. Returns ------- results : List[Span] Filtered spans discarded : List[Span], optional Discarded spans \"\"\" sorted_spans = sorted ( spans , key = get_sort_key , reverse = True ) result = [] discarded = [] seen_tokens = set () for span in sorted_spans : # Check for end - 1 here because boundaries are inclusive if span . start not in seen_tokens and span . end - 1 not in seen_tokens : if label_to_remove is None or span . label_ != label_to_remove : result . append ( span ) if label_to_remove is None or span . label_ == label_to_remove : seen_tokens . update ( range ( span . start , span . end )) elif label_to_remove is None or span . label_ != label_to_remove : discarded . append ( span ) result = sorted ( result , key = lambda span : span . start ) discarded = sorted ( discarded , key = lambda span : span . start ) if return_discarded : return result , discarded return result","title":"filter_spans()"},{"location":"reference/utils/filter/#edsnlp.utils.filter.consume_spans","text":"Consume a list of span, according to a filter. Warning This method makes the hard hypothesis that: Spans are sorted. Spans are consumed in sequence and only once. The second item is problematic for the way we treat long entities, hence the second_chance parameter, which lets entities be seen more than once. PARAMETER DESCRIPTION spans List of spans to filter TYPE: List of spans filter Filtering function. Should return True when the item is to be included. TYPE: Callable second_chance Optional list of spans to include again (useful for long entities), by default None TYPE: List of spans, optional DEFAULT: None RETURNS DESCRIPTION matches List of spans consumed by the filter. TYPE: List of spans remainder List of remaining spans in the original spans parameter. TYPE: List of spans Source code in edsnlp/utils/filter.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def consume_spans ( spans : List [ Span ], filter : Callable , second_chance : Optional [ List [ Span ]] = None , ) -> Tuple [ List [ Span ], List [ Span ]]: \"\"\" Consume a list of span, according to a filter. !!! warning This method makes the hard hypothesis that: 1. Spans are sorted. 2. Spans are consumed in sequence and only once. The second item is problematic for the way we treat long entities, hence the `second_chance` parameter, which lets entities be seen more than once. Parameters ---------- spans : List of spans List of spans to filter filter : Callable Filtering function. Should return True when the item is to be included. second_chance : List of spans, optional Optional list of spans to include again (useful for long entities), by default None Returns ------- matches : List of spans List of spans consumed by the filter. remainder : List of spans List of remaining spans in the original `spans` parameter. \"\"\" if not second_chance : second_chance = [] else : second_chance = [ m for m in second_chance if filter ( m )] if not spans : return second_chance , [] for i , span in enumerate ( spans ): if not filter ( span ): break else : i += 1 matches = spans [: i ] remainder = spans [ i :] matches . extend ( second_chance ) return matches , remainder","title":"consume_spans()"},{"location":"reference/utils/filter/#edsnlp.utils.filter.get_spans","text":"Extracts spans with a given label. Prefer using hash label for performance reasons. PARAMETER DESCRIPTION spans List of spans to filter. TYPE: List[Span] label Label to filter on. TYPE: Union[int, str] RETURNS DESCRIPTION List[Span] Filtered spans. Source code in edsnlp/utils/filter.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def get_spans ( spans : List [ Span ], label : Union [ int , str ]) -> List [ Span ]: \"\"\" Extracts spans with a given label. Prefer using hash label for performance reasons. Parameters ---------- spans : List[Span] List of spans to filter. label : Union[int, str] Label to filter on. Returns ------- List[Span] Filtered spans. \"\"\" if isinstance ( label , int ): return [ span for span in spans if span . label == label ] else : return [ span for span in spans if span . label_ == label ]","title":"get_spans()"},{"location":"reference/utils/inclusion/","text":"edsnlp.utils.inclusion check_inclusion ( span , start , end ) Checks whether the span overlaps the boundaries. PARAMETER DESCRIPTION span Span to check. TYPE: Span start Start of the boundary TYPE: int end End of the boundary TYPE: int RETURNS DESCRIPTION bool Whether the span overlaps the boundaries. Source code in edsnlp/utils/inclusion.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def check_inclusion ( span : Span , start : int , end : int ) -> bool : \"\"\" Checks whether the span overlaps the boundaries. Parameters ---------- span : Span Span to check. start : int Start of the boundary end : int End of the boundary Returns ------- bool Whether the span overlaps the boundaries. \"\"\" if span . start >= end or span . end <= start : return False return True","title":"inclusion"},{"location":"reference/utils/inclusion/#edsnlputilsinclusion","text":"","title":"edsnlp.utils.inclusion"},{"location":"reference/utils/inclusion/#edsnlp.utils.inclusion.check_inclusion","text":"Checks whether the span overlaps the boundaries. PARAMETER DESCRIPTION span Span to check. TYPE: Span start Start of the boundary TYPE: int end End of the boundary TYPE: int RETURNS DESCRIPTION bool Whether the span overlaps the boundaries. Source code in edsnlp/utils/inclusion.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def check_inclusion ( span : Span , start : int , end : int ) -> bool : \"\"\" Checks whether the span overlaps the boundaries. Parameters ---------- span : Span Span to check. start : int Start of the boundary end : int End of the boundary Returns ------- bool Whether the span overlaps the boundaries. \"\"\" if span . start >= end or span . end <= start : return False return True","title":"check_inclusion()"},{"location":"reference/utils/regex/","text":"edsnlp.utils.regex make_pattern ( patterns , with_breaks = False , name = None ) Create OR pattern from a list of patterns. PARAMETER DESCRIPTION patterns List of patterns to merge. TYPE: List[str] with_breaks Whether to add breaks ( \\b ) on each side, by default False TYPE: bool, optional DEFAULT: False name Name of the group, using regex ?P<> directive. TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION str Merged pattern. Source code in edsnlp/utils/regex.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def make_pattern ( patterns : List [ str ], with_breaks : bool = False , name : Optional [ str ] = None , ) -> str : r \"\"\" Create OR pattern from a list of patterns. Parameters ---------- patterns : List[str] List of patterns to merge. with_breaks : bool, optional Whether to add breaks (`\\b`) on each side, by default False name: str, optional Name of the group, using regex `?P<>` directive. Returns ------- str Merged pattern. \"\"\" if name : prefix = f \"(?P< { name } >\" else : prefix = \"(\" # Sorting by length might be more efficient patterns . sort ( key = len , reverse = True ) pattern = prefix + \"|\" . join ( patterns ) + \")\" if with_breaks : pattern = r \"\\b\" + pattern + r \"\\b\" return pattern","title":"regex"},{"location":"reference/utils/regex/#edsnlputilsregex","text":"","title":"edsnlp.utils.regex"},{"location":"reference/utils/regex/#edsnlp.utils.regex.make_pattern","text":"Create OR pattern from a list of patterns. PARAMETER DESCRIPTION patterns List of patterns to merge. TYPE: List[str] with_breaks Whether to add breaks ( \\b ) on each side, by default False TYPE: bool, optional DEFAULT: False name Name of the group, using regex ?P<> directive. TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION str Merged pattern. Source code in edsnlp/utils/regex.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def make_pattern ( patterns : List [ str ], with_breaks : bool = False , name : Optional [ str ] = None , ) -> str : r \"\"\" Create OR pattern from a list of patterns. Parameters ---------- patterns : List[str] List of patterns to merge. with_breaks : bool, optional Whether to add breaks (`\\b`) on each side, by default False name: str, optional Name of the group, using regex `?P<>` directive. Returns ------- str Merged pattern. \"\"\" if name : prefix = f \"(?P< { name } >\" else : prefix = \"(\" # Sorting by length might be more efficient patterns . sort ( key = len , reverse = True ) pattern = prefix + \"|\" . join ( patterns ) + \")\" if with_breaks : pattern = r \"\\b\" + pattern + r \"\\b\" return pattern","title":"make_pattern()"},{"location":"reference/utils/resources/","text":"edsnlp.utils.resources get_verbs ( verbs = None , check_contains = True ) Extract verbs from the resources, as a pandas dataframe. PARAMETER DESCRIPTION verbs List of verbs to keep. Returns all verbs by default. TYPE: List[str], optional DEFAULT: None check_contains Whether to check that no verb is missing if a list of verbs was provided. By default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION pd.DataFrame DataFrame containing conjugated verbs. Source code in edsnlp/utils/resources.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def get_verbs ( verbs : Optional [ List [ str ]] = None , check_contains : bool = True ) -> pd . DataFrame : \"\"\" Extract verbs from the resources, as a pandas dataframe. Parameters ---------- verbs : List[str], optional List of verbs to keep. Returns all verbs by default. check_contains : bool, optional Whether to check that no verb is missing if a list of verbs was provided. By default True Returns ------- pd.DataFrame DataFrame containing conjugated verbs. \"\"\" conjugated_verbs = pd . read_csv ( BASE_DIR / \"resources\" / \"verbs.csv\" ) if not verbs : return conjugated_verbs verbs = set ( verbs ) selected_verbs = conjugated_verbs [ conjugated_verbs . verb . isin ( verbs )] if check_contains : assert len ( verbs ) == selected_verbs . verb . nunique (), \"Some verbs are missing !\" return selected_verbs","title":"resources"},{"location":"reference/utils/resources/#edsnlputilsresources","text":"","title":"edsnlp.utils.resources"},{"location":"reference/utils/resources/#edsnlp.utils.resources.get_verbs","text":"Extract verbs from the resources, as a pandas dataframe. PARAMETER DESCRIPTION verbs List of verbs to keep. Returns all verbs by default. TYPE: List[str], optional DEFAULT: None check_contains Whether to check that no verb is missing if a list of verbs was provided. By default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION pd.DataFrame DataFrame containing conjugated verbs. Source code in edsnlp/utils/resources.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def get_verbs ( verbs : Optional [ List [ str ]] = None , check_contains : bool = True ) -> pd . DataFrame : \"\"\" Extract verbs from the resources, as a pandas dataframe. Parameters ---------- verbs : List[str], optional List of verbs to keep. Returns all verbs by default. check_contains : bool, optional Whether to check that no verb is missing if a list of verbs was provided. By default True Returns ------- pd.DataFrame DataFrame containing conjugated verbs. \"\"\" conjugated_verbs = pd . read_csv ( BASE_DIR / \"resources\" / \"verbs.csv\" ) if not verbs : return conjugated_verbs verbs = set ( verbs ) selected_verbs = conjugated_verbs [ conjugated_verbs . verb . isin ( verbs )] if check_contains : assert len ( verbs ) == selected_verbs . verb . nunique (), \"Some verbs are missing !\" return selected_verbs","title":"get_verbs()"},{"location":"tutorials/","text":"Tutorials We provide step-by-step guides to get you started. We cover the following use-cases: Matching a terminology : you're looking for a concept within a corpus of texts. Qualifying entities : you want to make sure that the concept you've extracted are not invalidated by linguistic modulation. Detecting dates , which could serve as the basis for an event ordering algorithm. Processing multiple texts : to improve the inference speed of your pipeline ! Detecting Hospitalisation Reason : you want to look spans that mention the reason of hospitalisation or tag entities as the reason. Detecting false endlines : classify each endline and add the attribute excluded to the these tokens. Rationale In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents. Now, consider the following example: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort. Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. To curb this issue, EDS-NLP proposes rule-based pipelines that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort. To sum up, a typical medical NLP project consists in: Editing a terminology \"Matching\" this terminology on a corpus, ie extract phrases that belong to that terminology \"Qualifying\" entities to avoid false positives Once the pipeline is ready, we need to deploy it efficiently.","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"We provide step-by-step guides to get you started. We cover the following use-cases: Matching a terminology : you're looking for a concept within a corpus of texts. Qualifying entities : you want to make sure that the concept you've extracted are not invalidated by linguistic modulation. Detecting dates , which could serve as the basis for an event ordering algorithm. Processing multiple texts : to improve the inference speed of your pipeline ! Detecting Hospitalisation Reason : you want to look spans that mention the reason of hospitalisation or tag entities as the reason. Detecting false endlines : classify each endline and add the attribute excluded to the these tokens.","title":"Tutorials"},{"location":"tutorials/#rationale","text":"In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents. Now, consider the following example: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort. Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. To curb this issue, EDS-NLP proposes rule-based pipelines that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort. To sum up, a typical medical NLP project consists in: Editing a terminology \"Matching\" this terminology on a corpus, ie extract phrases that belong to that terminology \"Qualifying\" entities to avoid false positives Once the pipeline is ready, we need to deploy it efficiently.","title":"Rationale"},{"location":"tutorials/detecting-dates/","text":"Detecting dates We now know how to match a terminology and qualify detected entities, which covers most use cases for a typical medical NLP project. In this tutorial, we'll see how to use EDS-NLP to detect and normalise date mentions using eds.dates . This can have many applications, for dating medical events in particular. The eds.consultation_dates component, for instance, combines the date detection capabilities with a few simple patterns to detect the date of the consultation, when mentioned in clinical reports. Dates in clinical notes Consider the following example: French English Le patient est admis le 21 janvier pour une douleur dans le cou. Il se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans. The patient is admitted on January 21st for a neck pain. He complains about chronique pain that started three years ago. Clinical notes contain many different types of dates. To name a few examples: Type Description Examples Absolute Explicit date 2022-03-03 Partial Date missing the day, month or year le 3 janvier/on January 3rd , en 2021/in 2021 Relative Relative dates hier/yesterday , le mois dernier/last month Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. Extracting dates The followings snippet adds the eds.date component to the pipeline: import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.dates\" ) # (1) text = ( \"Le patient est admis le 21 janvier pour une douleur dans le cou. \\n \" \"Il se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans.\" ) # Detecting dates becomes trivial doc = nlp ( text ) # Likewise, accessing detected dates is hassle-free dates = doc . spans [ \"dates\" ] # (2) The date detection component is declared with eds.dates Dates are saved in the doc . spans [ \"dates\" ] key After this, accessing dates and there normalisation becomes trivial: # \u2191 Omitted code above \u2191 dates # (1) # Out: [21 janvier, il y a trois ans] dates is a list of spaCy Span objects. We can review each date and get its normalisation: date.text date._.date 21 janvier ????-01-21 il y a trois ans TD-1095 What next? The eds.dates pipeline component's role is merely to detect and normalise dates. It is the user's responsibility to use this information in a downstream application. For instance, you could use this pipeline to date medical entities. Let's do that. A medical event tagger Our pipeline will detect entities and events separately, and we will post-process the output Doc object to determine whether a given entity can be linked to a date. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.dates\" ) config = dict ( regex = dict ( admission = [ \"admissions?\" , \"admise?\" , \"prise? en charge\" ]), attr = \"LOWER\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) text = ( \"Le patient est admis le 12 avril pour une douleur \" \"survenue il y a trois jours. \" \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re.\" ) doc = nlp ( text ) At this point, the document is ready to be post-processed: its ents and spans [ \"dates\" ] are populated: # \u2191 Omitted code above \u2191 docs . ents # Out: (admis, pris en charge) doc . spans [ \"dates\" ] # Out: [12 avril, il y a trois jours, l'ann\u00e9e derni\u00e8re] As a first heuristic, let's consider that an entity can be linked to a date if the two are in the same sentence. In the case where multiple dates are present, we'll select the closest one. utils.py from spacy.tokens import Span from typing import List , Optional def candidate_dates ( ent : Span ) -> List [ Span ]: \"\"\"Return every dates in the same sentence as the entity\"\"\" return [ date for date in ent . doc . spans [ \"dates\" ] if date . sent == ent . sent ] def get_event_date ( ent : Span ) -> Optional [ Span ]: \"\"\"Link an entity to the closest date in the sentence, if any\"\"\" dates = candidate_dates ( ent ) # (1) if not dates : return dates = sorted ( dates , key = lambda d : min ( abs ( d . start - ent . end ), abs ( ent . start - d . end )), ) return dates [ 0 ] # (2) Get all dates present in the same sentence. Sort the dates, and keep the first item. We can apply this simple function: import spacy from utils import get_event_date nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.dates\" ) config = dict ( regex = dict ( admission = [ \"admissions?\" , \"admise?\" , \"prise? en charge\" ]), attr = \"LOWER\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) text = ( \"Le patient est admis le 12 avril pour une douleur \" \"survenue il y a trois jours. \" \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re.\" ) doc = nlp ( text ) for ent in doc . ents : print ( ent , get_event_date ( ent )) Which will output: ent get_event_date(ent) get_event_date(ent)._.date admis 12 avril ????-04-12 pris en charge l'ann\u00e9e derni\u00e8re TD-365","title":"Detecting dates"},{"location":"tutorials/detecting-dates/#detecting-dates","text":"We now know how to match a terminology and qualify detected entities, which covers most use cases for a typical medical NLP project. In this tutorial, we'll see how to use EDS-NLP to detect and normalise date mentions using eds.dates . This can have many applications, for dating medical events in particular. The eds.consultation_dates component, for instance, combines the date detection capabilities with a few simple patterns to detect the date of the consultation, when mentioned in clinical reports.","title":"Detecting dates"},{"location":"tutorials/detecting-dates/#dates-in-clinical-notes","text":"Consider the following example: French English Le patient est admis le 21 janvier pour une douleur dans le cou. Il se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans. The patient is admitted on January 21st for a neck pain. He complains about chronique pain that started three years ago. Clinical notes contain many different types of dates. To name a few examples: Type Description Examples Absolute Explicit date 2022-03-03 Partial Date missing the day, month or year le 3 janvier/on January 3rd , en 2021/in 2021 Relative Relative dates hier/yesterday , le mois dernier/last month Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.","title":"Dates in clinical notes"},{"location":"tutorials/detecting-dates/#extracting-dates","text":"The followings snippet adds the eds.date component to the pipeline: import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.dates\" ) # (1) text = ( \"Le patient est admis le 21 janvier pour une douleur dans le cou. \\n \" \"Il se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans.\" ) # Detecting dates becomes trivial doc = nlp ( text ) # Likewise, accessing detected dates is hassle-free dates = doc . spans [ \"dates\" ] # (2) The date detection component is declared with eds.dates Dates are saved in the doc . spans [ \"dates\" ] key After this, accessing dates and there normalisation becomes trivial: # \u2191 Omitted code above \u2191 dates # (1) # Out: [21 janvier, il y a trois ans] dates is a list of spaCy Span objects. We can review each date and get its normalisation: date.text date._.date 21 janvier ????-01-21 il y a trois ans TD-1095","title":"Extracting dates"},{"location":"tutorials/detecting-dates/#what-next","text":"The eds.dates pipeline component's role is merely to detect and normalise dates. It is the user's responsibility to use this information in a downstream application. For instance, you could use this pipeline to date medical entities. Let's do that.","title":"What next?"},{"location":"tutorials/detecting-dates/#a-medical-event-tagger","text":"Our pipeline will detect entities and events separately, and we will post-process the output Doc object to determine whether a given entity can be linked to a date. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.dates\" ) config = dict ( regex = dict ( admission = [ \"admissions?\" , \"admise?\" , \"prise? en charge\" ]), attr = \"LOWER\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) text = ( \"Le patient est admis le 12 avril pour une douleur \" \"survenue il y a trois jours. \" \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re.\" ) doc = nlp ( text ) At this point, the document is ready to be post-processed: its ents and spans [ \"dates\" ] are populated: # \u2191 Omitted code above \u2191 docs . ents # Out: (admis, pris en charge) doc . spans [ \"dates\" ] # Out: [12 avril, il y a trois jours, l'ann\u00e9e derni\u00e8re] As a first heuristic, let's consider that an entity can be linked to a date if the two are in the same sentence. In the case where multiple dates are present, we'll select the closest one. utils.py from spacy.tokens import Span from typing import List , Optional def candidate_dates ( ent : Span ) -> List [ Span ]: \"\"\"Return every dates in the same sentence as the entity\"\"\" return [ date for date in ent . doc . spans [ \"dates\" ] if date . sent == ent . sent ] def get_event_date ( ent : Span ) -> Optional [ Span ]: \"\"\"Link an entity to the closest date in the sentence, if any\"\"\" dates = candidate_dates ( ent ) # (1) if not dates : return dates = sorted ( dates , key = lambda d : min ( abs ( d . start - ent . end ), abs ( ent . start - d . end )), ) return dates [ 0 ] # (2) Get all dates present in the same sentence. Sort the dates, and keep the first item. We can apply this simple function: import spacy from utils import get_event_date nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.dates\" ) config = dict ( regex = dict ( admission = [ \"admissions?\" , \"admise?\" , \"prise? en charge\" ]), attr = \"LOWER\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) text = ( \"Le patient est admis le 12 avril pour une douleur \" \"survenue il y a trois jours. \" \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re.\" ) doc = nlp ( text ) for ent in doc . ents : print ( ent , get_event_date ( ent )) Which will output: ent get_event_date(ent) get_event_date(ent)._.date admis 12 avril ????-04-12 pris en charge l'ann\u00e9e derni\u00e8re TD-365","title":"A medical event tagger"},{"location":"tutorials/endlines/","text":"Detecting end-of-lines A common problem in medical corpus is that the character \\n does not necessarily correspond to a real new line as in other domains. For example, it is common to find texts like: Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. Inserted new line characters This issue is especially impactful for clinical notes that have been extracted from PDF documents. In that case, the new line character could be deliberately inserted by the doctor, or more likely added to respect the layout during the edition of the PDF. The aim of this tutorial is to train a unsupervised model to detect this false endlines and to use it for inference. The implemented model is based on the work of Zweigenbaum et al 1 . Training the model Let's train the model using an example corpus of three documents: import spacy from edsnlp.pipelines.core.endlines import EndLinesModel nlp = spacy . blank ( \"fr\" ) text1 = \"\"\"Le patient est arriv\u00e9 hier soir. Il est accompagn\u00e9 par son fils ANTECEDENTS Il a fait une TS en 2010; Fumeur, il est arr\u00eat\u00e9 il a 5 mois Chirurgie de coeur en 2011 CONCLUSION Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. DIAGNOSTIC : Antecedents Familiaux: - 1. P\u00e8re avec diab\u00e8te \"\"\" text2 = \"\"\"J'aime le \\n fromage... \\n \"\"\" text3 = ( \"/n\" \"Intervention(s) - acte(s) r\u00e9alis\u00e9(s) :/n\" \"Parathyro\u00efdectomie \u00e9lective le [DATE]\" ) texts = [ text1 , text2 , text3 , ] corpus = nlp . pipe ( texts ) # Fit the model endlines = EndLinesModel ( nlp = nlp ) # (1) df = endlines . fit_and_predict ( corpus ) # (2) # Save model PATH = \"/path_to_model\" endlines . save ( PATH ) Initialize the EndLinesModel object and then fit (and predict) in the training corpus. The corpus should be an iterable of spacy documents. Use a trained model for inference import spacy nlp = spacy . blank ( \"fr\" ) PATH = \"/path_to_model\" nlp . add_pipe ( \"eds.endlines\" , config = dict ( model_path = PATH )) # (1) docs = list ( nlp . pipe ([ text1 , text2 , text3 ])) doc = docs [ 1 ] doc # Out: J'aime le # Out: fromage... doc . spans [ \"new_lines\" ][ 0 ] . label_ # (2) # Out: 'space' you should specify the path to the trained model here. All new lines are stored in the doc . spans [ \"new_lines\" ] key. Whether or not a specific new line character is detected as a false endline is stored in the label attribute. Declared extensions The eds.endlines pipeline declares one spaCy extensions , on both Span and Token objects. The end_line attribute is a boolean, set to True if the pipeline predicts that the new line is an end line character. Otherwise, it is set to False if the new line is classified as a space. The pipeline also sets the excluded custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation ) for more detail. Pierre Zweigenbaum, Cyril Grouin, and Thomas Lavergne. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (end-of-line classification with no supervision). In Actes de la conf\u00e9rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters) , 364\u2013371. Paris, France, 7 2016. AFCP - ATALA. URL: https://aclanthology.org/2016.jeptalnrecital-poster.7 . \u21a9","title":"Detecting end-of-lines"},{"location":"tutorials/endlines/#detecting-end-of-lines","text":"A common problem in medical corpus is that the character \\n does not necessarily correspond to a real new line as in other domains. For example, it is common to find texts like: Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. Inserted new line characters This issue is especially impactful for clinical notes that have been extracted from PDF documents. In that case, the new line character could be deliberately inserted by the doctor, or more likely added to respect the layout during the edition of the PDF. The aim of this tutorial is to train a unsupervised model to detect this false endlines and to use it for inference. The implemented model is based on the work of Zweigenbaum et al 1 .","title":"Detecting end-of-lines"},{"location":"tutorials/endlines/#training-the-model","text":"Let's train the model using an example corpus of three documents: import spacy from edsnlp.pipelines.core.endlines import EndLinesModel nlp = spacy . blank ( \"fr\" ) text1 = \"\"\"Le patient est arriv\u00e9 hier soir. Il est accompagn\u00e9 par son fils ANTECEDENTS Il a fait une TS en 2010; Fumeur, il est arr\u00eat\u00e9 il a 5 mois Chirurgie de coeur en 2011 CONCLUSION Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. DIAGNOSTIC : Antecedents Familiaux: - 1. P\u00e8re avec diab\u00e8te \"\"\" text2 = \"\"\"J'aime le \\n fromage... \\n \"\"\" text3 = ( \"/n\" \"Intervention(s) - acte(s) r\u00e9alis\u00e9(s) :/n\" \"Parathyro\u00efdectomie \u00e9lective le [DATE]\" ) texts = [ text1 , text2 , text3 , ] corpus = nlp . pipe ( texts ) # Fit the model endlines = EndLinesModel ( nlp = nlp ) # (1) df = endlines . fit_and_predict ( corpus ) # (2) # Save model PATH = \"/path_to_model\" endlines . save ( PATH ) Initialize the EndLinesModel object and then fit (and predict) in the training corpus. The corpus should be an iterable of spacy documents.","title":"Training the model"},{"location":"tutorials/endlines/#use-a-trained-model-for-inference","text":"import spacy nlp = spacy . blank ( \"fr\" ) PATH = \"/path_to_model\" nlp . add_pipe ( \"eds.endlines\" , config = dict ( model_path = PATH )) # (1) docs = list ( nlp . pipe ([ text1 , text2 , text3 ])) doc = docs [ 1 ] doc # Out: J'aime le # Out: fromage... doc . spans [ \"new_lines\" ][ 0 ] . label_ # (2) # Out: 'space' you should specify the path to the trained model here. All new lines are stored in the doc . spans [ \"new_lines\" ] key. Whether or not a specific new line character is detected as a false endline is stored in the label attribute.","title":"Use a trained model for inference"},{"location":"tutorials/endlines/#declared-extensions","text":"The eds.endlines pipeline declares one spaCy extensions , on both Span and Token objects. The end_line attribute is a boolean, set to True if the pipeline predicts that the new line is an end line character. Otherwise, it is set to False if the new line is classified as a space. The pipeline also sets the excluded custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation ) for more detail. Pierre Zweigenbaum, Cyril Grouin, and Thomas Lavergne. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (end-of-line classification with no supervision). In Actes de la conf\u00e9rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters) , 364\u2013371. Paris, France, 7 2016. AFCP - ATALA. URL: https://aclanthology.org/2016.jeptalnrecital-poster.7 . \u21a9","title":"Declared extensions"},{"location":"tutorials/matching-a-terminology/","text":"Matching a terminology Matching a terminology is perhaps the most basic application of a medical NLP pipeline. In this tutorial, we will cover : Matching a terminology using spaCy's matchers, as well as RegExps Matching on a specific attribute You should consider reading the matcher's specific documentation for a description. Comparison to spaCy's matcher spaCy's Matcher and PhraseMatcher use a very efficient algorithm that compare a hashed representation token by token. They are not components by themselves, but can underpin rule-based pipelines. EDS-NLP's RegexMatcher lets the user match entire expressions using regular expressions. To achieve this, the matcher has to get to the text representation, match on it, and get back to spaCy's abstraction. The EDSPhraseMatcher lets EDS-NLP reuse spaCy's efficient algorithm, while adding the ability to skip pollution tokens (see the normalisation documentation for detail) A simple use case : finding COVID19 Let's try to find mentions of COVID19 and references to patients within a clinical note. import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], respiratoire = [ \"asthmatique\" , \"respiratoire\" ], ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms )) doc = nlp ( text ) doc . ents # Out: (asthmatique) Let's unpack what happened: We defined a dictionary of terms to look for, in the form {'label': list of terms} . We declared a spaCy pipeline, and add the eds.matcher component. We applied the pipeline to the texts... ... and explored the extracted entities. This example showcases a limitation of our term dictionary : the phrases COVID19 and difficult\u00e9s respiratoires were not detected by the pipeline. To increase recall, we could just add every possible variation : terms = dict( - covid=[\"coronavirus\", \"covid19\"], + covid=[\"coronavirus\", \"covid19\", \"COVID19\"], - respiratoire=[\"asthmatique\", \"respiratoire\"], + respiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"], ) But what if we come across Coronavirus ? Surely we can do better! Matching on normalised text We can modify the matcher's configuration to match on other attributes instead of the verbatim input. You can refer to spaCy's list of available token attributes . Let's focus on two: The LOWER attribute, which lets you match on a lowercased version of the text. The NORM attribute, which adds some basic normalisation (eg \u0153 to oe ). EDS-NLP provides a eds.normalizer component that extends the level of cleaning on the NORM attribute. The LOWER attribute Matching on the lowercased version is extremely easy: import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], respiratoire = [ \"asthmatique\" , \"respiratoire\" , \"respiratoires\" ], ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , attr = \"LOWER\" , # (1) ), ) doc = nlp ( text ) doc . ents # Out: (COVID19, respiratoires, asthmatique) The matcher's attr parameter defines the attribute that the matcher will use. It is set to \"TEXT\" by default (ie verbatim text). This code is complete, and should run as is. Using the normalisation component EDS-NLP provides its own normalisation component, which modifies the NORM attribute in place. It handles: removal of accentuated characters; normalisation of quotes and apostrophes; lowercasing, which enabled by default in spaCy \u2013 EDS-NLP lets you disable it; removal of pollution. Pollution in clinical texts EDS-NLP is meant to be deployed on clinical reports extracted from hospitals information systems. As such, it is often riddled with extraction issues or administrative artifacts that \"pollute\" the report. As a core principle, EDS-NLP never modifies the input text , and nlp ( text ) . text == text is always true . However, we can tag some tokens as pollution elements, and avoid using them for matching the terminology. You can activate it like any other component. import spacy text = ( \"Motif de prise en charge : probable pneumopathie a ===== COVID19, \" # (1) \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" , \"pneumopathie \u00e0 covid19\" ], # (2) respiratoire = [ \"asthmatique\" , \"respiratoire\" , \"respiratoires\" ], ) nlp = spacy . blank ( \"fr\" ) # Add the normalisation component nlp . add_pipe ( \"eds.normalizer\" ) # (3) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , attr = \"NORM\" , # (4) ignore_excluded = True , # (5) ), ) doc = nlp ( text ) doc . ents # Out: (pneumopathie a ===== COVID19, respiratoires, asthmatique) We've modified the example to include a simple pollution. We've added pneumopathie \u00e0 covid19 to the list of synonyms detected by the pipeline. Note that in the synonym we provide, we kept the accentuated \u00e0 , whereas the example displays an unaccentuated a . The component can be configured. See the specific documentation for detail. The normalisation lives in the NORM attribute We can tell the matcher to ignore excluded tokens (tokens tagged as pollution by the normalisation component). This is not an obligation. Using the normalisation component, you can match on a normalised version of the text, as well as skip pollution tokens during the matching process . Using term matching with the normalisation If you use the term matcher with the normalisation, bear in mind that the examples go through the pipeline . That's how the matcher was able to recover pneumopathie a ===== COVID19 despite the fact that we used an accentuated \u00e0 in the terminology. The term matcher matches the input text to the provided terminology, using the selected attribute in both cases. The NORM attribute that corresponds to \u00e0 and a is the same: a . Preliminary conclusion We have matched all mentions! However, we had to spell out the singular and plural form of respiratoire ... And what if we wanted to detect covid 19 , or covid-19 ? Of course, we could write out every imaginable possibility, but this will quickly become tedious. Using regular expressions Let us redefine the pipeline once again, this time using regular expressions: import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , # (1) terms = terms , # (2) attr = \"LOWER\" , # (3) ), ) doc = nlp ( text ) doc . ents # Out: (COVID19, respiratoires, asthmatique) We can now match using regular expressions. We can mix and match patterns! Here we keep looking for patients using spaCy's term matching. RegExp matching is not limited to the verbatim text! You can choose to use one of spaCy's native attribute, ignore excluded tokens, etc. This code is complete, and should run as is. Using regular expressions can help define richer patterns using more compact queries. Visualising matched entities EDS-NLP is part of the spaCy ecosystem, which means we can benefit from spaCy helper functions. For instance, spaCy's visualiser displacy can let us visualise the matched entities: # \u2191 Omitted code above \u2191 colors = { \"covid\" : \"orange\" , \"respiratoire\" : \"steelblue\" , } options = { \"colors\" : colors , } displacy . render ( doc , style = \"ent\" , options = options ) If you run this within a notebook, you should get: Motif de prise en charge : probable pneumopathie a COVID19 covid , sans difficult\u00e9s respiratoires respiratoire Le p\u00e8re du patient est asthmatique respiratoire .","title":"Matching a terminology"},{"location":"tutorials/matching-a-terminology/#matching-a-terminology","text":"Matching a terminology is perhaps the most basic application of a medical NLP pipeline. In this tutorial, we will cover : Matching a terminology using spaCy's matchers, as well as RegExps Matching on a specific attribute You should consider reading the matcher's specific documentation for a description. Comparison to spaCy's matcher spaCy's Matcher and PhraseMatcher use a very efficient algorithm that compare a hashed representation token by token. They are not components by themselves, but can underpin rule-based pipelines. EDS-NLP's RegexMatcher lets the user match entire expressions using regular expressions. To achieve this, the matcher has to get to the text representation, match on it, and get back to spaCy's abstraction. The EDSPhraseMatcher lets EDS-NLP reuse spaCy's efficient algorithm, while adding the ability to skip pollution tokens (see the normalisation documentation for detail)","title":"Matching a terminology"},{"location":"tutorials/matching-a-terminology/#a-simple-use-case-finding-covid19","text":"Let's try to find mentions of COVID19 and references to patients within a clinical note. import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], respiratoire = [ \"asthmatique\" , \"respiratoire\" ], ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms )) doc = nlp ( text ) doc . ents # Out: (asthmatique) Let's unpack what happened: We defined a dictionary of terms to look for, in the form {'label': list of terms} . We declared a spaCy pipeline, and add the eds.matcher component. We applied the pipeline to the texts... ... and explored the extracted entities. This example showcases a limitation of our term dictionary : the phrases COVID19 and difficult\u00e9s respiratoires were not detected by the pipeline. To increase recall, we could just add every possible variation : terms = dict( - covid=[\"coronavirus\", \"covid19\"], + covid=[\"coronavirus\", \"covid19\", \"COVID19\"], - respiratoire=[\"asthmatique\", \"respiratoire\"], + respiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"], ) But what if we come across Coronavirus ? Surely we can do better!","title":"A simple use case : finding COVID19"},{"location":"tutorials/matching-a-terminology/#matching-on-normalised-text","text":"We can modify the matcher's configuration to match on other attributes instead of the verbatim input. You can refer to spaCy's list of available token attributes . Let's focus on two: The LOWER attribute, which lets you match on a lowercased version of the text. The NORM attribute, which adds some basic normalisation (eg \u0153 to oe ). EDS-NLP provides a eds.normalizer component that extends the level of cleaning on the NORM attribute.","title":"Matching on normalised text"},{"location":"tutorials/matching-a-terminology/#the-lower-attribute","text":"Matching on the lowercased version is extremely easy: import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], respiratoire = [ \"asthmatique\" , \"respiratoire\" , \"respiratoires\" ], ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , attr = \"LOWER\" , # (1) ), ) doc = nlp ( text ) doc . ents # Out: (COVID19, respiratoires, asthmatique) The matcher's attr parameter defines the attribute that the matcher will use. It is set to \"TEXT\" by default (ie verbatim text). This code is complete, and should run as is.","title":"The LOWER attribute"},{"location":"tutorials/matching-a-terminology/#using-the-normalisation-component","text":"EDS-NLP provides its own normalisation component, which modifies the NORM attribute in place. It handles: removal of accentuated characters; normalisation of quotes and apostrophes; lowercasing, which enabled by default in spaCy \u2013 EDS-NLP lets you disable it; removal of pollution. Pollution in clinical texts EDS-NLP is meant to be deployed on clinical reports extracted from hospitals information systems. As such, it is often riddled with extraction issues or administrative artifacts that \"pollute\" the report. As a core principle, EDS-NLP never modifies the input text , and nlp ( text ) . text == text is always true . However, we can tag some tokens as pollution elements, and avoid using them for matching the terminology. You can activate it like any other component. import spacy text = ( \"Motif de prise en charge : probable pneumopathie a ===== COVID19, \" # (1) \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" , \"pneumopathie \u00e0 covid19\" ], # (2) respiratoire = [ \"asthmatique\" , \"respiratoire\" , \"respiratoires\" ], ) nlp = spacy . blank ( \"fr\" ) # Add the normalisation component nlp . add_pipe ( \"eds.normalizer\" ) # (3) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , attr = \"NORM\" , # (4) ignore_excluded = True , # (5) ), ) doc = nlp ( text ) doc . ents # Out: (pneumopathie a ===== COVID19, respiratoires, asthmatique) We've modified the example to include a simple pollution. We've added pneumopathie \u00e0 covid19 to the list of synonyms detected by the pipeline. Note that in the synonym we provide, we kept the accentuated \u00e0 , whereas the example displays an unaccentuated a . The component can be configured. See the specific documentation for detail. The normalisation lives in the NORM attribute We can tell the matcher to ignore excluded tokens (tokens tagged as pollution by the normalisation component). This is not an obligation. Using the normalisation component, you can match on a normalised version of the text, as well as skip pollution tokens during the matching process . Using term matching with the normalisation If you use the term matcher with the normalisation, bear in mind that the examples go through the pipeline . That's how the matcher was able to recover pneumopathie a ===== COVID19 despite the fact that we used an accentuated \u00e0 in the terminology. The term matcher matches the input text to the provided terminology, using the selected attribute in both cases. The NORM attribute that corresponds to \u00e0 and a is the same: a .","title":"Using the normalisation component"},{"location":"tutorials/matching-a-terminology/#preliminary-conclusion","text":"We have matched all mentions! However, we had to spell out the singular and plural form of respiratoire ... And what if we wanted to detect covid 19 , or covid-19 ? Of course, we could write out every imaginable possibility, but this will quickly become tedious.","title":"Preliminary conclusion"},{"location":"tutorials/matching-a-terminology/#using-regular-expressions","text":"Let us redefine the pipeline once again, this time using regular expressions: import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , # (1) terms = terms , # (2) attr = \"LOWER\" , # (3) ), ) doc = nlp ( text ) doc . ents # Out: (COVID19, respiratoires, asthmatique) We can now match using regular expressions. We can mix and match patterns! Here we keep looking for patients using spaCy's term matching. RegExp matching is not limited to the verbatim text! You can choose to use one of spaCy's native attribute, ignore excluded tokens, etc. This code is complete, and should run as is. Using regular expressions can help define richer patterns using more compact queries.","title":"Using regular expressions"},{"location":"tutorials/matching-a-terminology/#visualising-matched-entities","text":"EDS-NLP is part of the spaCy ecosystem, which means we can benefit from spaCy helper functions. For instance, spaCy's visualiser displacy can let us visualise the matched entities: # \u2191 Omitted code above \u2191 colors = { \"covid\" : \"orange\" , \"respiratoire\" : \"steelblue\" , } options = { \"colors\" : colors , } displacy . render ( doc , style = \"ent\" , options = options ) If you run this within a notebook, you should get: Motif de prise en charge : probable pneumopathie a COVID19 covid , sans difficult\u00e9s respiratoires respiratoire Le p\u00e8re du patient est asthmatique respiratoire .","title":"Visualising matched entities"},{"location":"tutorials/multiple-texts/","text":"Processing multiple texts In the previous tutorials, we've seen how to apply a spaCy NLP pipeline to a single text. Once the pipeline is tested and ready to be applied on an entire corpus, we'll want to deploy it efficiently. In this tutorial, we'll cover a few best practices and some caveats to avoid. Then, we'll explore methods that EDS-NLP provides to use a spaCy pipeline directly on a pandas or Spark DataFrame. These can drastically increase throughput (up to 20x speed increase on our 64-core machines). Consider this simple pipeline: Pipeline definition: pipeline.py import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) config = dict ( terms = dict ( patient = [ \"patient\" , \"malade\" ]), attr = \"NORM\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) # Add qualifiers nlp . add_pipe ( \"eds.negation\" ) nlp . add_pipe ( \"eds.hypothesis\" ) nlp . add_pipe ( \"eds.family\" ) # Add date detection nlp . add_pipe ( \"eds.dates\" ) Let's deploy it on a large number of documents. What about a for loop? Suppose we have a corpus of text: text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) corpus = [ text ] * 10000 # (1) This is admittedly ugly. But you get the idea, we have a corpus of 10 000 documents we want to process... You could just apply the pipeline document by document. A naive approach # \u2191 Omitted code above \u2191 docs = [ nlp ( text ) for text in corpus ] It turns out spaCy has a powerful parallelisation engine for an efficient processing of multiple texts. So the first step for writing more efficient spaCy code is to use nlp.pipe when processing multiple texts: - docs = [nlp(text) for text in corpus] + docs = list(nlp.pipe(corpus)) The nlp.pipe method takes an iterable as input, and outputs a generator of Doc object. Under the hood, texts are processed in batches, which is often much more efficient. Batch processing and EDS-NLP For now, EDS-NLP does not natively parallelise its components, so the gain from using nlp.pipe will not be that significant. Nevertheless, it's good practice to avoid using for loops when possible. Moreover, you will benefit from the batched tokenisation step. The way EDS-NLP is used may depend on how many documents you are working with. Once working with tens of thousands of them, parallelising the processing can be really efficient (up to 20x faster), but will require a (tiny) bit more work. Here are shown 4 ways to analyse texts depending on your needs A wrapper is available to simply switch between those use cases. Processing a pandas DataFrame Processing text within a pandas DataFrame is a very common use case. In many applications, you'll select a corpus of documents over a distributed cluster, load it in memory and process all texts. The OMOP CDM In every tutorial that mentions distributing EDS-NLP over a corpus of documents, we will expect the data to be organised using a flavour of the OMOP Common Data Model . For instance, we expect the input table to provide at least two columns, note_id and note_text . To make sure we can follow along, we propose three recipes for getting the DataFrame: using a dummy dataset like before, loading a CSV or by loading a Spark DataFrame into memory. Dummy example Loading data from a CSV Loading data from a Spark DataFrame import pandas as pd text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) corpus = [ text ] * 1000 data = pd . DataFrame ( dict ( note_text = corpus )) data [ \"note_id\" ] = range ( len ( data )) import pandas as pd data = pd . read_csv ( \"note.csv\" ) from pyspark.sql.session import SparkSession spark = SparkSession . builder . getOrCreate () df = spark . sql ( \"SELECT * FROM note\" ) df = df . select ( \"note_id\" , \"note_text\" ) data = df . limit ( 1000 ) . toPandas () # (1) We limit the size of the DataFrame to make sure we do not overwhelm our machine. We'll see in what follows how we can efficiently deploy our pipeline on the data object. \"By hand\" We can deploy the pipeline using nlp.pipe directly, but we'll need some work to format the results in a usable way. Let's see how this might go, before using EDS-NLP's helper function to avoid the boilerplate code. processing.py from spacy.tokens import Doc from typing import Any , Dict , List def get_entities ( doc : Doc ) -> List [ Dict [ str , Any ]]: \"\"\"Return a list of dict representation for the entities\"\"\" entities = [] for ent in doc . ents : d = dict ( start = ent . start_char , end = ent . end_char , label = ent . label_ , lexical_variant = ent . text , negation = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , key = \"ents\" , ) entities . append ( d ) for date in doc . spans . get ( \"dates\" , []): d = dict ( start = date . start_char , end = date . end_char , label = date . _ . date , lexical_variant = date . text , key = \"dates\" , ) entities . append ( d ) return entities # \u2191 Omitted code above \u2191 from processing import get_entities import pandas as pd data [ \"doc\" ] = list ( nlp . pipe ( data . note_text )) # (1) data [ \"entities\" ] = data . doc . apply ( get_entities ) # (2) # \"Explode\" the dataframe data = data [[ \"note_id\" , \"entities\" ]] . explode ( \"entities\" ) data = data . dropna () data = data . reset_index ( drop = True ) data = data [[ \"note_id\" ]] . join ( pd . json_normalize ( data . entities )) We use spaCy's efficient nlp.pipe method This part is far from optimal, since it uses apply... But the computationally heavy part is in the previous line, since get_entities merely reads pre-computed values from the document. The result on the first note: note_id start end label lexical_variant negation hypothesis family key 0 0 7 patient Patient 0 0 0 ents 0 114 121 patient patient 0 0 1 ents 0 17 34 2021-09-25 25 septembre 2021 nan nan nan dates Using EDS-NLP's helper functions Let's see how we can efficiently deploy our pipeline using EDS-NLP's utility methods. They share the same arguments: Argument Description Default note A DataFrame, with two required columns, note_id and note_id Required nlp The pipeline object Required additional_spans Keys in doc.spans to include besides doc.ents [] extensions Custom extensions to use [] Depending on your pipeline, you may want to extract other extensions. To do so, simply provide those extension names (without the leading underscore) to the extensions argument. Single process EDS-NLP provides a single_pipe helper function that avoids the hassle we just went through in the previous section. Using it is trivial: # \u2191 Omitted code above \u2191 from edsnlp.processing import single_pipe note_nlp = single_pipe ( data , nlp , additional_spans = [ \"dates\" ], extensions = [ \"parsed_date\" ], ) In just two Python statements, we get the exact same result as before! Multiple processes Depending on the size of your corpus, and if you have CPU cores to spare, you may want to distribute the computation. Again, EDS-NLP makes it extremely easy for you, through the parallel_pipe helper: # \u2191 Omitted code above \u2191 from edsnlp.processing import parallel_pipe note_nlp = parallel_pipe ( data , nlp , additional_spans = [ \"dates\" ], extensions = [ \"parsed_date\" ], n_jobs =- 2 , # (1) ) The n_jobs parameter controls the number of workers that you deploy in parallel. Negative inputs means \"all cores minus abs ( n_jobs ) \" Using a large number of workers and memory use In spaCy, even a rule-based pipeline is a memory intensive object. Be wary of using too many workers, lest you get a memory error. Depending on your machine, you should get a significant speed boost (we got 20x acceleration on a shared cluster using 62 cores). Deploying EDS-NLP on Spark Should you need to deploy spaCy on larger-than-memory Spark DataFrames, EDS-NLP has you covered. Suppose you have a Spark DataFrame: Using a dummy example Loading a pre-existing table from pyspark.sql.session import SparkSession from pyspark.sql import types as T spark = SparkSession . builder . getOrCreate () schema = T . StructType ( [ T . StructField ( \"note_id\" , T . IntegerType ()), T . StructField ( \"note_text\" , T . StringType ()), ] ) text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) data = [( i , text ) for i in range ( 1000 )] df = spark . createDataFrame ( data = data , schema = schema ) from pyspark.sql.session import SparkSession spark = SparkSession . builder . getOrCreate () df = spark . sql ( \"SELECT * FROM note\" ) df = df . select ( \"note_id\" , \"note_text\" ) Declaring types There is a minor twist, though: Spark needs to know in advance the type of each extension you want to save. Thus, if you need additional extensions to be saved, you'll have to provide a dictionary to the extensions argument instead of a list of strings. This dictionary will have the name of the extension as keys and its PySpark type as value. Accepted types are the ones present in pyspark.sql.types . EDS-NLP provides a helper function, pyspark_type_finder , is available to get the correct type for most Python objects. You just need to provide an example of the type you wish to collect: dt_type = pyspark_type_finder ( datetime . datetime ( 2020 , 1 , 1 )) Be careful when providing the example Do not blindly provide the first entity matched by your pipeline : it might be ill-suited. For instance, the Span._.date makes sense for a date span, but will be None if you use an entity... Deploying the pipeline Once again, using the helper is trivial: # \u2191 Omitted code above \u2191 from edsnlp.processing import spark_pipe note_nlp = spark_pipe ( df , # (1) nlp , additional_spans = [ \"dates\" ], extensions = { \"parsed_date\" : dt_type }, ) # Check that the pipeline was correctly distributed: note_nlp . show ( 5 ) We called the Spark DataFrame df in the earlier example. Using Spark, you can deploy EDS-NLP pipelines on tens of millions of documents with ease! One function to rule them all EDS-NLP provides a wrapper to simplify deployment even further: # \u2191 Omitted code above \u2191 from edsnlp.processing import pipe ### Small pandas DataFrame note_nlp = pipe ( note = df . limit ( 1000 ) . toPandas (), nlp = nlp , how = \"simple\" , additional_spans = [ \"dates\" ], extensions = [ \"parsed_date\" ], ) ### Larger pandas DataFrame note_nlp = pipe ( note = df . limit ( 10000 ) . toPandas (), nlp = nlp , how = \"parallel\" , additional_spans = [ \"dates\" ], extensions = [ \"parsed_date\" ], ) ### Huge Spark DataFrame note_nlp = pipe ( note = df , nlp = nlp , how = \"spark\" , additional_spans = [ \"dates\" ], extensions = { \"parsed_date\" : dt_type }, )","title":"Processing multiple texts"},{"location":"tutorials/multiple-texts/#processing-multiple-texts","text":"In the previous tutorials, we've seen how to apply a spaCy NLP pipeline to a single text. Once the pipeline is tested and ready to be applied on an entire corpus, we'll want to deploy it efficiently. In this tutorial, we'll cover a few best practices and some caveats to avoid. Then, we'll explore methods that EDS-NLP provides to use a spaCy pipeline directly on a pandas or Spark DataFrame. These can drastically increase throughput (up to 20x speed increase on our 64-core machines). Consider this simple pipeline: Pipeline definition: pipeline.py import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) config = dict ( terms = dict ( patient = [ \"patient\" , \"malade\" ]), attr = \"NORM\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) # Add qualifiers nlp . add_pipe ( \"eds.negation\" ) nlp . add_pipe ( \"eds.hypothesis\" ) nlp . add_pipe ( \"eds.family\" ) # Add date detection nlp . add_pipe ( \"eds.dates\" ) Let's deploy it on a large number of documents.","title":"Processing multiple texts"},{"location":"tutorials/multiple-texts/#what-about-a-for-loop","text":"Suppose we have a corpus of text: text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) corpus = [ text ] * 10000 # (1) This is admittedly ugly. But you get the idea, we have a corpus of 10 000 documents we want to process... You could just apply the pipeline document by document. A naive approach # \u2191 Omitted code above \u2191 docs = [ nlp ( text ) for text in corpus ] It turns out spaCy has a powerful parallelisation engine for an efficient processing of multiple texts. So the first step for writing more efficient spaCy code is to use nlp.pipe when processing multiple texts: - docs = [nlp(text) for text in corpus] + docs = list(nlp.pipe(corpus)) The nlp.pipe method takes an iterable as input, and outputs a generator of Doc object. Under the hood, texts are processed in batches, which is often much more efficient. Batch processing and EDS-NLP For now, EDS-NLP does not natively parallelise its components, so the gain from using nlp.pipe will not be that significant. Nevertheless, it's good practice to avoid using for loops when possible. Moreover, you will benefit from the batched tokenisation step. The way EDS-NLP is used may depend on how many documents you are working with. Once working with tens of thousands of them, parallelising the processing can be really efficient (up to 20x faster), but will require a (tiny) bit more work. Here are shown 4 ways to analyse texts depending on your needs A wrapper is available to simply switch between those use cases.","title":"What about a for loop?"},{"location":"tutorials/multiple-texts/#processing-a-pandas-dataframe","text":"Processing text within a pandas DataFrame is a very common use case. In many applications, you'll select a corpus of documents over a distributed cluster, load it in memory and process all texts. The OMOP CDM In every tutorial that mentions distributing EDS-NLP over a corpus of documents, we will expect the data to be organised using a flavour of the OMOP Common Data Model . For instance, we expect the input table to provide at least two columns, note_id and note_text . To make sure we can follow along, we propose three recipes for getting the DataFrame: using a dummy dataset like before, loading a CSV or by loading a Spark DataFrame into memory. Dummy example Loading data from a CSV Loading data from a Spark DataFrame import pandas as pd text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) corpus = [ text ] * 1000 data = pd . DataFrame ( dict ( note_text = corpus )) data [ \"note_id\" ] = range ( len ( data )) import pandas as pd data = pd . read_csv ( \"note.csv\" ) from pyspark.sql.session import SparkSession spark = SparkSession . builder . getOrCreate () df = spark . sql ( \"SELECT * FROM note\" ) df = df . select ( \"note_id\" , \"note_text\" ) data = df . limit ( 1000 ) . toPandas () # (1) We limit the size of the DataFrame to make sure we do not overwhelm our machine. We'll see in what follows how we can efficiently deploy our pipeline on the data object.","title":"Processing a pandas DataFrame"},{"location":"tutorials/multiple-texts/#by-hand","text":"We can deploy the pipeline using nlp.pipe directly, but we'll need some work to format the results in a usable way. Let's see how this might go, before using EDS-NLP's helper function to avoid the boilerplate code. processing.py from spacy.tokens import Doc from typing import Any , Dict , List def get_entities ( doc : Doc ) -> List [ Dict [ str , Any ]]: \"\"\"Return a list of dict representation for the entities\"\"\" entities = [] for ent in doc . ents : d = dict ( start = ent . start_char , end = ent . end_char , label = ent . label_ , lexical_variant = ent . text , negation = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , key = \"ents\" , ) entities . append ( d ) for date in doc . spans . get ( \"dates\" , []): d = dict ( start = date . start_char , end = date . end_char , label = date . _ . date , lexical_variant = date . text , key = \"dates\" , ) entities . append ( d ) return entities # \u2191 Omitted code above \u2191 from processing import get_entities import pandas as pd data [ \"doc\" ] = list ( nlp . pipe ( data . note_text )) # (1) data [ \"entities\" ] = data . doc . apply ( get_entities ) # (2) # \"Explode\" the dataframe data = data [[ \"note_id\" , \"entities\" ]] . explode ( \"entities\" ) data = data . dropna () data = data . reset_index ( drop = True ) data = data [[ \"note_id\" ]] . join ( pd . json_normalize ( data . entities )) We use spaCy's efficient nlp.pipe method This part is far from optimal, since it uses apply... But the computationally heavy part is in the previous line, since get_entities merely reads pre-computed values from the document. The result on the first note: note_id start end label lexical_variant negation hypothesis family key 0 0 7 patient Patient 0 0 0 ents 0 114 121 patient patient 0 0 1 ents 0 17 34 2021-09-25 25 septembre 2021 nan nan nan dates","title":"\"By hand\""},{"location":"tutorials/multiple-texts/#using-eds-nlps-helper-functions","text":"Let's see how we can efficiently deploy our pipeline using EDS-NLP's utility methods. They share the same arguments: Argument Description Default note A DataFrame, with two required columns, note_id and note_id Required nlp The pipeline object Required additional_spans Keys in doc.spans to include besides doc.ents [] extensions Custom extensions to use [] Depending on your pipeline, you may want to extract other extensions. To do so, simply provide those extension names (without the leading underscore) to the extensions argument.","title":"Using EDS-NLP's helper functions"},{"location":"tutorials/multiple-texts/#single-process","text":"EDS-NLP provides a single_pipe helper function that avoids the hassle we just went through in the previous section. Using it is trivial: # \u2191 Omitted code above \u2191 from edsnlp.processing import single_pipe note_nlp = single_pipe ( data , nlp , additional_spans = [ \"dates\" ], extensions = [ \"parsed_date\" ], ) In just two Python statements, we get the exact same result as before!","title":"Single process"},{"location":"tutorials/multiple-texts/#multiple-processes","text":"Depending on the size of your corpus, and if you have CPU cores to spare, you may want to distribute the computation. Again, EDS-NLP makes it extremely easy for you, through the parallel_pipe helper: # \u2191 Omitted code above \u2191 from edsnlp.processing import parallel_pipe note_nlp = parallel_pipe ( data , nlp , additional_spans = [ \"dates\" ], extensions = [ \"parsed_date\" ], n_jobs =- 2 , # (1) ) The n_jobs parameter controls the number of workers that you deploy in parallel. Negative inputs means \"all cores minus abs ( n_jobs ) \" Using a large number of workers and memory use In spaCy, even a rule-based pipeline is a memory intensive object. Be wary of using too many workers, lest you get a memory error. Depending on your machine, you should get a significant speed boost (we got 20x acceleration on a shared cluster using 62 cores).","title":"Multiple processes"},{"location":"tutorials/multiple-texts/#deploying-eds-nlp-on-spark","text":"Should you need to deploy spaCy on larger-than-memory Spark DataFrames, EDS-NLP has you covered. Suppose you have a Spark DataFrame: Using a dummy example Loading a pre-existing table from pyspark.sql.session import SparkSession from pyspark.sql import types as T spark = SparkSession . builder . getOrCreate () schema = T . StructType ( [ T . StructField ( \"note_id\" , T . IntegerType ()), T . StructField ( \"note_text\" , T . StringType ()), ] ) text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) data = [( i , text ) for i in range ( 1000 )] df = spark . createDataFrame ( data = data , schema = schema ) from pyspark.sql.session import SparkSession spark = SparkSession . builder . getOrCreate () df = spark . sql ( \"SELECT * FROM note\" ) df = df . select ( \"note_id\" , \"note_text\" )","title":"Deploying EDS-NLP on Spark"},{"location":"tutorials/multiple-texts/#declaring-types","text":"There is a minor twist, though: Spark needs to know in advance the type of each extension you want to save. Thus, if you need additional extensions to be saved, you'll have to provide a dictionary to the extensions argument instead of a list of strings. This dictionary will have the name of the extension as keys and its PySpark type as value. Accepted types are the ones present in pyspark.sql.types . EDS-NLP provides a helper function, pyspark_type_finder , is available to get the correct type for most Python objects. You just need to provide an example of the type you wish to collect: dt_type = pyspark_type_finder ( datetime . datetime ( 2020 , 1 , 1 )) Be careful when providing the example Do not blindly provide the first entity matched by your pipeline : it might be ill-suited. For instance, the Span._.date makes sense for a date span, but will be None if you use an entity...","title":"Declaring types"},{"location":"tutorials/multiple-texts/#deploying-the-pipeline","text":"Once again, using the helper is trivial: # \u2191 Omitted code above \u2191 from edsnlp.processing import spark_pipe note_nlp = spark_pipe ( df , # (1) nlp , additional_spans = [ \"dates\" ], extensions = { \"parsed_date\" : dt_type }, ) # Check that the pipeline was correctly distributed: note_nlp . show ( 5 ) We called the Spark DataFrame df in the earlier example. Using Spark, you can deploy EDS-NLP pipelines on tens of millions of documents with ease!","title":"Deploying the pipeline"},{"location":"tutorials/multiple-texts/#one-function-to-rule-them-all","text":"EDS-NLP provides a wrapper to simplify deployment even further: # \u2191 Omitted code above \u2191 from edsnlp.processing import pipe ### Small pandas DataFrame note_nlp = pipe ( note = df . limit ( 1000 ) . toPandas (), nlp = nlp , how = \"simple\" , additional_spans = [ \"dates\" ], extensions = [ \"parsed_date\" ], ) ### Larger pandas DataFrame note_nlp = pipe ( note = df . limit ( 10000 ) . toPandas (), nlp = nlp , how = \"parallel\" , additional_spans = [ \"dates\" ], extensions = [ \"parsed_date\" ], ) ### Huge Spark DataFrame note_nlp = pipe ( note = df , nlp = nlp , how = \"spark\" , additional_spans = [ \"dates\" ], extensions = { \"parsed_date\" : dt_type }, )","title":"One function to rule them all"},{"location":"tutorials/qualifying-entities/","text":"Qualifying entities In the previous tutorial, we saw how to match a terminology on a text. Using the doc . ents attribute, we can check whether a document mentions a concept of interest to build a cohort or describe patients. The issue However, consider the classical example where we look for the diabetes concept: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. None of these expressions should be used to build a cohort: the detected entity is either negated, speculative, or does not concern the patient themself. That's why we need to qualify the matched entities . Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. The solution We can use EDS-NLP's qualifier pipelines to achieve that. Let's add specific components to our pipeline to detect these three modalities. Adding qualifiers Adding qualifier pipelines is straightforward: import spacy text = ( \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , terms = terms , attr = \"LOWER\" , ), ) nlp . add_pipe ( \"eds.sentences\" ) # (1) nlp . add_pipe ( \"eds.negation\" ) # Negation component nlp . add_pipe ( \"eds.hypothesis\" ) # Speculation pipeline nlp . add_pipe ( \"eds.family\" ) # Family context detection Qualifiers pipelines need sentence boundaries to be set (see the specific documentation for detail). This code is complete, and should run as is. Reading the results Let's output the results as a pandas DataFrame for better readability: import spacy import pandas as pd text = ( \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , terms = terms , attr = \"LOWER\" , ), ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.negation\" ) # Negation component nlp . add_pipe ( \"eds.hypothesis\" ) # Speculation pipeline nlp . add_pipe ( \"eds.family\" ) # Family context detection doc = nlp ( text ) # Extraction as a pandas DataFrame entities = [] for ent in doc . ents : d = dict ( lexical_variant = ent . text , label = ent . label_ , negation = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , ) entities . append ( d ) df = pd . DataFrame . from_records ( entities ) This code is complete, and should run as is. We get the following result: lexical_variant label negation hypothesis family COVID19 covid False True False respiratoires respiratoire True False False asthmatique respiratoire False False True Conclusion The qualifier pipelines limits the number of false positives by detecting linguistic modulations such as negations or speculations. Go to the full documentation for a complete presentation of the different pipelines, their configuration options and validation performance. Recall the qualifier pipeline proposed by EDS-NLP: Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based history detection","title":"Qualifying entities"},{"location":"tutorials/qualifying-entities/#qualifying-entities","text":"In the previous tutorial, we saw how to match a terminology on a text. Using the doc . ents attribute, we can check whether a document mentions a concept of interest to build a cohort or describe patients.","title":"Qualifying entities"},{"location":"tutorials/qualifying-entities/#the-issue","text":"However, consider the classical example where we look for the diabetes concept: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. None of these expressions should be used to build a cohort: the detected entity is either negated, speculative, or does not concern the patient themself. That's why we need to qualify the matched entities . Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.","title":"The issue"},{"location":"tutorials/qualifying-entities/#the-solution","text":"We can use EDS-NLP's qualifier pipelines to achieve that. Let's add specific components to our pipeline to detect these three modalities.","title":"The solution"},{"location":"tutorials/qualifying-entities/#adding-qualifiers","text":"Adding qualifier pipelines is straightforward: import spacy text = ( \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , terms = terms , attr = \"LOWER\" , ), ) nlp . add_pipe ( \"eds.sentences\" ) # (1) nlp . add_pipe ( \"eds.negation\" ) # Negation component nlp . add_pipe ( \"eds.hypothesis\" ) # Speculation pipeline nlp . add_pipe ( \"eds.family\" ) # Family context detection Qualifiers pipelines need sentence boundaries to be set (see the specific documentation for detail). This code is complete, and should run as is.","title":"Adding qualifiers"},{"location":"tutorials/qualifying-entities/#reading-the-results","text":"Let's output the results as a pandas DataFrame for better readability: import spacy import pandas as pd text = ( \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , terms = terms , attr = \"LOWER\" , ), ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.negation\" ) # Negation component nlp . add_pipe ( \"eds.hypothesis\" ) # Speculation pipeline nlp . add_pipe ( \"eds.family\" ) # Family context detection doc = nlp ( text ) # Extraction as a pandas DataFrame entities = [] for ent in doc . ents : d = dict ( lexical_variant = ent . text , label = ent . label_ , negation = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , ) entities . append ( d ) df = pd . DataFrame . from_records ( entities ) This code is complete, and should run as is. We get the following result: lexical_variant label negation hypothesis family COVID19 covid False True False respiratoires respiratoire True False False asthmatique respiratoire False False True","title":"Reading the results"},{"location":"tutorials/qualifying-entities/#conclusion","text":"The qualifier pipelines limits the number of false positives by detecting linguistic modulations such as negations or speculations. Go to the full documentation for a complete presentation of the different pipelines, their configuration options and validation performance. Recall the qualifier pipeline proposed by EDS-NLP: Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based history detection","title":"Conclusion"},{"location":"tutorials/reason/","text":"Detecting Reason of Hospitalisation In this tutorial we will use the pipeline eds.reason to : Identify spans that corresponds to the reason of hospitalisation Check if there are named entities overlapping with my span of 'reason of hospitalisation' Check for all named entities if they are tagged is_reason import spacy import edsnlp.components text = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018 MOTIF D'HOSPITALISATION Monsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978, a \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. ANT\u00c9C\u00c9DENTS Ant\u00e9c\u00e9dents m\u00e9dicaux : Premier \u00e9pisode d'asthme en mai 2018.\"\"\" nlp = spacy . blank ( \"fr\" ) # Extraction d'entit\u00e9s nomm\u00e9es nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( respiratoire = [ \"asthmatique\" , \"asthme\" , \"toux\" , ] ) ), ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.sections\" ) nlp . add_pipe ( \"eds.reason\" , config = dict ( use_sections = True )) doc = nlp ( text ) The pipeline reason will add a key of spans called reasons . We check the first item in this list. # \u2191 Omitted code above \u2191 reason = doc . spans [ \"reasons\" ][ 0 ] reason # Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. Naturally, all spans included the reasons key have the attribute reason . _ . is_reason == True . # \u2191 Omitted code above \u2191 reason . _ . is_reason # Out: True # \u2191 Omitted code above \u2191 entities = reason . _ . ents_reason # (1) for e in entities : print ( \"Entity:\" , e . text , \"-- Label:\" , e . label_ , \"-- is_reason:\" , e . _ . is_reason , ) # Out: Entity: asthme -- Label: respiratoire -- is_reason: True We check if the span include named entities, their labels and the attribute is_reason We can verify that named entities that do not overlap with the spans of reason, have their attribute reason . _ . is_reason == False : for e in doc . ents : print ( e . start , e , e . _ . is_reason ) # Out: 42 asthme True # Out: 54 asthme False","title":"Detecting Reason of Hospitalisation"},{"location":"tutorials/reason/#detecting-reason-of-hospitalisation","text":"In this tutorial we will use the pipeline eds.reason to : Identify spans that corresponds to the reason of hospitalisation Check if there are named entities overlapping with my span of 'reason of hospitalisation' Check for all named entities if they are tagged is_reason import spacy import edsnlp.components text = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018 MOTIF D'HOSPITALISATION Monsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978, a \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. ANT\u00c9C\u00c9DENTS Ant\u00e9c\u00e9dents m\u00e9dicaux : Premier \u00e9pisode d'asthme en mai 2018.\"\"\" nlp = spacy . blank ( \"fr\" ) # Extraction d'entit\u00e9s nomm\u00e9es nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( respiratoire = [ \"asthmatique\" , \"asthme\" , \"toux\" , ] ) ), ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.sections\" ) nlp . add_pipe ( \"eds.reason\" , config = dict ( use_sections = True )) doc = nlp ( text ) The pipeline reason will add a key of spans called reasons . We check the first item in this list. # \u2191 Omitted code above \u2191 reason = doc . spans [ \"reasons\" ][ 0 ] reason # Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. Naturally, all spans included the reasons key have the attribute reason . _ . is_reason == True . # \u2191 Omitted code above \u2191 reason . _ . is_reason # Out: True # \u2191 Omitted code above \u2191 entities = reason . _ . ents_reason # (1) for e in entities : print ( \"Entity:\" , e . text , \"-- Label:\" , e . label_ , \"-- is_reason:\" , e . _ . is_reason , ) # Out: Entity: asthme -- Label: respiratoire -- is_reason: True We check if the span include named entities, their labels and the attribute is_reason We can verify that named entities that do not overlap with the spans of reason, have their attribute reason . _ . is_reason == False : for e in doc . ents : print ( e . start , e , e . _ . is_reason ) # Out: 42 asthme True # Out: 54 asthme False","title":"Detecting Reason of Hospitalisation"},{"location":"tutorials/spacy101/","text":"spaCy 101 EDS-NLP is a spaCy library. To use it, you will need to familiarise yourself with some key spaCy concepts. Skip if you're familiar with spaCy This page is intended as a crash course for the very basic spaCy concepts that are needed to use EDS-NLP. If you've already used spaCy, you should probably skip to the next page. In a nutshell, spaCy offers three things: a convenient abstraction with a language-dependent, rule-based, deterministic and non-destructive tokenizer a rich set of rule-based and trainable components a configuration and training system We will focus on the first item. Be sure to check out spaCy's crash course page for more information on the possibilities offered by the library. Resources The spaCy documentation is one of the great strengths of the library. In particular, you should check out the \"Advanced NLP with spaCy\" course , which provides a more in-depth presentation. spaCy in action Consider the following minimal example: import spacy # (1) # Initialise a spaCy pipeline nlp = spacy . blank ( \"fr\" ) # (2) text = \"Michel est un penseur lat\u00e9ral.\" # (3) # Apply the pipeline doc = nlp ( text ) # (4) doc . text # Out: 'Michel est un penseur lat\u00e9ral.' Import spaCy... Load a pipeline. In spaCy, the nlp object handles the entire processing. Define a text you want to process. Apply the pipeline and get a spaCy Doc object. We just created a spaCy pipeline and applied it to a sample text. It's that simple. Note that we use spaCy's \"blank\" NLP pipeline here. It actually carries a lot of information, and defines spaCy's language-dependent, rule-based tokenizer. Non-destructive processing In EDS-NLP, just like spaCy, non-destructiveness is a core principle. Your detected entities will always be linked to the original text . In other words, nlp ( text ) . text == text is always true. The Doc abstraction The doc object carries the result of the entire processing. It's the most important abstraction in spaCy, and holds a token-based representation of the text along with the results of every pipeline components. It also keeps track of the input text in a non-destructive manner, meaning that doc . text == text is always true. # \u2191 Omitted code above \u2191 # Text processing in spaCy is non-destructive doc . text == text # (1) # You can access a specific token token = doc [ 2 ] # (2) # And create a Span using slices span = doc [: 3 ] # (3) # Entities are tracked in the ents attribute doc . ents # (4) # Out: (,) This feature is a core principle in spaCy. It will always be true in EDS-NLP. token is a Token object referencing the third token span is a Span object referencing the first three tokens. We have not declared any entity recognizer in our pipeline, hence this attribute is empty. Adding pipeline components You can add pipeline components with the nlp . add_pipe method. Let's add two simple components to our pipeline. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # (1) nlp . add_pipe ( \"eds.dates\" ) # (2) text = \"Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.\" doc = nlp ( text ) Like the name suggests, this pipeline is declared by EDS-NLP. eds.sentences is a rule-based sentence boundary prediction. See its documentation for detail. Like the name suggests, this pipeline is declared by EDS-NLP. eds.dates is a date extraction and normalisation component. See its documentation for detail. The doc object just became more interesting! # \u2191 Omitted code above \u2191 # We can split the document into sentences doc . sents # (1) # Out: [Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.] # And look for dates doc . spans [ \"dates\" ] # (2) # Out: [5 mai 2005] span = doc . spans [ \"dates\" ][ 0 ] # (3) span . _ . date # (4) # Out: \"2005-05-05\" In this example, there is only one sentence... The eds.dates adds a key to the doc.spans attribute span is a spaCy Span object. In spaCy, you can declare custom extensions that live in the _ attribute. Here, the eds.dates pipeline uses a Span._.date extension to persist the normalised date. Conclusion This page is just a glimpse of a few possibilities offered by spaCy. To get a sense of what spaCy can help you achieve, we strongly recommend you visit their documentation and take the time to follow the spaCy course . Moreover, be sure to check out spaCy's own crash course , which is an excellent read. It goes into more detail on what's possible with the library.","title":"spaCy 101"},{"location":"tutorials/spacy101/#spacy-101","text":"EDS-NLP is a spaCy library. To use it, you will need to familiarise yourself with some key spaCy concepts. Skip if you're familiar with spaCy This page is intended as a crash course for the very basic spaCy concepts that are needed to use EDS-NLP. If you've already used spaCy, you should probably skip to the next page. In a nutshell, spaCy offers three things: a convenient abstraction with a language-dependent, rule-based, deterministic and non-destructive tokenizer a rich set of rule-based and trainable components a configuration and training system We will focus on the first item. Be sure to check out spaCy's crash course page for more information on the possibilities offered by the library.","title":"spaCy 101"},{"location":"tutorials/spacy101/#resources","text":"The spaCy documentation is one of the great strengths of the library. In particular, you should check out the \"Advanced NLP with spaCy\" course , which provides a more in-depth presentation.","title":"Resources"},{"location":"tutorials/spacy101/#spacy-in-action","text":"Consider the following minimal example: import spacy # (1) # Initialise a spaCy pipeline nlp = spacy . blank ( \"fr\" ) # (2) text = \"Michel est un penseur lat\u00e9ral.\" # (3) # Apply the pipeline doc = nlp ( text ) # (4) doc . text # Out: 'Michel est un penseur lat\u00e9ral.' Import spaCy... Load a pipeline. In spaCy, the nlp object handles the entire processing. Define a text you want to process. Apply the pipeline and get a spaCy Doc object. We just created a spaCy pipeline and applied it to a sample text. It's that simple. Note that we use spaCy's \"blank\" NLP pipeline here. It actually carries a lot of information, and defines spaCy's language-dependent, rule-based tokenizer. Non-destructive processing In EDS-NLP, just like spaCy, non-destructiveness is a core principle. Your detected entities will always be linked to the original text . In other words, nlp ( text ) . text == text is always true.","title":"spaCy in action"},{"location":"tutorials/spacy101/#the-doc-abstraction","text":"The doc object carries the result of the entire processing. It's the most important abstraction in spaCy, and holds a token-based representation of the text along with the results of every pipeline components. It also keeps track of the input text in a non-destructive manner, meaning that doc . text == text is always true. # \u2191 Omitted code above \u2191 # Text processing in spaCy is non-destructive doc . text == text # (1) # You can access a specific token token = doc [ 2 ] # (2) # And create a Span using slices span = doc [: 3 ] # (3) # Entities are tracked in the ents attribute doc . ents # (4) # Out: (,) This feature is a core principle in spaCy. It will always be true in EDS-NLP. token is a Token object referencing the third token span is a Span object referencing the first three tokens. We have not declared any entity recognizer in our pipeline, hence this attribute is empty.","title":"The Doc abstraction"},{"location":"tutorials/spacy101/#adding-pipeline-components","text":"You can add pipeline components with the nlp . add_pipe method. Let's add two simple components to our pipeline. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # (1) nlp . add_pipe ( \"eds.dates\" ) # (2) text = \"Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.\" doc = nlp ( text ) Like the name suggests, this pipeline is declared by EDS-NLP. eds.sentences is a rule-based sentence boundary prediction. See its documentation for detail. Like the name suggests, this pipeline is declared by EDS-NLP. eds.dates is a date extraction and normalisation component. See its documentation for detail. The doc object just became more interesting! # \u2191 Omitted code above \u2191 # We can split the document into sentences doc . sents # (1) # Out: [Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.] # And look for dates doc . spans [ \"dates\" ] # (2) # Out: [5 mai 2005] span = doc . spans [ \"dates\" ][ 0 ] # (3) span . _ . date # (4) # Out: \"2005-05-05\" In this example, there is only one sentence... The eds.dates adds a key to the doc.spans attribute span is a spaCy Span object. In spaCy, you can declare custom extensions that live in the _ attribute. Here, the eds.dates pipeline uses a Span._.date extension to persist the normalised date.","title":"Adding pipeline components"},{"location":"tutorials/spacy101/#conclusion","text":"This page is just a glimpse of a few possibilities offered by spaCy. To get a sense of what spaCy can help you achieve, we strongly recommend you visit their documentation and take the time to follow the spaCy course . Moreover, be sure to check out spaCy's own crash course , which is an excellent read. It goes into more detail on what's possible with the library.","title":"Conclusion"},{"location":"utilities/","text":"Utilities EDS-NLP provides a few utilities to deploy pipelines, process RegExps, etc.","title":"Utilities"},{"location":"utilities/#utilities","text":"EDS-NLP provides a few utilities to deploy pipelines, process RegExps, etc.","title":"Utilities"},{"location":"utilities/evaluation/","text":"Pipeline evaluation","title":"Pipeline evaluation"},{"location":"utilities/evaluation/#pipeline-evaluation","text":"","title":"Pipeline evaluation"},{"location":"utilities/regex/","text":"Work with RegExp","title":"Work with RegExp"},{"location":"utilities/regex/#work-with-regexp","text":"","title":"Work with RegExp"},{"location":"utilities/connectors/","text":"Overview of connectors EDS-NLP provides a series of connectors apt to convert back and forth from different formats into spaCy representation. We provide the following connectors: BRAT OMOP","title":"Overview of connectors"},{"location":"utilities/connectors/#overview-of-connectors","text":"EDS-NLP provides a series of connectors apt to convert back and forth from different formats into spaCy representation. We provide the following connectors: BRAT OMOP","title":"Overview of connectors"},{"location":"utilities/connectors/brat/","text":"BRAT Connector BRAT is currently the only supported in-text annotation editor at EDS. BRAT annotations are in the standoff format . Consider the following document: Le patient est admis pour une pneumopathie au coronavirus. On lui prescrit du parac\u00e9tamol. It could be annotated as follows : T1 Patient 4 11 patient T2 Disease 31 58 pneumopathie au coronavirus T3 Drug 79 90 parac\u00e9tamol The point of the BRAT connector is to go from the standoff annotation format to an annotated spaCy document : import spacy from edsnlp.connectors.brat import BratConnector # Instantiate the connector brat = BratConnector ( \"path/to/brat\" ) # Instantiate the spacy pipeline nlp = spacy . blank ( \"fr\" ) # Convert all BRAT files to a list of documents docs = brat . brat2docs ( nlp ) doc = docs [ 0 ] doc . ents # Out: [patient, pneumopathie au coronavirus, parac\u00e9tamol] doc . ents [ 0 ] . label_ # Out: Patient The connector can also go the other way around, enabling pre-annotations and an ersatz of active learning.","title":"BRAT Connector"},{"location":"utilities/connectors/brat/#brat-connector","text":"BRAT is currently the only supported in-text annotation editor at EDS. BRAT annotations are in the standoff format . Consider the following document: Le patient est admis pour une pneumopathie au coronavirus. On lui prescrit du parac\u00e9tamol. It could be annotated as follows : T1 Patient 4 11 patient T2 Disease 31 58 pneumopathie au coronavirus T3 Drug 79 90 parac\u00e9tamol The point of the BRAT connector is to go from the standoff annotation format to an annotated spaCy document : import spacy from edsnlp.connectors.brat import BratConnector # Instantiate the connector brat = BratConnector ( \"path/to/brat\" ) # Instantiate the spacy pipeline nlp = spacy . blank ( \"fr\" ) # Convert all BRAT files to a list of documents docs = brat . brat2docs ( nlp ) doc = docs [ 0 ] doc . ents # Out: [patient, pneumopathie au coronavirus, parac\u00e9tamol] doc . ents [ 0 ] . label_ # Out: Patient The connector can also go the other way around, enabling pre-annotations and an ersatz of active learning.","title":"BRAT Connector"},{"location":"utilities/connectors/labeltool/","text":"LabelTool Connector LabelTool is an in-house module enabling rapid annotation of pre-extracted entities. We provide a ready-to-use function that converts a list of annotated spaCy documents into a pandas DataFrame that is readable to LabelTool. import spacy from edsnlp.connectors.labeltool import docs2labeltool corpus = [ \"Ceci est un document m\u00e9dical.\" , \"Le patient n'est pas malade.\" , ] # Instantiate the spacy pipeline nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( medical = \"m\u00e9dical\" , malade = \"malade\" ))) nlp . add_pipe ( \"eds.negation\" ) # Convert all BRAT files to a list of documents docs = nlp . pipe ( corpus ) df = docs2labeltool ( docs , extensions = [ \"negation\" ]) The results: note_id note_text start end label lexical_variant negation 0 Ceci est un document m\u00e9dical. 21 28 medical m\u00e9dical False 1 Le patient n'est pas malade. 21 27 malade malade True","title":"LabelTool Connector"},{"location":"utilities/connectors/labeltool/#labeltool-connector","text":"LabelTool is an in-house module enabling rapid annotation of pre-extracted entities. We provide a ready-to-use function that converts a list of annotated spaCy documents into a pandas DataFrame that is readable to LabelTool. import spacy from edsnlp.connectors.labeltool import docs2labeltool corpus = [ \"Ceci est un document m\u00e9dical.\" , \"Le patient n'est pas malade.\" , ] # Instantiate the spacy pipeline nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( medical = \"m\u00e9dical\" , malade = \"malade\" ))) nlp . add_pipe ( \"eds.negation\" ) # Convert all BRAT files to a list of documents docs = nlp . pipe ( corpus ) df = docs2labeltool ( docs , extensions = [ \"negation\" ]) The results: note_id note_text start end label lexical_variant negation 0 Ceci est un document m\u00e9dical. 21 28 medical m\u00e9dical False 1 Le patient n'est pas malade. 21 27 malade malade True","title":"LabelTool Connector"},{"location":"utilities/connectors/omop/","text":"OMOP Connector We provide a connector between OMOP-formatted dataframes and spaCy documents. OMOP-style dataframes Consider a corpus of just one document: Le patient est admis pour une pneumopathie au coronavirus. On lui prescrit du parac\u00e9tamol. And its OMOP-style representation, separated in two tables note and note_nlp (here with selected columns) : note : note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23 note_nlp : note_nlp_id note_id start_char end_char note_nlp_source_value lexical_variant 0 0 46 57 disease coronavirus 1 0 77 88 drug parac\u00e9tamol Using the connector The following snippet expects the tables note and note_nlp to be already defined (eg through PySpark's toPandas() method). import spacy from edsnlp.connectors.omop import OmopConnector # Instantiate a spacy pipeline nlp = spacy . blank ( \"fr\" ) # Instantiate the connector connector = OmopConnector ( nlp ) # Convert OMOP tables (note and note_nlp) to a list of documents docs = connector . omop2docs ( note , note_nlp ) doc = docs [ 0 ] doc . ents # Out: [coronavirus, parac\u00e9tamol] doc . ents [ 0 ] . label_ # Out: disease doc . text == note . loc [ 0 ] . note_text # Out: True The object docs now contains a list of documents that reflects the information contained in the OMOP-formatted dataframes.","title":"OMOP Connector"},{"location":"utilities/connectors/omop/#omop-connector","text":"We provide a connector between OMOP-formatted dataframes and spaCy documents.","title":"OMOP Connector"},{"location":"utilities/connectors/omop/#omop-style-dataframes","text":"Consider a corpus of just one document: Le patient est admis pour une pneumopathie au coronavirus. On lui prescrit du parac\u00e9tamol. And its OMOP-style representation, separated in two tables note and note_nlp (here with selected columns) : note : note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23 note_nlp : note_nlp_id note_id start_char end_char note_nlp_source_value lexical_variant 0 0 46 57 disease coronavirus 1 0 77 88 drug parac\u00e9tamol","title":"OMOP-style dataframes"},{"location":"utilities/connectors/omop/#using-the-connector","text":"The following snippet expects the tables note and note_nlp to be already defined (eg through PySpark's toPandas() method). import spacy from edsnlp.connectors.omop import OmopConnector # Instantiate a spacy pipeline nlp = spacy . blank ( \"fr\" ) # Instantiate the connector connector = OmopConnector ( nlp ) # Convert OMOP tables (note and note_nlp) to a list of documents docs = connector . omop2docs ( note , note_nlp ) doc = docs [ 0 ] doc . ents # Out: [coronavirus, parac\u00e9tamol] doc . ents [ 0 ] . label_ # Out: disease doc . text == note . loc [ 0 ] . note_text # Out: True The object docs now contains a list of documents that reflects the information contained in the OMOP-formatted dataframes.","title":"Using the connector"},{"location":"utilities/processing/","text":"Overview of processing","title":"Overview of processing"},{"location":"utilities/processing/#overview-of-processing","text":"","title":"Overview of processing"},{"location":"utilities/processing/multi/","text":"Multiprocessing","title":"Multiprocessing"},{"location":"utilities/processing/multi/#multiprocessing","text":"","title":"Multiprocessing"},{"location":"utilities/processing/single/","text":"Single processing","title":"Single processing"},{"location":"utilities/processing/single/#single-processing","text":"","title":"Single processing"},{"location":"utilities/processing/spark/","text":"Deploying on Spark We provide a simple connector to distribute a pipeline on a Spark cluster. We expose a Spark UDF (user-defined function) factory that handles the nitty gritty of distributing a pipeline over a cluster of Spark-enabled machines. Distributing a pipeline Because of the way Spark distributes Python objects, we need to re-declare custom extensions on the executors. To make this step as smooth as possible, EDS-NLP provides a BaseComponent class that implements a set_extensions method. When the pipeline is distributed, every component that extend BaseComponent rerun their set_extensions method. Since spaCy Doc objects cannot easily be serialised, the UDF we provide returns a list of detected entities along with selected qualifiers. Example See the dedicated tutorial for a step-by-step presentation. Authors and citation The Spark connector was developed by AP-HP's Data Science team.","title":"Deploying on Spark"},{"location":"utilities/processing/spark/#deploying-on-spark","text":"We provide a simple connector to distribute a pipeline on a Spark cluster. We expose a Spark UDF (user-defined function) factory that handles the nitty gritty of distributing a pipeline over a cluster of Spark-enabled machines.","title":"Deploying on Spark"},{"location":"utilities/processing/spark/#distributing-a-pipeline","text":"Because of the way Spark distributes Python objects, we need to re-declare custom extensions on the executors. To make this step as smooth as possible, EDS-NLP provides a BaseComponent class that implements a set_extensions method. When the pipeline is distributed, every component that extend BaseComponent rerun their set_extensions method. Since spaCy Doc objects cannot easily be serialised, the UDF we provide returns a list of detected entities along with selected qualifiers.","title":"Distributing a pipeline"},{"location":"utilities/processing/spark/#example","text":"See the dedicated tutorial for a step-by-step presentation.","title":"Example"},{"location":"utilities/processing/spark/#authors-and-citation","text":"The Spark connector was developed by AP-HP's Data Science team.","title":"Authors and citation"}]}